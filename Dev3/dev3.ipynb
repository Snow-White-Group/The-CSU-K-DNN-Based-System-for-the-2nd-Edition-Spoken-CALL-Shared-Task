{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN_PAPER_v3_Handin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clearY(y):\n",
    "    clean_input = np.array([]).reshape(0, 1)\n",
    "    for data in y:\n",
    "        pos1 = data[0]\n",
    "        pos2 = data[1]\n",
    "        pos3 = data[2]\n",
    "        if  pos1 == 1 and pos2 == 0 and pos3 ==0:\n",
    "                clean_input = np.vstack((clean_input, [1]))\n",
    "        else:\n",
    "                clean_input = np.vstack((clean_input, [0]))\n",
    "    return clean_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_y, pred_y):\n",
    "    true_classes = []\n",
    "    for array in true_y:\n",
    "        if np.array_equal(array,[1, 0, 0]):\n",
    "            true_classes.append(0)\n",
    "        elif np.array_equal(array,[0, 1, 0]):\n",
    "            true_classes.append(1)\n",
    "        else:\n",
    "            true_classes.append(2)\n",
    "        \n",
    "    CR, CA, PFA, GFA, FR, k = 0, 0, 0, 0, 0, 3.0\n",
    "    for idx, prediction in enumerate(pred_y):\n",
    "        # the students answer is correct in meaning and language\n",
    "        # the system says the same -> accept\n",
    "        if true_classes[idx] == 0 and prediction == 1:\n",
    "            CA += 1\n",
    "        # the system says correct meaning wrong language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "\n",
    "        # students answer is correct in meaning and wrong in language\n",
    "        #The system says the same -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning and correct language -> accept\n",
    "        elif true_classes[idx] == 1 and prediction == 1:\n",
    "            PFA += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "        # students answer is incorrect in meaning and incorrect in language\n",
    "        # the system says the same -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning correct language -> accept\n",
    "        elif true_classes[idx] == 2 and prediction == 1: \n",
    "            GFA += 1\n",
    "        # the system says correct meaning incorrect language -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "    FA = PFA + k * GFA\n",
    "    Correct = CA + FR\n",
    "    Incorrect = CR + GFA + PFA\n",
    "    IncorrectRejectionRate = CR / ( CR + FA + 0.0 )\n",
    "    CorrectRejectionRate = FR / ( FR + CA + 0.0 )\n",
    "    # Further metrics\n",
    "    Z = CA + CR + FA + FR\n",
    "    Ca = CA / Z\n",
    "    Cr = CR / Z\n",
    "    Fa = FA / Z\n",
    "    Fr = FR / Z\n",
    "    \n",
    "    P = Ca / (Ca + Fa)\n",
    "    R = Ca / (Ca + Fr)\n",
    "    F = (2 * P * R)/( P + R)\n",
    "    \n",
    "    RCa = Ca / (Fr + Ca)\n",
    "    RFa = Fa / (Cr + Fa)\n",
    "    \n",
    "    D = IncorrectRejectionRate / CorrectRejectionRate\n",
    "    Da = RCa / RFa\n",
    "    Df = math.sqrt((Da*D))\n",
    "    \n",
    "    print('\\nINCORRECT UTTERANCES (' + str(Incorrect) + ')' )\n",
    "    print('CorrectReject    ' + str(CR) )\n",
    "    print('GrossFalseAccept ' + str(GFA) + '*' + str(k) + ' = ' + str(GFA * k) )\n",
    "    print('PlainFalseAccept ' + str(PFA) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(IncorrectRejectionRate) )\n",
    "\n",
    "    print('\\nCORRECT UTTERANCES (' + str(Correct) + ')')\n",
    "    print('CorrectAccept    ' + str(CA) )\n",
    "    print('FalseReject      ' + str(FR) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(CorrectRejectionRate) )\n",
    "    \n",
    "    print('\\n--------------REPORT---------------')\n",
    "    print('-----------------------------------')\n",
    "    print('Pr                            ' +  \"{:.3f}\".format(P) )\n",
    "    print('F                             ' +  \"{:.3f}\".format(F) )\n",
    "    print('Sa                            ' +  \"{:.3f}\".format(R) )\n",
    "    #print('R                             ' +  \"{:.3f}\".format(R) )\n",
    "\n",
    "    \n",
    "    print('\\n--------------Metrics--------------')\n",
    "    print('D                             ' +  \"{:.3f}\".format(D) )\n",
    "    print('Da                            ' +  \"{:.3f}\".format(Da) )\n",
    "    print('Df                            ' +  \"{:.3f}\".format(Df) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x:    10092\n",
      "Train_y:    10092\n",
      "\n",
      "\n",
      "Dev_Test_x:    2524\n",
      "Dev_Test_y:    2524\n",
      "\n",
      "\n",
      "St2_Test_x:    1000\n",
      "St2_Test_y:    1000\n"
     ]
    }
   ],
   "source": [
    "train_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_train_x.csv' ,delimiter=',',usecols=range(11)[1:])\n",
    "train_y = clearY(np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_train_y.csv', delimiter=',',usecols=range(4)[1:]))\n",
    "\n",
    "print('Train_x:    ' + str(len(train_x)))\n",
    "print('Train_y:    ' + str(len(train_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "dev_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "dev_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('Dev_Test_x:    ' + str(len(dev_test_x)))\n",
    "print('Dev_Test_y:    ' + str(len(dev_test_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "st2_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_st2_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "st2_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_st2_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('St2_Test_x:    ' + str(len(st2_test_x)))\n",
    "print('St2_Test_y:    ' + str(len(st2_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sclae the vectors inorder to get better classification\n",
    "sc = StandardScaler()\n",
    "scaled_train_x = sc.fit_transform(train_x)\n",
    "scaled_dev_test_x = sc.transform(dev_test_x)\n",
    "scaled_st2_test_x = sc.transform(st2_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(64, activation='relu', input_dim=10))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "classifier.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9082 samples, validate on 1010 samples\n",
      "Epoch 1/600\n",
      "9082/9082 [==============================] - 0s 33us/step - loss: 0.5524 - acc: 0.7356 - val_loss: 0.5025 - val_acc: 0.7545\n",
      "Epoch 2/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.4581 - acc: 0.7808 - val_loss: 0.4375 - val_acc: 0.8050\n",
      "Epoch 3/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3849 - acc: 0.8391 - val_loss: 0.4154 - val_acc: 0.8277\n",
      "Epoch 4/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3505 - acc: 0.8641 - val_loss: 0.4152 - val_acc: 0.8327\n",
      "Epoch 5/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3371 - acc: 0.8741 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 6/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3292 - acc: 0.8823 - val_loss: 0.4149 - val_acc: 0.8396\n",
      "Epoch 7/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3254 - acc: 0.8827 - val_loss: 0.4170 - val_acc: 0.8406\n",
      "Epoch 8/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3211 - acc: 0.8882 - val_loss: 0.4202 - val_acc: 0.8396\n",
      "Epoch 9/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3198 - acc: 0.8880 - val_loss: 0.4199 - val_acc: 0.8416\n",
      "Epoch 10/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3193 - acc: 0.8896 - val_loss: 0.4168 - val_acc: 0.8406\n",
      "Epoch 11/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3184 - acc: 0.8899 - val_loss: 0.4203 - val_acc: 0.8386\n",
      "Epoch 12/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3158 - acc: 0.8907 - val_loss: 0.4265 - val_acc: 0.8396\n",
      "Epoch 13/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3133 - acc: 0.8911 - val_loss: 0.4200 - val_acc: 0.8396\n",
      "Epoch 14/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3128 - acc: 0.8914 - val_loss: 0.4267 - val_acc: 0.8386\n",
      "Epoch 15/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3115 - acc: 0.8917 - val_loss: 0.4229 - val_acc: 0.8396\n",
      "Epoch 16/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3124 - acc: 0.8915 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 17/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3122 - acc: 0.8928 - val_loss: 0.4260 - val_acc: 0.8406\n",
      "Epoch 18/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3108 - acc: 0.8933 - val_loss: 0.4269 - val_acc: 0.8406\n",
      "Epoch 19/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3115 - acc: 0.8929 - val_loss: 0.4244 - val_acc: 0.8386\n",
      "Epoch 20/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3107 - acc: 0.8931 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 21/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3101 - acc: 0.8933 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 22/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3090 - acc: 0.8929 - val_loss: 0.4267 - val_acc: 0.8386\n",
      "Epoch 23/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3101 - acc: 0.8924 - val_loss: 0.4273 - val_acc: 0.8386\n",
      "Epoch 24/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3091 - acc: 0.8931 - val_loss: 0.4243 - val_acc: 0.8366\n",
      "Epoch 25/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3095 - acc: 0.8935 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 26/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3096 - acc: 0.8920 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 27/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3083 - acc: 0.8932 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 28/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3076 - acc: 0.8932 - val_loss: 0.4288 - val_acc: 0.8366\n",
      "Epoch 29/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3075 - acc: 0.8932 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 30/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3076 - acc: 0.8941 - val_loss: 0.4262 - val_acc: 0.8376\n",
      "Epoch 31/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3094 - acc: 0.8931 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 32/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3087 - acc: 0.8939 - val_loss: 0.4322 - val_acc: 0.8406\n",
      "Epoch 33/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8932 - val_loss: 0.4241 - val_acc: 0.8376\n",
      "Epoch 34/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3079 - acc: 0.8934 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 35/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3074 - acc: 0.8929 - val_loss: 0.4327 - val_acc: 0.8406\n",
      "Epoch 36/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3076 - acc: 0.8933 - val_loss: 0.4399 - val_acc: 0.8406\n",
      "Epoch 37/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8929 - val_loss: 0.4216 - val_acc: 0.8376\n",
      "Epoch 38/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3090 - acc: 0.8934 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 39/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3064 - acc: 0.8937 - val_loss: 0.4288 - val_acc: 0.8366\n",
      "Epoch 40/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3075 - acc: 0.8932 - val_loss: 0.4263 - val_acc: 0.8347\n",
      "Epoch 41/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3062 - acc: 0.8934 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 42/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3070 - acc: 0.8939 - val_loss: 0.4312 - val_acc: 0.8356\n",
      "Epoch 43/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3079 - acc: 0.8932 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 44/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3060 - acc: 0.8931 - val_loss: 0.4133 - val_acc: 0.8327\n",
      "Epoch 45/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3065 - acc: 0.8936 - val_loss: 0.4131 - val_acc: 0.8347\n",
      "Epoch 46/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3068 - acc: 0.8941 - val_loss: 0.4397 - val_acc: 0.8386\n",
      "Epoch 47/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3052 - acc: 0.8944 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 48/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8941 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 49/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3070 - acc: 0.8937 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 50/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3067 - acc: 0.8939 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 51/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3080 - acc: 0.8939 - val_loss: 0.4370 - val_acc: 0.8366\n",
      "Epoch 52/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3071 - acc: 0.8943 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 53/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3042 - acc: 0.8937 - val_loss: 0.4303 - val_acc: 0.8366\n",
      "Epoch 54/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3064 - acc: 0.8932 - val_loss: 0.4297 - val_acc: 0.8376\n",
      "Epoch 55/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3039 - acc: 0.8935 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 56/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3046 - acc: 0.8941 - val_loss: 0.4192 - val_acc: 0.8327\n",
      "Epoch 57/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3045 - acc: 0.8936 - val_loss: 0.4333 - val_acc: 0.8366\n",
      "Epoch 58/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8933 - val_loss: 0.4307 - val_acc: 0.8396\n",
      "Epoch 59/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3057 - acc: 0.8934 - val_loss: 0.4351 - val_acc: 0.8386\n",
      "Epoch 60/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3045 - acc: 0.8940 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 61/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3042 - acc: 0.8935 - val_loss: 0.4155 - val_acc: 0.8327\n",
      "Epoch 62/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8936 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 63/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3042 - acc: 0.8939 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 64/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3038 - acc: 0.8940 - val_loss: 0.4292 - val_acc: 0.8347\n",
      "Epoch 65/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3037 - acc: 0.8943 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 66/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3057 - acc: 0.8930 - val_loss: 0.4306 - val_acc: 0.8347\n",
      "Epoch 67/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.4143 - val_acc: 0.8327\n",
      "Epoch 68/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3034 - acc: 0.8939 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 69/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8936 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 70/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8935 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 71/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3042 - acc: 0.8942 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 72/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3051 - acc: 0.8942 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 73/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3062 - acc: 0.8933 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 74/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3046 - acc: 0.8939 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 75/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3036 - acc: 0.8939 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 76/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3042 - acc: 0.8936 - val_loss: 0.4303 - val_acc: 0.8347\n",
      "Epoch 77/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8942 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 78/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3035 - acc: 0.8937 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 79/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3029 - acc: 0.8928 - val_loss: 0.4079 - val_acc: 0.8327\n",
      "Epoch 80/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3038 - acc: 0.8935 - val_loss: 0.4069 - val_acc: 0.8327\n",
      "Epoch 81/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8933 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 82/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3041 - acc: 0.8939 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 83/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3023 - acc: 0.8942 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 84/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8931 - val_loss: 0.4289 - val_acc: 0.8347\n",
      "Epoch 85/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8932 - val_loss: 0.4305 - val_acc: 0.8376\n",
      "Epoch 86/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3031 - acc: 0.8933 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 87/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3030 - acc: 0.8935 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 88/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3025 - acc: 0.8936 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 89/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3035 - acc: 0.8937 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 90/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3041 - acc: 0.8932 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 91/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3027 - acc: 0.8941 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 92/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3025 - acc: 0.8939 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 93/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3014 - acc: 0.8937 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 94/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3057 - acc: 0.8935 - val_loss: 0.4112 - val_acc: 0.8327\n",
      "Epoch 95/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3026 - acc: 0.8940 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 96/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3033 - acc: 0.8937 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 97/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3019 - acc: 0.8937 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 98/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8941 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 99/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3006 - acc: 0.8929 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 100/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3046 - acc: 0.8937 - val_loss: 0.4109 - val_acc: 0.8327\n",
      "Epoch 101/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3026 - acc: 0.8941 - val_loss: 0.4182 - val_acc: 0.8327\n",
      "Epoch 102/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3030 - acc: 0.8937 - val_loss: 0.4162 - val_acc: 0.8347\n",
      "Epoch 103/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3024 - acc: 0.8934 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 104/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3029 - acc: 0.8941 - val_loss: 0.4260 - val_acc: 0.8376\n",
      "Epoch 105/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8935 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 106/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3018 - acc: 0.8947 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 107/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3022 - acc: 0.8937 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 108/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2993 - acc: 0.8942 - val_loss: 0.4293 - val_acc: 0.8356\n",
      "Epoch 109/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3030 - acc: 0.8937 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 110/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3009 - acc: 0.8942 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 111/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3020 - acc: 0.8947 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 112/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3013 - acc: 0.8941 - val_loss: 0.4269 - val_acc: 0.8347\n",
      "Epoch 113/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8941 - val_loss: 0.4129 - val_acc: 0.8327\n",
      "Epoch 114/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3020 - acc: 0.8943 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 115/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3022 - acc: 0.8937 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 116/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8940 - val_loss: 0.4105 - val_acc: 0.8327\n",
      "Epoch 117/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3018 - acc: 0.8939 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 118/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3024 - acc: 0.8946 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 119/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2998 - acc: 0.8935 - val_loss: 0.4123 - val_acc: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3000 - acc: 0.8939 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 121/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3000 - acc: 0.8942 - val_loss: 0.4115 - val_acc: 0.8347\n",
      "Epoch 122/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2996 - acc: 0.8939 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 123/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3006 - acc: 0.8935 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 124/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3003 - acc: 0.8942 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 125/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3020 - acc: 0.8940 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 126/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3027 - acc: 0.8944 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 127/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3015 - acc: 0.8946 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 128/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3017 - acc: 0.8940 - val_loss: 0.4288 - val_acc: 0.8347\n",
      "Epoch 129/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2994 - acc: 0.8944 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 130/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3004 - acc: 0.8941 - val_loss: 0.4232 - val_acc: 0.8366\n",
      "Epoch 131/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3010 - acc: 0.8937 - val_loss: 0.4146 - val_acc: 0.8347\n",
      "Epoch 132/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3015 - acc: 0.8937 - val_loss: 0.4327 - val_acc: 0.8347\n",
      "Epoch 133/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3019 - acc: 0.8935 - val_loss: 0.4165 - val_acc: 0.8327\n",
      "Epoch 134/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3027 - acc: 0.8940 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 135/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3004 - acc: 0.8944 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 136/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3003 - acc: 0.8936 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 137/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3023 - acc: 0.8940 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 138/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2991 - acc: 0.8941 - val_loss: 0.4085 - val_acc: 0.8327\n",
      "Epoch 139/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8941 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 140/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2984 - acc: 0.8944 - val_loss: 0.4437 - val_acc: 0.8396\n",
      "Epoch 141/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3010 - acc: 0.8941 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 142/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2995 - acc: 0.8942 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 143/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3015 - acc: 0.8939 - val_loss: 0.4126 - val_acc: 0.8347\n",
      "Epoch 144/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2996 - acc: 0.8935 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 145/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2997 - acc: 0.8939 - val_loss: 0.4153 - val_acc: 0.8327\n",
      "Epoch 146/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3002 - acc: 0.8939 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 147/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8941 - val_loss: 0.4114 - val_acc: 0.8347\n",
      "Epoch 148/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2990 - acc: 0.8939 - val_loss: 0.4326 - val_acc: 0.8356\n",
      "Epoch 149/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3005 - acc: 0.8939 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 150/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2993 - acc: 0.8944 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 151/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2989 - acc: 0.8944 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 152/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2987 - acc: 0.8942 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 153/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3004 - acc: 0.8943 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 154/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3000 - acc: 0.8936 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 155/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3017 - acc: 0.8943 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 156/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3018 - acc: 0.8940 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 157/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3015 - acc: 0.8943 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 158/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2992 - acc: 0.8942 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 159/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2989 - acc: 0.8934 - val_loss: 0.4096 - val_acc: 0.8327\n",
      "Epoch 160/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3000 - acc: 0.8941 - val_loss: 0.4202 - val_acc: 0.8356\n",
      "Epoch 161/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2989 - acc: 0.8933 - val_loss: 0.4073 - val_acc: 0.8327\n",
      "Epoch 162/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2977 - acc: 0.8937 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 163/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3003 - acc: 0.8932 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 164/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3013 - acc: 0.8942 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 165/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2990 - acc: 0.8936 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 166/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2986 - acc: 0.8939 - val_loss: 0.4118 - val_acc: 0.8327\n",
      "Epoch 167/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2985 - acc: 0.8942 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 168/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2992 - acc: 0.8935 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 169/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2990 - acc: 0.8939 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 170/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2998 - acc: 0.8939 - val_loss: 0.4273 - val_acc: 0.8386\n",
      "Epoch 171/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8948 - val_loss: 0.4070 - val_acc: 0.8347\n",
      "Epoch 172/600\n",
      "9082/9082 [==============================] - 0s 20us/step - loss: 0.2998 - acc: 0.8942 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 173/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8939 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 174/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2985 - acc: 0.8942 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 175/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2993 - acc: 0.8940 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 176/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2979 - acc: 0.8941 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 177/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2995 - acc: 0.8935 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 178/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2995 - acc: 0.8939 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 179/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2985 - acc: 0.8940 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 180/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2979 - acc: 0.8931 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 181/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2994 - acc: 0.8939 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 182/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2986 - acc: 0.8939 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 183/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2989 - acc: 0.8946 - val_loss: 0.4137 - val_acc: 0.8347\n",
      "Epoch 184/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2997 - acc: 0.8944 - val_loss: 0.4075 - val_acc: 0.8327\n",
      "Epoch 185/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3011 - acc: 0.8943 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 186/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2994 - acc: 0.8936 - val_loss: 0.4140 - val_acc: 0.8327\n",
      "Epoch 187/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2995 - acc: 0.8947 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 188/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3008 - acc: 0.8942 - val_loss: 0.4089 - val_acc: 0.8347\n",
      "Epoch 189/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3012 - acc: 0.8930 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 190/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2985 - acc: 0.8939 - val_loss: 0.4354 - val_acc: 0.8347\n",
      "Epoch 191/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2993 - acc: 0.8943 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 192/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2970 - acc: 0.8941 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 193/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2998 - acc: 0.8941 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 194/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2988 - acc: 0.8935 - val_loss: 0.4119 - val_acc: 0.8327\n",
      "Epoch 195/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2973 - acc: 0.8939 - val_loss: 0.4286 - val_acc: 0.8347\n",
      "Epoch 196/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2984 - acc: 0.8939 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 197/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2974 - acc: 0.8941 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 198/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2985 - acc: 0.8948 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 199/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8940 - val_loss: 0.4291 - val_acc: 0.8347\n",
      "Epoch 200/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2973 - acc: 0.8937 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 201/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2970 - acc: 0.8943 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 202/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2996 - acc: 0.8945 - val_loss: 0.4125 - val_acc: 0.8347\n",
      "Epoch 203/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2979 - acc: 0.8940 - val_loss: 0.4108 - val_acc: 0.8347\n",
      "Epoch 204/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2981 - acc: 0.8933 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 205/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2988 - acc: 0.8936 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 206/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2974 - acc: 0.8939 - val_loss: 0.4105 - val_acc: 0.8327\n",
      "Epoch 207/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2980 - acc: 0.8944 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 208/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2989 - acc: 0.8941 - val_loss: 0.4171 - val_acc: 0.8327\n",
      "Epoch 209/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2980 - acc: 0.8951 - val_loss: 0.4232 - val_acc: 0.8356\n",
      "Epoch 210/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2997 - acc: 0.8941 - val_loss: 0.4083 - val_acc: 0.8327\n",
      "Epoch 211/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2988 - acc: 0.8941 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 212/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2977 - acc: 0.8942 - val_loss: 0.4125 - val_acc: 0.8347\n",
      "Epoch 213/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2994 - acc: 0.8940 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 214/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2985 - acc: 0.8943 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 215/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2994 - acc: 0.8943 - val_loss: 0.4064 - val_acc: 0.8347\n",
      "Epoch 216/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2979 - acc: 0.8942 - val_loss: 0.4087 - val_acc: 0.8347\n",
      "Epoch 217/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2985 - acc: 0.8942 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 218/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3001 - acc: 0.8937 - val_loss: 0.4113 - val_acc: 0.8347\n",
      "Epoch 219/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2975 - acc: 0.8937 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 220/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2989 - acc: 0.8941 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 221/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2955 - acc: 0.8940 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 222/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2987 - acc: 0.8944 - val_loss: 0.4075 - val_acc: 0.8347\n",
      "Epoch 223/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2979 - acc: 0.8940 - val_loss: 0.4050 - val_acc: 0.8347\n",
      "Epoch 224/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2983 - acc: 0.8947 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 225/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2993 - acc: 0.8936 - val_loss: 0.4115 - val_acc: 0.8347\n",
      "Epoch 226/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2988 - acc: 0.8943 - val_loss: 0.4082 - val_acc: 0.8327\n",
      "Epoch 227/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2979 - acc: 0.8948 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 228/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2991 - acc: 0.8942 - val_loss: 0.4072 - val_acc: 0.8327\n",
      "Epoch 229/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2970 - acc: 0.8933 - val_loss: 0.4143 - val_acc: 0.8327\n",
      "Epoch 230/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3018 - acc: 0.8934 - val_loss: 0.4085 - val_acc: 0.8327\n",
      "Epoch 231/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2957 - acc: 0.8937 - val_loss: 0.4129 - val_acc: 0.8327\n",
      "Epoch 232/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2964 - acc: 0.8941 - val_loss: 0.4049 - val_acc: 0.8347\n",
      "Epoch 233/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2969 - acc: 0.8952 - val_loss: 0.4152 - val_acc: 0.8356\n",
      "Epoch 234/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2974 - acc: 0.8943 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 235/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2971 - acc: 0.8942 - val_loss: 0.4134 - val_acc: 0.8327\n",
      "Epoch 236/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2975 - acc: 0.8937 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 237/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2969 - acc: 0.8945 - val_loss: 0.4143 - val_acc: 0.8347\n",
      "Epoch 238/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2948 - acc: 0.8937 - val_loss: 0.4140 - val_acc: 0.8347\n",
      "Epoch 239/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2997 - acc: 0.8945 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 240/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2980 - acc: 0.8943 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 241/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2980 - acc: 0.8936 - val_loss: 0.4213 - val_acc: 0.8356\n",
      "Epoch 242/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2971 - acc: 0.8935 - val_loss: 0.4044 - val_acc: 0.8347\n",
      "Epoch 243/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2984 - acc: 0.8939 - val_loss: 0.4141 - val_acc: 0.8327\n",
      "Epoch 244/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2973 - acc: 0.8940 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 245/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2973 - acc: 0.8941 - val_loss: 0.4168 - val_acc: 0.8347\n",
      "Epoch 246/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2953 - acc: 0.8946 - val_loss: 0.4142 - val_acc: 0.8327\n",
      "Epoch 247/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2969 - acc: 0.8942 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 248/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2970 - acc: 0.8942 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 249/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2978 - acc: 0.8941 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 250/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2966 - acc: 0.8939 - val_loss: 0.4109 - val_acc: 0.8347\n",
      "Epoch 251/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2966 - acc: 0.8934 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 252/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2961 - acc: 0.8941 - val_loss: 0.4150 - val_acc: 0.8356\n",
      "Epoch 253/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2976 - acc: 0.8946 - val_loss: 0.4271 - val_acc: 0.8356\n",
      "Epoch 254/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2971 - acc: 0.8939 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 255/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2979 - acc: 0.8942 - val_loss: 0.4105 - val_acc: 0.8347\n",
      "Epoch 256/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2976 - acc: 0.8944 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 257/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2967 - acc: 0.8944 - val_loss: 0.4071 - val_acc: 0.8327\n",
      "Epoch 258/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2954 - acc: 0.8948 - val_loss: 0.4143 - val_acc: 0.8347\n",
      "Epoch 259/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2983 - acc: 0.8942 - val_loss: 0.4094 - val_acc: 0.8327\n",
      "Epoch 260/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2968 - acc: 0.8947 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 261/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2961 - acc: 0.8944 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 262/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2988 - acc: 0.8941 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 263/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2957 - acc: 0.8941 - val_loss: 0.4137 - val_acc: 0.8347\n",
      "Epoch 264/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2968 - acc: 0.8946 - val_loss: 0.4413 - val_acc: 0.8396\n",
      "Epoch 265/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2948 - acc: 0.8937 - val_loss: 0.4072 - val_acc: 0.8327\n",
      "Epoch 266/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2949 - acc: 0.8944 - val_loss: 0.4107 - val_acc: 0.8347\n",
      "Epoch 267/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2959 - acc: 0.8948 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 268/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2951 - acc: 0.8944 - val_loss: 0.4110 - val_acc: 0.8327\n",
      "Epoch 269/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2963 - acc: 0.8942 - val_loss: 0.4075 - val_acc: 0.8327\n",
      "Epoch 270/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2961 - acc: 0.8946 - val_loss: 0.4015 - val_acc: 0.8327\n",
      "Epoch 271/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2976 - acc: 0.8945 - val_loss: 0.4050 - val_acc: 0.8327\n",
      "Epoch 272/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2949 - acc: 0.8943 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 273/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2991 - acc: 0.8944 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 274/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2947 - acc: 0.8948 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 275/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2959 - acc: 0.8948 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 276/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2960 - acc: 0.8939 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 277/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2957 - acc: 0.8941 - val_loss: 0.4022 - val_acc: 0.8327\n",
      "Epoch 278/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2965 - acc: 0.8947 - val_loss: 0.4034 - val_acc: 0.8327\n",
      "Epoch 279/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2957 - acc: 0.8943 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 280/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2956 - acc: 0.8944 - val_loss: 0.4024 - val_acc: 0.8327\n",
      "Epoch 281/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2970 - acc: 0.8943 - val_loss: 0.4098 - val_acc: 0.8327\n",
      "Epoch 282/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2972 - acc: 0.8944 - val_loss: 0.4067 - val_acc: 0.8327\n",
      "Epoch 283/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2944 - acc: 0.8942 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 284/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2944 - acc: 0.8940 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 285/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2960 - acc: 0.8951 - val_loss: 0.4162 - val_acc: 0.8347\n",
      "Epoch 286/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8953 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 287/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2961 - acc: 0.8946 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 288/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2964 - acc: 0.8950 - val_loss: 0.4115 - val_acc: 0.8347\n",
      "Epoch 289/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2958 - acc: 0.8945 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 290/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2976 - acc: 0.8942 - val_loss: 0.4055 - val_acc: 0.8327\n",
      "Epoch 291/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2972 - acc: 0.8936 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 292/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2966 - acc: 0.8942 - val_loss: 0.4051 - val_acc: 0.8347\n",
      "Epoch 293/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2967 - acc: 0.8943 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 294/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2960 - acc: 0.8941 - val_loss: 0.4098 - val_acc: 0.8347\n",
      "Epoch 295/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2925 - acc: 0.8947 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 296/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2956 - acc: 0.8945 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 297/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2939 - acc: 0.8946 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 298/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2958 - acc: 0.8943 - val_loss: 0.4066 - val_acc: 0.8327\n",
      "Epoch 299/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2941 - acc: 0.8947 - val_loss: 0.4129 - val_acc: 0.8347\n",
      "Epoch 300/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2959 - acc: 0.8948 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 301/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2966 - acc: 0.8941 - val_loss: 0.4079 - val_acc: 0.8327\n",
      "Epoch 302/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2939 - acc: 0.8947 - val_loss: 0.4055 - val_acc: 0.8327\n",
      "Epoch 303/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2956 - acc: 0.8950 - val_loss: 0.4163 - val_acc: 0.8347\n",
      "Epoch 304/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2951 - acc: 0.8939 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 305/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2950 - acc: 0.8947 - val_loss: 0.4072 - val_acc: 0.8347\n",
      "Epoch 306/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2935 - acc: 0.8945 - val_loss: 0.4169 - val_acc: 0.8347\n",
      "Epoch 307/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2944 - acc: 0.8942 - val_loss: 0.4125 - val_acc: 0.8347\n",
      "Epoch 308/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2952 - acc: 0.8946 - val_loss: 0.4315 - val_acc: 0.8347\n",
      "Epoch 309/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2942 - acc: 0.8948 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 310/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2986 - acc: 0.8937 - val_loss: 0.4128 - val_acc: 0.8347\n",
      "Epoch 311/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8942 - val_loss: 0.4315 - val_acc: 0.8366\n",
      "Epoch 312/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2947 - acc: 0.8948 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 313/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2936 - acc: 0.8951 - val_loss: 0.3999 - val_acc: 0.8327\n",
      "Epoch 314/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2925 - acc: 0.8944 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 315/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2977 - acc: 0.8944 - val_loss: 0.4164 - val_acc: 0.8356\n",
      "Epoch 316/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2956 - acc: 0.8944 - val_loss: 0.4058 - val_acc: 0.8327\n",
      "Epoch 317/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2933 - acc: 0.8947 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 318/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2946 - acc: 0.8946 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 319/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2937 - acc: 0.8945 - val_loss: 0.4103 - val_acc: 0.8347\n",
      "Epoch 320/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2943 - acc: 0.8946 - val_loss: 0.4178 - val_acc: 0.8356\n",
      "Epoch 321/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2952 - acc: 0.8945 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 322/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2956 - acc: 0.8945 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 323/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2957 - acc: 0.8948 - val_loss: 0.4102 - val_acc: 0.8327\n",
      "Epoch 324/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2941 - acc: 0.8947 - val_loss: 0.4218 - val_acc: 0.8356\n",
      "Epoch 325/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2965 - acc: 0.8942 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 326/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8940 - val_loss: 0.4107 - val_acc: 0.8347\n",
      "Epoch 327/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2943 - acc: 0.8942 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 328/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2938 - acc: 0.8944 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 329/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2952 - acc: 0.8939 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 330/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2930 - acc: 0.8948 - val_loss: 0.4049 - val_acc: 0.8347\n",
      "Epoch 331/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2952 - acc: 0.8946 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 332/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2937 - acc: 0.8939 - val_loss: 0.4113 - val_acc: 0.8347\n",
      "Epoch 333/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2943 - acc: 0.8947 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 334/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2950 - acc: 0.8943 - val_loss: 0.4087 - val_acc: 0.8327\n",
      "Epoch 335/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2948 - acc: 0.8939 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 336/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2942 - acc: 0.8944 - val_loss: 0.4106 - val_acc: 0.8347\n",
      "Epoch 337/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2959 - acc: 0.8934 - val_loss: 0.4043 - val_acc: 0.8327\n",
      "Epoch 338/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2952 - acc: 0.8936 - val_loss: 0.4087 - val_acc: 0.8327\n",
      "Epoch 339/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2969 - acc: 0.8943 - val_loss: 0.4054 - val_acc: 0.8327\n",
      "Epoch 340/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2954 - acc: 0.8945 - val_loss: 0.4071 - val_acc: 0.8327\n",
      "Epoch 341/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2951 - acc: 0.8943 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 342/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2926 - acc: 0.8943 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 343/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2957 - acc: 0.8939 - val_loss: 0.4026 - val_acc: 0.8327\n",
      "Epoch 344/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8950 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 345/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8942 - val_loss: 0.4033 - val_acc: 0.8327\n",
      "Epoch 346/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2945 - acc: 0.8943 - val_loss: 0.4107 - val_acc: 0.8347\n",
      "Epoch 347/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2936 - acc: 0.8942 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 348/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2954 - acc: 0.8947 - val_loss: 0.4062 - val_acc: 0.8327\n",
      "Epoch 349/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2949 - acc: 0.8942 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 350/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2940 - acc: 0.8945 - val_loss: 0.3982 - val_acc: 0.8327\n",
      "Epoch 351/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2948 - acc: 0.8937 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 352/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2938 - acc: 0.8940 - val_loss: 0.4049 - val_acc: 0.8347\n",
      "Epoch 353/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2947 - acc: 0.8939 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 354/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2929 - acc: 0.8933 - val_loss: 0.4109 - val_acc: 0.8347\n",
      "Epoch 355/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2944 - acc: 0.8936 - val_loss: 0.4105 - val_acc: 0.8347\n",
      "Epoch 356/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8945 - val_loss: 0.4279 - val_acc: 0.8347\n",
      "Epoch 357/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2954 - acc: 0.8941 - val_loss: 0.4089 - val_acc: 0.8347\n",
      "Epoch 358/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2913 - acc: 0.8947 - val_loss: 0.4092 - val_acc: 0.8327\n",
      "Epoch 359/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2967 - acc: 0.8947 - val_loss: 0.4126 - val_acc: 0.8347\n",
      "Epoch 360/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2940 - acc: 0.8948 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 361/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2920 - acc: 0.8948 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 362/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2948 - acc: 0.8939 - val_loss: 0.4169 - val_acc: 0.8347\n",
      "Epoch 363/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2939 - acc: 0.8945 - val_loss: 0.4014 - val_acc: 0.8347\n",
      "Epoch 364/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2929 - acc: 0.8950 - val_loss: 0.4150 - val_acc: 0.8356\n",
      "Epoch 365/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2941 - acc: 0.8944 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 366/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2932 - acc: 0.8950 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 367/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2935 - acc: 0.8950 - val_loss: 0.4130 - val_acc: 0.8347\n",
      "Epoch 368/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2938 - acc: 0.8943 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 369/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2944 - acc: 0.8941 - val_loss: 0.4160 - val_acc: 0.8327\n",
      "Epoch 370/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2959 - acc: 0.8943 - val_loss: 0.4245 - val_acc: 0.8356\n",
      "Epoch 371/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2936 - acc: 0.8950 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 372/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2937 - acc: 0.8950 - val_loss: 0.4051 - val_acc: 0.8347\n",
      "Epoch 373/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2967 - acc: 0.8956 - val_loss: 0.4185 - val_acc: 0.8356\n",
      "Epoch 374/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2934 - acc: 0.8946 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 375/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8945 - val_loss: 0.4055 - val_acc: 0.8327\n",
      "Epoch 376/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2940 - acc: 0.8947 - val_loss: 0.4185 - val_acc: 0.8327\n",
      "Epoch 377/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2965 - acc: 0.8941 - val_loss: 0.4081 - val_acc: 0.8347\n",
      "Epoch 378/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2939 - acc: 0.8948 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 379/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2925 - acc: 0.8946 - val_loss: 0.3976 - val_acc: 0.8327\n",
      "Epoch 380/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2936 - acc: 0.8948 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 381/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2948 - acc: 0.8941 - val_loss: 0.4097 - val_acc: 0.8347\n",
      "Epoch 382/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2949 - acc: 0.8946 - val_loss: 0.4075 - val_acc: 0.8347\n",
      "Epoch 383/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2934 - acc: 0.8942 - val_loss: 0.3990 - val_acc: 0.8347\n",
      "Epoch 384/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2946 - acc: 0.8934 - val_loss: 0.4198 - val_acc: 0.8356\n",
      "Epoch 385/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2951 - acc: 0.8957 - val_loss: 0.4116 - val_acc: 0.8347\n",
      "Epoch 386/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2948 - acc: 0.8942 - val_loss: 0.4092 - val_acc: 0.8347\n",
      "Epoch 387/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2926 - acc: 0.8942 - val_loss: 0.4029 - val_acc: 0.8337\n",
      "Epoch 388/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2938 - acc: 0.8948 - val_loss: 0.4078 - val_acc: 0.8337\n",
      "Epoch 389/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2931 - acc: 0.8947 - val_loss: 0.4336 - val_acc: 0.8386\n",
      "Epoch 390/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2956 - acc: 0.8943 - val_loss: 0.4179 - val_acc: 0.8356\n",
      "Epoch 391/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2926 - acc: 0.8950 - val_loss: 0.4069 - val_acc: 0.8347\n",
      "Epoch 392/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2924 - acc: 0.8953 - val_loss: 0.4038 - val_acc: 0.8347\n",
      "Epoch 393/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2930 - acc: 0.8939 - val_loss: 0.4109 - val_acc: 0.8327\n",
      "Epoch 394/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2937 - acc: 0.8943 - val_loss: 0.4098 - val_acc: 0.8347\n",
      "Epoch 395/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2922 - acc: 0.8954 - val_loss: 0.4062 - val_acc: 0.8347\n",
      "Epoch 396/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2924 - acc: 0.8954 - val_loss: 0.4113 - val_acc: 0.8347\n",
      "Epoch 397/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2924 - acc: 0.8951 - val_loss: 0.4133 - val_acc: 0.8356\n",
      "Epoch 398/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2946 - acc: 0.8935 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 399/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2920 - acc: 0.8950 - val_loss: 0.4106 - val_acc: 0.8327\n",
      "Epoch 400/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2943 - acc: 0.8941 - val_loss: 0.4023 - val_acc: 0.8347\n",
      "Epoch 401/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2925 - acc: 0.8944 - val_loss: 0.4049 - val_acc: 0.8327\n",
      "Epoch 402/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2926 - acc: 0.8946 - val_loss: 0.4135 - val_acc: 0.8347\n",
      "Epoch 403/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2930 - acc: 0.8950 - val_loss: 0.4093 - val_acc: 0.8337\n",
      "Epoch 404/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2935 - acc: 0.8946 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 405/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2937 - acc: 0.8943 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 406/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2924 - acc: 0.8946 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 407/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2956 - acc: 0.8945 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 408/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2922 - acc: 0.8941 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 409/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8946 - val_loss: 0.4092 - val_acc: 0.8327\n",
      "Epoch 410/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2925 - acc: 0.8935 - val_loss: 0.4079 - val_acc: 0.8347\n",
      "Epoch 411/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2936 - acc: 0.8944 - val_loss: 0.4083 - val_acc: 0.8347\n",
      "Epoch 412/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2933 - acc: 0.8941 - val_loss: 0.4014 - val_acc: 0.8327\n",
      "Epoch 413/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2933 - acc: 0.8947 - val_loss: 0.4004 - val_acc: 0.8327\n",
      "Epoch 414/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2937 - acc: 0.8946 - val_loss: 0.4076 - val_acc: 0.8347\n",
      "Epoch 415/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8954 - val_loss: 0.4090 - val_acc: 0.8347\n",
      "Epoch 416/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2931 - acc: 0.8952 - val_loss: 0.4134 - val_acc: 0.8347\n",
      "Epoch 417/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2926 - acc: 0.8944 - val_loss: 0.4024 - val_acc: 0.8327\n",
      "Epoch 418/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2951 - acc: 0.8942 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 419/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2933 - acc: 0.8947 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 420/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2904 - acc: 0.8945 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 421/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2935 - acc: 0.8943 - val_loss: 0.4033 - val_acc: 0.8347\n",
      "Epoch 422/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2930 - acc: 0.8941 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 423/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2926 - acc: 0.8956 - val_loss: 0.4112 - val_acc: 0.8327\n",
      "Epoch 424/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2907 - acc: 0.8950 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 425/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2936 - acc: 0.8948 - val_loss: 0.4032 - val_acc: 0.8347\n",
      "Epoch 426/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2939 - acc: 0.8947 - val_loss: 0.4111 - val_acc: 0.8347\n",
      "Epoch 427/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2931 - acc: 0.8934 - val_loss: 0.4036 - val_acc: 0.8337\n",
      "Epoch 428/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2934 - acc: 0.8955 - val_loss: 0.4199 - val_acc: 0.8356\n",
      "Epoch 429/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2929 - acc: 0.8950 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 430/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2915 - acc: 0.8948 - val_loss: 0.4098 - val_acc: 0.8356\n",
      "Epoch 431/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2922 - acc: 0.8953 - val_loss: 0.4077 - val_acc: 0.8327\n",
      "Epoch 432/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2935 - acc: 0.8941 - val_loss: 0.4022 - val_acc: 0.8327\n",
      "Epoch 433/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2906 - acc: 0.8942 - val_loss: 0.4404 - val_acc: 0.8386\n",
      "Epoch 434/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2952 - acc: 0.8941 - val_loss: 0.4143 - val_acc: 0.8356\n",
      "Epoch 435/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8943 - val_loss: 0.4163 - val_acc: 0.8347\n",
      "Epoch 436/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2918 - acc: 0.8946 - val_loss: 0.4131 - val_acc: 0.8356\n",
      "Epoch 437/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2949 - acc: 0.8947 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 438/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8944 - val_loss: 0.4120 - val_acc: 0.8356\n",
      "Epoch 439/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2935 - acc: 0.8946 - val_loss: 0.4118 - val_acc: 0.8356\n",
      "Epoch 440/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2912 - acc: 0.8944 - val_loss: 0.4116 - val_acc: 0.8356\n",
      "Epoch 441/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2937 - acc: 0.8935 - val_loss: 0.4031 - val_acc: 0.8327\n",
      "Epoch 442/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2934 - acc: 0.8939 - val_loss: 0.4143 - val_acc: 0.8347\n",
      "Epoch 443/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8952 - val_loss: 0.4070 - val_acc: 0.8347\n",
      "Epoch 444/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8948 - val_loss: 0.4189 - val_acc: 0.8366\n",
      "Epoch 445/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2910 - acc: 0.8954 - val_loss: 0.4038 - val_acc: 0.8347\n",
      "Epoch 446/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2945 - acc: 0.8939 - val_loss: 0.4126 - val_acc: 0.8356\n",
      "Epoch 447/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2933 - acc: 0.8944 - val_loss: 0.4029 - val_acc: 0.8347\n",
      "Epoch 448/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2922 - acc: 0.8947 - val_loss: 0.4096 - val_acc: 0.8347\n",
      "Epoch 449/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2926 - acc: 0.8939 - val_loss: 0.4059 - val_acc: 0.8356\n",
      "Epoch 450/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2910 - acc: 0.8955 - val_loss: 0.4061 - val_acc: 0.8356\n",
      "Epoch 451/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2926 - acc: 0.8948 - val_loss: 0.4180 - val_acc: 0.8366\n",
      "Epoch 452/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2909 - acc: 0.8951 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 453/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2903 - acc: 0.8945 - val_loss: 0.4178 - val_acc: 0.8356\n",
      "Epoch 454/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8946 - val_loss: 0.4109 - val_acc: 0.8347\n",
      "Epoch 455/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8947 - val_loss: 0.4143 - val_acc: 0.8356\n",
      "Epoch 456/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2929 - acc: 0.8946 - val_loss: 0.4034 - val_acc: 0.8347\n",
      "Epoch 457/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2927 - acc: 0.8950 - val_loss: 0.4092 - val_acc: 0.8356\n",
      "Epoch 458/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2924 - acc: 0.8943 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 459/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2927 - acc: 0.8940 - val_loss: 0.4094 - val_acc: 0.8347\n",
      "Epoch 460/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2927 - acc: 0.8951 - val_loss: 0.4112 - val_acc: 0.8347\n",
      "Epoch 461/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2914 - acc: 0.8952 - val_loss: 0.4040 - val_acc: 0.8337\n",
      "Epoch 462/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2937 - acc: 0.8943 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 463/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8951 - val_loss: 0.4117 - val_acc: 0.8366\n",
      "Epoch 464/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2909 - acc: 0.8947 - val_loss: 0.4116 - val_acc: 0.8356\n",
      "Epoch 465/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2918 - acc: 0.8950 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 466/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2916 - acc: 0.8956 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 467/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2946 - acc: 0.8944 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 468/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2925 - acc: 0.8950 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 469/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2926 - acc: 0.8947 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 470/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2901 - acc: 0.8944 - val_loss: 0.4003 - val_acc: 0.8327\n",
      "Epoch 471/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2922 - acc: 0.8950 - val_loss: 0.4032 - val_acc: 0.8327\n",
      "Epoch 472/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.8946 - val_loss: 0.4027 - val_acc: 0.8337\n",
      "Epoch 473/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2897 - acc: 0.8950 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 474/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.8937 - val_loss: 0.4047 - val_acc: 0.8356\n",
      "Epoch 475/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8947 - val_loss: 0.4035 - val_acc: 0.8347\n",
      "Epoch 476/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2917 - acc: 0.8944 - val_loss: 0.4142 - val_acc: 0.8327\n",
      "Epoch 477/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2929 - acc: 0.8945 - val_loss: 0.4089 - val_acc: 0.8347\n",
      "Epoch 478/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2917 - acc: 0.8945 - val_loss: 0.3990 - val_acc: 0.8327\n",
      "Epoch 479/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2911 - acc: 0.8947 - val_loss: 0.4010 - val_acc: 0.8347\n",
      "Epoch 480/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2919 - acc: 0.8950 - val_loss: 0.4005 - val_acc: 0.8327\n",
      "Epoch 481/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2915 - acc: 0.8956 - val_loss: 0.4026 - val_acc: 0.8347\n",
      "Epoch 482/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2903 - acc: 0.8957 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 483/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2903 - acc: 0.8951 - val_loss: 0.4075 - val_acc: 0.8347\n",
      "Epoch 484/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2936 - acc: 0.8945 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 485/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2930 - acc: 0.8948 - val_loss: 0.4239 - val_acc: 0.8337\n",
      "Epoch 486/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2926 - acc: 0.8952 - val_loss: 0.4094 - val_acc: 0.8347\n",
      "Epoch 487/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2899 - acc: 0.8947 - val_loss: 0.4101 - val_acc: 0.8327\n",
      "Epoch 488/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2901 - acc: 0.8952 - val_loss: 0.4083 - val_acc: 0.8327\n",
      "Epoch 489/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2901 - acc: 0.8952 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 490/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2916 - acc: 0.8939 - val_loss: 0.4164 - val_acc: 0.8356\n",
      "Epoch 491/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2903 - acc: 0.8947 - val_loss: 0.4084 - val_acc: 0.8347\n",
      "Epoch 492/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2910 - acc: 0.8941 - val_loss: 0.3959 - val_acc: 0.8347\n",
      "Epoch 493/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2907 - acc: 0.8942 - val_loss: 0.4134 - val_acc: 0.8356\n",
      "Epoch 494/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8955 - val_loss: 0.4203 - val_acc: 0.8356\n",
      "Epoch 495/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2904 - acc: 0.8950 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 496/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2893 - acc: 0.8946 - val_loss: 0.4069 - val_acc: 0.8347\n",
      "Epoch 497/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2916 - acc: 0.8950 - val_loss: 0.4012 - val_acc: 0.8366\n",
      "Epoch 498/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2905 - acc: 0.8947 - val_loss: 0.4031 - val_acc: 0.8337\n",
      "Epoch 499/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2909 - acc: 0.8952 - val_loss: 0.4151 - val_acc: 0.8356\n",
      "Epoch 500/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2922 - acc: 0.8947 - val_loss: 0.4049 - val_acc: 0.8327\n",
      "Epoch 501/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2939 - acc: 0.8939 - val_loss: 0.4048 - val_acc: 0.8347\n",
      "Epoch 502/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2917 - acc: 0.8945 - val_loss: 0.4111 - val_acc: 0.8327\n",
      "Epoch 503/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2910 - acc: 0.8940 - val_loss: 0.4092 - val_acc: 0.8347\n",
      "Epoch 504/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2910 - acc: 0.8955 - val_loss: 0.4091 - val_acc: 0.8347\n",
      "Epoch 505/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2921 - acc: 0.8958 - val_loss: 0.3994 - val_acc: 0.8327\n",
      "Epoch 506/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2900 - acc: 0.8944 - val_loss: 0.4021 - val_acc: 0.8347\n",
      "Epoch 507/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2911 - acc: 0.8943 - val_loss: 0.4129 - val_acc: 0.8347\n",
      "Epoch 508/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2913 - acc: 0.8946 - val_loss: 0.4017 - val_acc: 0.8327\n",
      "Epoch 509/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2908 - acc: 0.8944 - val_loss: 0.4006 - val_acc: 0.8327\n",
      "Epoch 510/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2919 - acc: 0.8953 - val_loss: 0.4044 - val_acc: 0.8356\n",
      "Epoch 511/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2928 - acc: 0.8946 - val_loss: 0.4088 - val_acc: 0.8356\n",
      "Epoch 512/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2901 - acc: 0.8953 - val_loss: 0.3989 - val_acc: 0.8337\n",
      "Epoch 513/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2918 - acc: 0.8950 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 514/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2903 - acc: 0.8944 - val_loss: 0.4078 - val_acc: 0.8347\n",
      "Epoch 515/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8947 - val_loss: 0.4034 - val_acc: 0.8337\n",
      "Epoch 516/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2909 - acc: 0.8946 - val_loss: 0.3920 - val_acc: 0.8327\n",
      "Epoch 517/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2882 - acc: 0.8951 - val_loss: 0.4049 - val_acc: 0.8347\n",
      "Epoch 518/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2911 - acc: 0.8952 - val_loss: 0.4043 - val_acc: 0.8337\n",
      "Epoch 519/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2906 - acc: 0.8947 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 520/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2916 - acc: 0.8948 - val_loss: 0.3997 - val_acc: 0.8327\n",
      "Epoch 521/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2914 - acc: 0.8947 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 522/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2926 - acc: 0.8941 - val_loss: 0.4105 - val_acc: 0.8356\n",
      "Epoch 523/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2886 - acc: 0.8946 - val_loss: 0.4038 - val_acc: 0.8347\n",
      "Epoch 524/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2884 - acc: 0.8952 - val_loss: 0.4228 - val_acc: 0.8356\n",
      "Epoch 525/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2912 - acc: 0.8954 - val_loss: 0.4042 - val_acc: 0.8356\n",
      "Epoch 526/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2901 - acc: 0.8951 - val_loss: 0.4177 - val_acc: 0.8366\n",
      "Epoch 527/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2908 - acc: 0.8954 - val_loss: 0.4143 - val_acc: 0.8356\n",
      "Epoch 528/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2904 - acc: 0.8951 - val_loss: 0.4033 - val_acc: 0.8337\n",
      "Epoch 529/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2891 - acc: 0.8943 - val_loss: 0.4002 - val_acc: 0.8347\n",
      "Epoch 530/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2903 - acc: 0.8952 - val_loss: 0.4026 - val_acc: 0.8327\n",
      "Epoch 531/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2897 - acc: 0.8951 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 532/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2920 - acc: 0.8942 - val_loss: 0.3966 - val_acc: 0.8347\n",
      "Epoch 533/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2914 - acc: 0.8945 - val_loss: 0.4090 - val_acc: 0.8356\n",
      "Epoch 534/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2907 - acc: 0.8943 - val_loss: 0.4069 - val_acc: 0.8356\n",
      "Epoch 535/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2909 - acc: 0.8953 - val_loss: 0.3994 - val_acc: 0.8327\n",
      "Epoch 536/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2928 - acc: 0.8947 - val_loss: 0.4025 - val_acc: 0.8376\n",
      "Epoch 537/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2915 - acc: 0.8957 - val_loss: 0.4097 - val_acc: 0.8327\n",
      "Epoch 538/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2892 - acc: 0.8957 - val_loss: 0.3957 - val_acc: 0.8337\n",
      "Epoch 539/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2902 - acc: 0.8946 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 540/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2906 - acc: 0.8953 - val_loss: 0.3993 - val_acc: 0.8327\n",
      "Epoch 541/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2903 - acc: 0.8952 - val_loss: 0.4054 - val_acc: 0.8327\n",
      "Epoch 542/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2903 - acc: 0.8955 - val_loss: 0.4002 - val_acc: 0.8356\n",
      "Epoch 543/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2899 - acc: 0.8946 - val_loss: 0.4078 - val_acc: 0.8356\n",
      "Epoch 544/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2928 - acc: 0.8946 - val_loss: 0.4190 - val_acc: 0.8376\n",
      "Epoch 545/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2904 - acc: 0.8945 - val_loss: 0.4115 - val_acc: 0.8356\n",
      "Epoch 546/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2912 - acc: 0.8952 - val_loss: 0.4091 - val_acc: 0.8327\n",
      "Epoch 547/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2888 - acc: 0.8948 - val_loss: 0.4022 - val_acc: 0.8327\n",
      "Epoch 548/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2908 - acc: 0.8943 - val_loss: 0.4216 - val_acc: 0.8356\n",
      "Epoch 549/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2919 - acc: 0.8936 - val_loss: 0.4034 - val_acc: 0.8327\n",
      "Epoch 550/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2920 - acc: 0.8953 - val_loss: 0.4026 - val_acc: 0.8356\n",
      "Epoch 551/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2895 - acc: 0.8956 - val_loss: 0.4053 - val_acc: 0.8356\n",
      "Epoch 552/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2897 - acc: 0.8942 - val_loss: 0.4010 - val_acc: 0.8347\n",
      "Epoch 553/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2905 - acc: 0.8956 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 554/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2900 - acc: 0.8946 - val_loss: 0.4037 - val_acc: 0.8327\n",
      "Epoch 555/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2923 - acc: 0.8948 - val_loss: 0.4063 - val_acc: 0.8347\n",
      "Epoch 556/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2899 - acc: 0.8950 - val_loss: 0.4179 - val_acc: 0.8376\n",
      "Epoch 557/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2907 - acc: 0.8944 - val_loss: 0.4111 - val_acc: 0.8347\n",
      "Epoch 558/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2898 - acc: 0.8948 - val_loss: 0.4129 - val_acc: 0.8347\n",
      "Epoch 559/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2898 - acc: 0.8962 - val_loss: 0.4001 - val_acc: 0.8347\n",
      "Epoch 560/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2882 - acc: 0.8951 - val_loss: 0.3946 - val_acc: 0.8317\n",
      "Epoch 561/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2905 - acc: 0.8946 - val_loss: 0.3998 - val_acc: 0.8356\n",
      "Epoch 562/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2925 - acc: 0.8941 - val_loss: 0.3980 - val_acc: 0.8347\n",
      "Epoch 563/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2923 - acc: 0.8943 - val_loss: 0.4123 - val_acc: 0.8366\n",
      "Epoch 564/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2893 - acc: 0.8955 - val_loss: 0.3958 - val_acc: 0.8347\n",
      "Epoch 565/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.8964 - val_loss: 0.3996 - val_acc: 0.8327\n",
      "Epoch 566/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2884 - acc: 0.8950 - val_loss: 0.4061 - val_acc: 0.8347\n",
      "Epoch 567/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2891 - acc: 0.8955 - val_loss: 0.4077 - val_acc: 0.8327\n",
      "Epoch 568/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2893 - acc: 0.8954 - val_loss: 0.4133 - val_acc: 0.8337\n",
      "Epoch 569/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2886 - acc: 0.8952 - val_loss: 0.4015 - val_acc: 0.8356\n",
      "Epoch 570/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2894 - acc: 0.8952 - val_loss: 0.4304 - val_acc: 0.8386\n",
      "Epoch 571/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2910 - acc: 0.8953 - val_loss: 0.4052 - val_acc: 0.8327\n",
      "Epoch 572/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2907 - acc: 0.8940 - val_loss: 0.4050 - val_acc: 0.8347\n",
      "Epoch 573/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2898 - acc: 0.8941 - val_loss: 0.4155 - val_acc: 0.8386\n",
      "Epoch 574/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2888 - acc: 0.8942 - val_loss: 0.4048 - val_acc: 0.8347\n",
      "Epoch 575/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2904 - acc: 0.8952 - val_loss: 0.4045 - val_acc: 0.8376\n",
      "Epoch 576/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2883 - acc: 0.8956 - val_loss: 0.4002 - val_acc: 0.8356\n",
      "Epoch 577/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2909 - acc: 0.8952 - val_loss: 0.4020 - val_acc: 0.8376\n",
      "Epoch 578/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2903 - acc: 0.8963 - val_loss: 0.3980 - val_acc: 0.8356\n",
      "Epoch 579/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.2901 - acc: 0.8948 - val_loss: 0.4133 - val_acc: 0.8356\n",
      "Epoch 580/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2922 - acc: 0.8945 - val_loss: 0.4025 - val_acc: 0.8337\n",
      "Epoch 581/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2901 - acc: 0.8950 - val_loss: 0.4015 - val_acc: 0.8337\n",
      "Epoch 582/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2878 - acc: 0.8952 - val_loss: 0.4299 - val_acc: 0.8366\n",
      "Epoch 583/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2923 - acc: 0.8948 - val_loss: 0.4072 - val_acc: 0.8327\n",
      "Epoch 584/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2900 - acc: 0.8947 - val_loss: 0.3997 - val_acc: 0.8327\n",
      "Epoch 585/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2890 - acc: 0.8956 - val_loss: 0.4130 - val_acc: 0.8337\n",
      "Epoch 586/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.2895 - acc: 0.8963 - val_loss: 0.4092 - val_acc: 0.8327\n",
      "Epoch 587/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2895 - acc: 0.8954 - val_loss: 0.4078 - val_acc: 0.8327\n",
      "Epoch 588/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2893 - acc: 0.8953 - val_loss: 0.3999 - val_acc: 0.8356\n",
      "Epoch 589/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2903 - acc: 0.8955 - val_loss: 0.4090 - val_acc: 0.8347\n",
      "Epoch 590/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2915 - acc: 0.8954 - val_loss: 0.4010 - val_acc: 0.8356\n",
      "Epoch 591/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2901 - acc: 0.8957 - val_loss: 0.4115 - val_acc: 0.8366\n",
      "Epoch 592/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2889 - acc: 0.8948 - val_loss: 0.4115 - val_acc: 0.8366\n",
      "Epoch 593/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2902 - acc: 0.8951 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 594/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2901 - acc: 0.8952 - val_loss: 0.4052 - val_acc: 0.8386\n",
      "Epoch 595/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2885 - acc: 0.8942 - val_loss: 0.4037 - val_acc: 0.8366\n",
      "Epoch 596/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.2888 - acc: 0.8941 - val_loss: 0.3989 - val_acc: 0.8356\n",
      "Epoch 597/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2920 - acc: 0.8953 - val_loss: 0.4066 - val_acc: 0.8337\n",
      "Epoch 598/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2900 - acc: 0.8947 - val_loss: 0.4067 - val_acc: 0.8356\n",
      "Epoch 599/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2897 - acc: 0.8956 - val_loss: 0.4093 - val_acc: 0.8366\n",
      "Epoch 600/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2885 - acc: 0.8954 - val_loss: 0.3944 - val_acc: 0.8396\n"
     ]
    }
   ],
   "source": [
    "# Fitting our model \n",
    "hist = classifier.fit(scaled_train_x, train_y, batch_size = 150, epochs = 600, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7cc8abd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation3/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c7a522e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation3/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with development test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (726)\n",
      "CorrectReject    637\n",
      "GrossFalseAccept 22*3.0 = 66.0\n",
      "PlainFalseAccept 67\n",
      "RejectionRate    0.827\n",
      "\n",
      "CORRECT UTTERANCES (1798)\n",
      "CorrectAccept    1628\n",
      "FalseReject      170\n",
      "RejectionRate    0.095\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.924\n",
      "F                             0.915\n",
      "Sa                            0.905\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             8.750\n",
      "Da                            5.242\n",
      "Df                            6.772\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred = classifier.predict_classes(scaled_dev_test_x)\n",
    "evaluate(dev_test_y, dev_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with st2 test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (250)\n",
      "CorrectReject    230\n",
      "GrossFalseAccept 1*3.0 = 3.0\n",
      "PlainFalseAccept 19\n",
      "RejectionRate    0.913\n",
      "\n",
      "CORRECT UTTERANCES (750)\n",
      "CorrectAccept    697\n",
      "FalseReject      53\n",
      "RejectionRate    0.071\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.969\n",
      "F                             0.949\n",
      "Sa                            0.929\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             12.916\n",
      "Da                            10.645\n",
      "Df                            11.725\n"
     ]
    }
   ],
   "source": [
    "st2_y_pred = classifier.predict_classes(scaled_st2_test_x)\n",
    "evaluate(st2_test_y, st2_y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
