{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN_PAPER_V3_Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clearY(y):\n",
    "    clean_input = np.array([]).reshape(0, 1)\n",
    "    for data in y:\n",
    "        pos1 = data[0]\n",
    "        pos2 = data[1]\n",
    "        pos3 = data[2]\n",
    "        if  pos1 == 1 and pos2 == 0 and pos3 ==0:\n",
    "                clean_input = np.vstack((clean_input, [1]))\n",
    "        else:\n",
    "                clean_input = np.vstack((clean_input, [0]))\n",
    "    return clean_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_y, pred_y):\n",
    "    true_classes = []\n",
    "    for array in true_y:\n",
    "        if np.array_equal(array,[1, 0, 0]):\n",
    "            true_classes.append(0)\n",
    "        elif np.array_equal(array,[0, 1, 0]):\n",
    "            true_classes.append(1)\n",
    "        else:\n",
    "            true_classes.append(2)\n",
    "        \n",
    "    CR, CA, PFA, GFA, FR, k = 0, 0, 0, 0, 0, 3.0\n",
    "    for idx, prediction in enumerate(pred_y):\n",
    "        # the students answer is correct in meaning and language\n",
    "        # the system says the same -> accept\n",
    "        if true_classes[idx] == 0 and prediction == 1:\n",
    "            CA += 1\n",
    "        # the system says correct meaning wrong language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "\n",
    "        # students answer is correct in meaning and wrong in language\n",
    "        #The system says the same -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning and correct language -> accept\n",
    "        elif true_classes[idx] == 1 and prediction == 1:\n",
    "            PFA += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "        # students answer is incorrect in meaning and incorrect in language\n",
    "        # the system says the same -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning correct language -> accept\n",
    "        elif true_classes[idx] == 2 and prediction == 1: \n",
    "            GFA += 1\n",
    "        # the system says correct meaning incorrect language -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "    FA = PFA + k * GFA\n",
    "    Correct = CA + FR\n",
    "    Incorrect = CR + GFA + PFA\n",
    "    IncorrectRejectionRate = CR / ( CR + FA + 0.0 )\n",
    "    CorrectRejectionRate = FR / ( FR + CA + 0.0 )\n",
    "    # Further metrics\n",
    "    Z = CA + CR + FA + FR\n",
    "    Ca = CA / Z\n",
    "    Cr = CR / Z\n",
    "    Fa = FA / Z\n",
    "    Fr = FR / Z\n",
    "    \n",
    "    P = Ca / (Ca + Fa)\n",
    "    R = Ca / (Ca + Fr)\n",
    "    SA = Ca + Cr\n",
    "    F = (2 * P * R)/( P + R)\n",
    "    \n",
    "    RCa = Ca / (Fr + Ca)\n",
    "    RFa = Fa / (Cr + Fa)\n",
    "    \n",
    "    D = IncorrectRejectionRate / CorrectRejectionRate\n",
    "    Da = RCa / RFa\n",
    "    Df = math.sqrt((Da*D))\n",
    "    \n",
    "    print('\\nINCORRECT UTTERANCES (' + str(Incorrect) + ')' )\n",
    "    print('CorrectReject    ' + str(CR) )\n",
    "    print('GrossFalseAccept ' + str(GFA) + '*' + str(k) + ' = ' + str(GFA * k) )\n",
    "    print('PlainFalseAccept ' + str(PFA) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(IncorrectRejectionRate) )\n",
    "\n",
    "    print('\\nCORRECT UTTERANCES (' + str(Correct) + ')')\n",
    "    print('CorrectAccept    ' + str(CA) )\n",
    "    print('FalseReject      ' + str(FR) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(CorrectRejectionRate) )\n",
    "    \n",
    "    print('\\n--------------REPORT---------------')\n",
    "    print('-----------------------------------')\n",
    "    print('Pr                            ' +  \"{:.3f}\".format(P) )\n",
    "    print('F                             ' +  \"{:.3f}\".format(F) )\n",
    "    print('Sa                            ' +  \"{:.3f}\".format(SA) )\n",
    "    \n",
    "    print('\\n--------------Metrics--------------')\n",
    "    print('D                             ' +  \"{:.3f}\".format(D) )\n",
    "    print('Da                            ' +  \"{:.3f}\".format(Da) )\n",
    "    print('Df                            ' +  \"{:.3f}\".format(Df) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x:    10092\n",
      "Train_y:    10092\n",
      "\n",
      "\n",
      "Dev_Test_x:    2524\n",
      "Dev_Test_y:    2524\n",
      "\n",
      "\n",
      "St2_Test_x:    1000\n",
      "St2_Test_y:    1000\n"
     ]
    }
   ],
   "source": [
    "train_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_train_x.csv' ,delimiter=',',usecols=range(11)[1:])\n",
    "train_y = clearY(np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_train_y.csv', delimiter=',',usecols=range(4)[1:]))\n",
    "\n",
    "print('Train_x:    ' + str(len(train_x)))\n",
    "print('Train_y:    ' + str(len(train_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "dev_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "dev_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('Dev_Test_x:    ' + str(len(dev_test_x)))\n",
    "print('Dev_Test_y:    ' + str(len(dev_test_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "st2_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_st2_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "st2_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation3/vec_st2_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('St2_Test_x:    ' + str(len(st2_test_x)))\n",
    "print('St2_Test_y:    ' + str(len(st2_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sclae the vectors inorder to get better classification\n",
    "sc = StandardScaler()\n",
    "scaled_train_x = sc.fit_transform(train_x)\n",
    "scaled_dev_test_x = sc.transform(dev_test_x)\n",
    "scaled_st2_test_x = sc.transform(st2_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(64, activation='relu', input_dim=10))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "classifier.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,929\n",
      "Trainable params: 4,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.summary())\n",
    "plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9082 samples, validate on 1010 samples\n",
      "Epoch 1/600\n",
      "9082/9082 [==============================] - 0s 31us/step - loss: 0.5524 - acc: 0.7356 - val_loss: 0.5025 - val_acc: 0.7545\n",
      "Epoch 2/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.4581 - acc: 0.7808 - val_loss: 0.4375 - val_acc: 0.8050\n",
      "Epoch 3/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3849 - acc: 0.8394 - val_loss: 0.4154 - val_acc: 0.8277\n",
      "Epoch 4/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3504 - acc: 0.8643 - val_loss: 0.4151 - val_acc: 0.8337\n",
      "Epoch 5/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3371 - acc: 0.8739 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 6/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3292 - acc: 0.8823 - val_loss: 0.4150 - val_acc: 0.8396\n",
      "Epoch 7/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3254 - acc: 0.8825 - val_loss: 0.4172 - val_acc: 0.8406\n",
      "Epoch 8/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3211 - acc: 0.8882 - val_loss: 0.4203 - val_acc: 0.8396\n",
      "Epoch 9/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3198 - acc: 0.8884 - val_loss: 0.4199 - val_acc: 0.8416\n",
      "Epoch 10/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3193 - acc: 0.8895 - val_loss: 0.4171 - val_acc: 0.8406\n",
      "Epoch 11/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3183 - acc: 0.8899 - val_loss: 0.4203 - val_acc: 0.8386\n",
      "Epoch 12/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3158 - acc: 0.8908 - val_loss: 0.4270 - val_acc: 0.8396\n",
      "Epoch 13/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3133 - acc: 0.8911 - val_loss: 0.4201 - val_acc: 0.8396\n",
      "Epoch 14/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3128 - acc: 0.8917 - val_loss: 0.4267 - val_acc: 0.8386\n",
      "Epoch 15/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8918 - val_loss: 0.4227 - val_acc: 0.8396\n",
      "Epoch 16/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3123 - acc: 0.8914 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 17/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3122 - acc: 0.8924 - val_loss: 0.4256 - val_acc: 0.8406\n",
      "Epoch 18/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3107 - acc: 0.8932 - val_loss: 0.4271 - val_acc: 0.8406\n",
      "Epoch 19/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3115 - acc: 0.8931 - val_loss: 0.4245 - val_acc: 0.8396\n",
      "Epoch 20/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.8930 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 21/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3102 - acc: 0.8934 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 22/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3091 - acc: 0.8929 - val_loss: 0.4268 - val_acc: 0.8386\n",
      "Epoch 23/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3101 - acc: 0.8923 - val_loss: 0.4269 - val_acc: 0.8386\n",
      "Epoch 24/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3092 - acc: 0.8931 - val_loss: 0.4242 - val_acc: 0.8366\n",
      "Epoch 25/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3095 - acc: 0.8932 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 26/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3097 - acc: 0.8920 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 27/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3083 - acc: 0.8932 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 28/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3076 - acc: 0.8933 - val_loss: 0.4279 - val_acc: 0.8366\n",
      "Epoch 29/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3075 - acc: 0.8930 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 30/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3076 - acc: 0.8942 - val_loss: 0.4258 - val_acc: 0.8396\n",
      "Epoch 31/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3096 - acc: 0.8931 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 32/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3087 - acc: 0.8940 - val_loss: 0.4325 - val_acc: 0.8406\n",
      "Epoch 33/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8932 - val_loss: 0.4244 - val_acc: 0.8376\n",
      "Epoch 34/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3079 - acc: 0.8934 - val_loss: 0.4268 - val_acc: 0.8347\n",
      "Epoch 35/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3073 - acc: 0.8934 - val_loss: 0.4333 - val_acc: 0.8406\n",
      "Epoch 36/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3076 - acc: 0.8936 - val_loss: 0.4389 - val_acc: 0.8406\n",
      "Epoch 37/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3077 - acc: 0.8928 - val_loss: 0.4214 - val_acc: 0.8376\n",
      "Epoch 38/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3089 - acc: 0.8937 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 39/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3062 - acc: 0.8936 - val_loss: 0.4283 - val_acc: 0.8366\n",
      "Epoch 40/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3075 - acc: 0.8933 - val_loss: 0.4273 - val_acc: 0.8347\n",
      "Epoch 41/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3062 - acc: 0.8935 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 42/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3069 - acc: 0.8940 - val_loss: 0.4320 - val_acc: 0.8366\n",
      "Epoch 43/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3078 - acc: 0.8932 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 44/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3060 - acc: 0.8931 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 45/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3066 - acc: 0.8937 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 46/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8942 - val_loss: 0.4388 - val_acc: 0.8386\n",
      "Epoch 47/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8942 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 48/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8937 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 49/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3068 - acc: 0.8941 - val_loss: 0.4212 - val_acc: 0.8347\n",
      "Epoch 50/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3067 - acc: 0.8936 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 51/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8935 - val_loss: 0.4362 - val_acc: 0.8366\n",
      "Epoch 52/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8942 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 53/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3041 - acc: 0.8941 - val_loss: 0.4302 - val_acc: 0.8366\n",
      "Epoch 54/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3067 - acc: 0.8931 - val_loss: 0.4288 - val_acc: 0.8376\n",
      "Epoch 55/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8937 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 56/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3047 - acc: 0.8942 - val_loss: 0.4190 - val_acc: 0.8327\n",
      "Epoch 57/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3046 - acc: 0.8939 - val_loss: 0.4333 - val_acc: 0.8366\n",
      "Epoch 58/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3060 - acc: 0.8933 - val_loss: 0.4306 - val_acc: 0.8406\n",
      "Epoch 59/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3056 - acc: 0.8935 - val_loss: 0.4352 - val_acc: 0.8386\n",
      "Epoch 60/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3046 - acc: 0.8939 - val_loss: 0.4157 - val_acc: 0.8347\n",
      "Epoch 61/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3043 - acc: 0.8937 - val_loss: 0.4156 - val_acc: 0.8327\n",
      "Epoch 62/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3048 - acc: 0.8935 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 63/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3043 - acc: 0.8937 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 64/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3037 - acc: 0.8940 - val_loss: 0.4297 - val_acc: 0.8347\n",
      "Epoch 65/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3036 - acc: 0.8944 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 66/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3057 - acc: 0.8934 - val_loss: 0.4307 - val_acc: 0.8347\n",
      "Epoch 67/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3047 - acc: 0.8937 - val_loss: 0.4142 - val_acc: 0.8327\n",
      "Epoch 68/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3033 - acc: 0.8939 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 69/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3058 - acc: 0.8940 - val_loss: 0.4136 - val_acc: 0.8347\n",
      "Epoch 70/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8937 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 71/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3043 - acc: 0.8941 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 72/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3053 - acc: 0.8940 - val_loss: 0.4265 - val_acc: 0.8356\n",
      "Epoch 73/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3065 - acc: 0.8934 - val_loss: 0.4191 - val_acc: 0.8356\n",
      "Epoch 74/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3046 - acc: 0.8937 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 75/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.8940 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 76/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8937 - val_loss: 0.4307 - val_acc: 0.8347\n",
      "Epoch 77/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3035 - acc: 0.8942 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 78/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8935 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 79/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3031 - acc: 0.8931 - val_loss: 0.4089 - val_acc: 0.8327\n",
      "Epoch 80/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8937 - val_loss: 0.4068 - val_acc: 0.8327\n",
      "Epoch 81/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.8935 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 82/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3040 - acc: 0.8941 - val_loss: 0.4268 - val_acc: 0.8356\n",
      "Epoch 83/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.8944 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 84/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.8933 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 85/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.8932 - val_loss: 0.4312 - val_acc: 0.8376\n",
      "Epoch 86/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3034 - acc: 0.8934 - val_loss: 0.4256 - val_acc: 0.8366\n",
      "Epoch 87/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3035 - acc: 0.8932 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 88/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3027 - acc: 0.8936 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 89/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8936 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 90/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3042 - acc: 0.8936 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 91/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3028 - acc: 0.8942 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 92/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3023 - acc: 0.8944 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 93/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3017 - acc: 0.8939 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 94/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3057 - acc: 0.8934 - val_loss: 0.4120 - val_acc: 0.8327\n",
      "Epoch 95/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3027 - acc: 0.8941 - val_loss: 0.4121 - val_acc: 0.8347\n",
      "Epoch 96/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8936 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 97/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3023 - acc: 0.8935 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 98/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8937 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 99/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3007 - acc: 0.8932 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 100/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3043 - acc: 0.8937 - val_loss: 0.4110 - val_acc: 0.8327\n",
      "Epoch 101/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3025 - acc: 0.8940 - val_loss: 0.4166 - val_acc: 0.8327\n",
      "Epoch 102/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3030 - acc: 0.8940 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 103/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3024 - acc: 0.8932 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 104/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8936 - val_loss: 0.4260 - val_acc: 0.8366\n",
      "Epoch 105/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3037 - acc: 0.8936 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 106/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8948 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 107/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3017 - acc: 0.8936 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 108/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2994 - acc: 0.8942 - val_loss: 0.4301 - val_acc: 0.8356\n",
      "Epoch 109/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3033 - acc: 0.8939 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 110/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8942 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 111/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3022 - acc: 0.8946 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 112/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3014 - acc: 0.8939 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 113/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3037 - acc: 0.8936 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 114/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3021 - acc: 0.8942 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 115/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8939 - val_loss: 0.4105 - val_acc: 0.8347\n",
      "Epoch 116/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3038 - acc: 0.8937 - val_loss: 0.4081 - val_acc: 0.8327\n",
      "Epoch 117/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3019 - acc: 0.8939 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 118/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3024 - acc: 0.8948 - val_loss: 0.4163 - val_acc: 0.8347\n",
      "Epoch 119/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.8933 - val_loss: 0.4138 - val_acc: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3006 - acc: 0.8939 - val_loss: 0.4248 - val_acc: 0.8356\n",
      "Epoch 121/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8936 - val_loss: 0.4113 - val_acc: 0.8347\n",
      "Epoch 122/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3000 - acc: 0.8936 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 123/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3009 - acc: 0.8935 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 124/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3006 - acc: 0.8941 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 125/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3021 - acc: 0.8941 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 126/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3027 - acc: 0.8941 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 127/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3014 - acc: 0.8941 - val_loss: 0.4299 - val_acc: 0.8347\n",
      "Epoch 128/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3016 - acc: 0.8939 - val_loss: 0.4268 - val_acc: 0.8347\n",
      "Epoch 129/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3000 - acc: 0.8941 - val_loss: 0.4254 - val_acc: 0.8356\n",
      "Epoch 130/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3005 - acc: 0.8941 - val_loss: 0.4245 - val_acc: 0.8356\n",
      "Epoch 131/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3012 - acc: 0.8934 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 132/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3010 - acc: 0.8939 - val_loss: 0.4333 - val_acc: 0.8347\n",
      "Epoch 133/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3020 - acc: 0.8936 - val_loss: 0.4175 - val_acc: 0.8347\n",
      "Epoch 134/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3028 - acc: 0.8942 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 135/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3008 - acc: 0.8940 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 136/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3005 - acc: 0.8940 - val_loss: 0.4168 - val_acc: 0.8347\n",
      "Epoch 137/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3021 - acc: 0.8940 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 138/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2990 - acc: 0.8944 - val_loss: 0.4088 - val_acc: 0.8327\n",
      "Epoch 139/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2999 - acc: 0.8941 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 140/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2984 - acc: 0.8943 - val_loss: 0.4455 - val_acc: 0.8396\n",
      "Epoch 141/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3013 - acc: 0.8939 - val_loss: 0.4243 - val_acc: 0.8356\n",
      "Epoch 142/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2995 - acc: 0.8944 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 143/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3018 - acc: 0.8939 - val_loss: 0.4130 - val_acc: 0.8347\n",
      "Epoch 144/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2995 - acc: 0.8937 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 145/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3001 - acc: 0.8940 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 146/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3001 - acc: 0.8937 - val_loss: 0.4141 - val_acc: 0.8347\n",
      "Epoch 147/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3000 - acc: 0.8940 - val_loss: 0.4115 - val_acc: 0.8347\n",
      "Epoch 148/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8939 - val_loss: 0.4301 - val_acc: 0.8356\n",
      "Epoch 149/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3005 - acc: 0.8940 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 150/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2998 - acc: 0.8946 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 151/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.8942 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 152/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8937 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 153/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3009 - acc: 0.8939 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 154/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3005 - acc: 0.8935 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 155/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3020 - acc: 0.8941 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 156/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3017 - acc: 0.8935 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 157/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3009 - acc: 0.8943 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 158/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2998 - acc: 0.8941 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 159/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.8937 - val_loss: 0.4103 - val_acc: 0.8327\n",
      "Epoch 160/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.8941 - val_loss: 0.4195 - val_acc: 0.8356\n",
      "Epoch 161/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2992 - acc: 0.8935 - val_loss: 0.4062 - val_acc: 0.8347\n",
      "Epoch 162/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.8936 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 163/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3005 - acc: 0.8934 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 164/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3018 - acc: 0.8941 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 165/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2988 - acc: 0.8939 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 166/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2992 - acc: 0.8939 - val_loss: 0.4127 - val_acc: 0.8327\n",
      "Epoch 167/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.8936 - val_loss: 0.4258 - val_acc: 0.8366\n",
      "Epoch 168/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2993 - acc: 0.8937 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 169/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2994 - acc: 0.8939 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 170/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2996 - acc: 0.8937 - val_loss: 0.4268 - val_acc: 0.8376\n",
      "Epoch 171/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.8943 - val_loss: 0.4060 - val_acc: 0.8347\n",
      "Epoch 172/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.8941 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 173/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2995 - acc: 0.8941 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 174/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2977 - acc: 0.8942 - val_loss: 0.4140 - val_acc: 0.8347\n",
      "Epoch 175/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2988 - acc: 0.8937 - val_loss: 0.4079 - val_acc: 0.8327\n",
      "Epoch 176/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8942 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 177/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8939 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 178/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.8942 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 179/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8936 - val_loss: 0.4218 - val_acc: 0.8366\n",
      "Epoch 180/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2966 - acc: 0.8931 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 181/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2994 - acc: 0.8937 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 182/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2987 - acc: 0.8936 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 183/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2989 - acc: 0.8945 - val_loss: 0.4111 - val_acc: 0.8347\n",
      "Epoch 184/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2999 - acc: 0.8943 - val_loss: 0.4068 - val_acc: 0.8327\n",
      "Epoch 185/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3007 - acc: 0.8941 - val_loss: 0.4124 - val_acc: 0.8347\n",
      "Epoch 186/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2987 - acc: 0.8939 - val_loss: 0.4093 - val_acc: 0.8327\n",
      "Epoch 187/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2992 - acc: 0.8943 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 188/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.8939 - val_loss: 0.4066 - val_acc: 0.8347\n",
      "Epoch 189/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3010 - acc: 0.8935 - val_loss: 0.4117 - val_acc: 0.8327\n",
      "Epoch 190/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2987 - acc: 0.8939 - val_loss: 0.4305 - val_acc: 0.8356\n",
      "Epoch 191/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2985 - acc: 0.8941 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 192/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2960 - acc: 0.8941 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 193/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2994 - acc: 0.8941 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 194/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2982 - acc: 0.8934 - val_loss: 0.4119 - val_acc: 0.8327\n",
      "Epoch 195/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8939 - val_loss: 0.4258 - val_acc: 0.8356\n",
      "Epoch 196/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2984 - acc: 0.8937 - val_loss: 0.4112 - val_acc: 0.8347\n",
      "Epoch 197/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2974 - acc: 0.8941 - val_loss: 0.4150 - val_acc: 0.8327\n",
      "Epoch 198/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2985 - acc: 0.8943 - val_loss: 0.4266 - val_acc: 0.8356\n",
      "Epoch 199/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2996 - acc: 0.8939 - val_loss: 0.4249 - val_acc: 0.8356\n",
      "Epoch 200/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2970 - acc: 0.8937 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 201/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2968 - acc: 0.8941 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 202/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2997 - acc: 0.8942 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 203/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2988 - acc: 0.8935 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 204/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2976 - acc: 0.8935 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 205/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2987 - acc: 0.8937 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 206/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2980 - acc: 0.8939 - val_loss: 0.4052 - val_acc: 0.8327\n",
      "Epoch 207/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2979 - acc: 0.8936 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 208/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2990 - acc: 0.8934 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 209/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2979 - acc: 0.8948 - val_loss: 0.4238 - val_acc: 0.8356\n",
      "Epoch 210/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2993 - acc: 0.8941 - val_loss: 0.4074 - val_acc: 0.8327\n",
      "Epoch 211/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2981 - acc: 0.8939 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 212/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2974 - acc: 0.8942 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 213/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2990 - acc: 0.8939 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 214/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2984 - acc: 0.8943 - val_loss: 0.4112 - val_acc: 0.8347\n",
      "Epoch 215/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2993 - acc: 0.8941 - val_loss: 0.4070 - val_acc: 0.8347\n",
      "Epoch 216/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2967 - acc: 0.8939 - val_loss: 0.4092 - val_acc: 0.8347\n",
      "Epoch 217/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2987 - acc: 0.8935 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 218/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2984 - acc: 0.8944 - val_loss: 0.4094 - val_acc: 0.8347\n",
      "Epoch 219/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2972 - acc: 0.8942 - val_loss: 0.4074 - val_acc: 0.8347\n",
      "Epoch 220/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2986 - acc: 0.8939 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 221/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2951 - acc: 0.8940 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 222/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2979 - acc: 0.8942 - val_loss: 0.4071 - val_acc: 0.8347\n",
      "Epoch 223/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2976 - acc: 0.8941 - val_loss: 0.4063 - val_acc: 0.8347\n",
      "Epoch 224/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2980 - acc: 0.8943 - val_loss: 0.4118 - val_acc: 0.8347\n",
      "Epoch 225/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2989 - acc: 0.8930 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 226/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2978 - acc: 0.8948 - val_loss: 0.4078 - val_acc: 0.8327\n",
      "Epoch 227/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2974 - acc: 0.8947 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 228/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2991 - acc: 0.8942 - val_loss: 0.4063 - val_acc: 0.8327\n",
      "Epoch 229/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2962 - acc: 0.8935 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 230/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3014 - acc: 0.8936 - val_loss: 0.4057 - val_acc: 0.8327\n",
      "Epoch 231/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.8941 - val_loss: 0.4106 - val_acc: 0.8327\n",
      "Epoch 232/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2962 - acc: 0.8942 - val_loss: 0.4041 - val_acc: 0.8347\n",
      "Epoch 233/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2960 - acc: 0.8947 - val_loss: 0.4132 - val_acc: 0.8356\n",
      "Epoch 234/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2968 - acc: 0.8940 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 235/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2967 - acc: 0.8942 - val_loss: 0.4101 - val_acc: 0.8347\n",
      "Epoch 236/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2976 - acc: 0.8935 - val_loss: 0.4126 - val_acc: 0.8347\n",
      "Epoch 237/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2968 - acc: 0.8943 - val_loss: 0.4147 - val_acc: 0.8356\n",
      "Epoch 238/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2945 - acc: 0.8937 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 239/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3000 - acc: 0.8936 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 240/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2972 - acc: 0.8939 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 241/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2977 - acc: 0.8939 - val_loss: 0.4253 - val_acc: 0.8356\n",
      "Epoch 242/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2969 - acc: 0.8937 - val_loss: 0.4045 - val_acc: 0.8347\n",
      "Epoch 243/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2973 - acc: 0.8942 - val_loss: 0.4098 - val_acc: 0.8327\n",
      "Epoch 244/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2975 - acc: 0.8940 - val_loss: 0.4313 - val_acc: 0.8347\n",
      "Epoch 245/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2966 - acc: 0.8944 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 246/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2946 - acc: 0.8943 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 247/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2967 - acc: 0.8943 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 248/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2969 - acc: 0.8935 - val_loss: 0.4188 - val_acc: 0.8356\n",
      "Epoch 249/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2970 - acc: 0.8941 - val_loss: 0.4203 - val_acc: 0.8356\n",
      "Epoch 250/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2962 - acc: 0.8944 - val_loss: 0.4099 - val_acc: 0.8347\n",
      "Epoch 251/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2958 - acc: 0.8940 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 252/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2959 - acc: 0.8941 - val_loss: 0.4146 - val_acc: 0.8356\n",
      "Epoch 253/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2968 - acc: 0.8936 - val_loss: 0.4285 - val_acc: 0.8366\n",
      "Epoch 254/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2972 - acc: 0.8942 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 255/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2977 - acc: 0.8941 - val_loss: 0.4126 - val_acc: 0.8347\n",
      "Epoch 256/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2968 - acc: 0.8941 - val_loss: 0.4130 - val_acc: 0.8347\n",
      "Epoch 257/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2969 - acc: 0.8941 - val_loss: 0.4076 - val_acc: 0.8327\n",
      "Epoch 258/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.8946 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 259/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2976 - acc: 0.8940 - val_loss: 0.4103 - val_acc: 0.8327\n",
      "Epoch 260/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2961 - acc: 0.8939 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 261/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2956 - acc: 0.8942 - val_loss: 0.4203 - val_acc: 0.8356\n",
      "Epoch 262/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2975 - acc: 0.8947 - val_loss: 0.4077 - val_acc: 0.8347\n",
      "Epoch 263/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2956 - acc: 0.8940 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 264/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2958 - acc: 0.8946 - val_loss: 0.4441 - val_acc: 0.8406\n",
      "Epoch 265/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2955 - acc: 0.8937 - val_loss: 0.4090 - val_acc: 0.8327\n",
      "Epoch 266/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2947 - acc: 0.8940 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 267/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2958 - acc: 0.8945 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 268/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2958 - acc: 0.8945 - val_loss: 0.4117 - val_acc: 0.8327\n",
      "Epoch 269/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2966 - acc: 0.8939 - val_loss: 0.4109 - val_acc: 0.8327\n",
      "Epoch 270/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2956 - acc: 0.8945 - val_loss: 0.4037 - val_acc: 0.8347\n",
      "Epoch 271/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2971 - acc: 0.8947 - val_loss: 0.4072 - val_acc: 0.8327\n",
      "Epoch 272/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2951 - acc: 0.8944 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 273/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.8941 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 274/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8942 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 275/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2956 - acc: 0.8944 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 276/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2956 - acc: 0.8936 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 277/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2954 - acc: 0.8936 - val_loss: 0.4039 - val_acc: 0.8327\n",
      "Epoch 278/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2956 - acc: 0.8942 - val_loss: 0.4035 - val_acc: 0.8347\n",
      "Epoch 279/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2955 - acc: 0.8942 - val_loss: 0.4128 - val_acc: 0.8347\n",
      "Epoch 280/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2958 - acc: 0.8943 - val_loss: 0.4035 - val_acc: 0.8327\n",
      "Epoch 281/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2965 - acc: 0.8945 - val_loss: 0.4106 - val_acc: 0.8347\n",
      "Epoch 282/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2963 - acc: 0.8947 - val_loss: 0.4091 - val_acc: 0.8347\n",
      "Epoch 283/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2939 - acc: 0.8936 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 284/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2941 - acc: 0.8940 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 285/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2958 - acc: 0.8946 - val_loss: 0.4162 - val_acc: 0.8347\n",
      "Epoch 286/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2953 - acc: 0.8947 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 287/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2956 - acc: 0.8946 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 288/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2958 - acc: 0.8946 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 289/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.8942 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 290/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2969 - acc: 0.8943 - val_loss: 0.4074 - val_acc: 0.8347\n",
      "Epoch 291/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2969 - acc: 0.8936 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 292/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2966 - acc: 0.8942 - val_loss: 0.4061 - val_acc: 0.8347\n",
      "Epoch 293/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2960 - acc: 0.8942 - val_loss: 0.4109 - val_acc: 0.8347\n",
      "Epoch 294/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2957 - acc: 0.8936 - val_loss: 0.4069 - val_acc: 0.8347\n",
      "Epoch 295/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8944 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 296/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.8944 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 297/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2932 - acc: 0.8939 - val_loss: 0.4309 - val_acc: 0.8347\n",
      "Epoch 298/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2963 - acc: 0.8944 - val_loss: 0.4062 - val_acc: 0.8327\n",
      "Epoch 299/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8947 - val_loss: 0.4103 - val_acc: 0.8347\n",
      "Epoch 300/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2954 - acc: 0.8948 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 301/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.8942 - val_loss: 0.4053 - val_acc: 0.8347\n",
      "Epoch 302/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8939 - val_loss: 0.4057 - val_acc: 0.8327\n",
      "Epoch 303/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2955 - acc: 0.8956 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 304/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8935 - val_loss: 0.4224 - val_acc: 0.8356\n",
      "Epoch 305/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.8945 - val_loss: 0.4061 - val_acc: 0.8347\n",
      "Epoch 306/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8941 - val_loss: 0.4169 - val_acc: 0.8347\n",
      "Epoch 307/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2942 - acc: 0.8948 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 308/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2957 - acc: 0.8945 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 309/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2939 - acc: 0.8946 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 310/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2982 - acc: 0.8937 - val_loss: 0.4140 - val_acc: 0.8347\n",
      "Epoch 311/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2953 - acc: 0.8944 - val_loss: 0.4297 - val_acc: 0.8366\n",
      "Epoch 312/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2944 - acc: 0.8944 - val_loss: 0.4157 - val_acc: 0.8347\n",
      "Epoch 313/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2939 - acc: 0.8945 - val_loss: 0.4015 - val_acc: 0.8327\n",
      "Epoch 314/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2931 - acc: 0.8946 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 315/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2965 - acc: 0.8942 - val_loss: 0.4163 - val_acc: 0.8356\n",
      "Epoch 316/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2956 - acc: 0.8946 - val_loss: 0.4062 - val_acc: 0.8327\n",
      "Epoch 317/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2938 - acc: 0.8944 - val_loss: 0.4229 - val_acc: 0.8356\n",
      "Epoch 318/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2941 - acc: 0.8946 - val_loss: 0.4086 - val_acc: 0.8347\n",
      "Epoch 319/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2924 - acc: 0.8948 - val_loss: 0.4118 - val_acc: 0.8347\n",
      "Epoch 320/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2950 - acc: 0.8945 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 321/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8944 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 322/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2951 - acc: 0.8946 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 323/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2944 - acc: 0.8952 - val_loss: 0.4069 - val_acc: 0.8327\n",
      "Epoch 324/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2935 - acc: 0.8945 - val_loss: 0.4129 - val_acc: 0.8356\n",
      "Epoch 325/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2966 - acc: 0.8942 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 326/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2952 - acc: 0.8942 - val_loss: 0.4024 - val_acc: 0.8347\n",
      "Epoch 327/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8943 - val_loss: 0.4176 - val_acc: 0.8356\n",
      "Epoch 328/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8946 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 329/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2951 - acc: 0.8939 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 330/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2927 - acc: 0.8946 - val_loss: 0.4043 - val_acc: 0.8347\n",
      "Epoch 331/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2951 - acc: 0.8942 - val_loss: 0.4120 - val_acc: 0.8327\n",
      "Epoch 332/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2944 - acc: 0.8935 - val_loss: 0.4114 - val_acc: 0.8347\n",
      "Epoch 333/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8947 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 334/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8942 - val_loss: 0.4131 - val_acc: 0.8347\n",
      "Epoch 335/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2950 - acc: 0.8943 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 336/600\n",
      "9082/9082 [==============================] - 0s 20us/step - loss: 0.2939 - acc: 0.8942 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 337/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2959 - acc: 0.8933 - val_loss: 0.4070 - val_acc: 0.8327\n",
      "Epoch 338/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2953 - acc: 0.8940 - val_loss: 0.4093 - val_acc: 0.8347\n",
      "Epoch 339/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2958 - acc: 0.8946 - val_loss: 0.4065 - val_acc: 0.8347\n",
      "Epoch 340/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2956 - acc: 0.8939 - val_loss: 0.4101 - val_acc: 0.8347\n",
      "Epoch 341/600\n",
      "9082/9082 [==============================] - 0s 20us/step - loss: 0.2949 - acc: 0.8947 - val_loss: 0.4204 - val_acc: 0.8356\n",
      "Epoch 342/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2921 - acc: 0.8937 - val_loss: 0.4222 - val_acc: 0.8356\n",
      "Epoch 343/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2948 - acc: 0.8937 - val_loss: 0.4034 - val_acc: 0.8347\n",
      "Epoch 344/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2959 - acc: 0.8951 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 345/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.8937 - val_loss: 0.4036 - val_acc: 0.8327\n",
      "Epoch 346/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2938 - acc: 0.8942 - val_loss: 0.4104 - val_acc: 0.8347\n",
      "Epoch 347/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2931 - acc: 0.8946 - val_loss: 0.4152 - val_acc: 0.8347\n",
      "Epoch 348/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8947 - val_loss: 0.4072 - val_acc: 0.8327\n",
      "Epoch 349/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2946 - acc: 0.8947 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 350/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2935 - acc: 0.8944 - val_loss: 0.3993 - val_acc: 0.8327\n",
      "Epoch 351/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2945 - acc: 0.8946 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 352/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8941 - val_loss: 0.4051 - val_acc: 0.8347\n",
      "Epoch 353/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2935 - acc: 0.8946 - val_loss: 0.4164 - val_acc: 0.8356\n",
      "Epoch 354/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2928 - acc: 0.8943 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 355/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2949 - acc: 0.8941 - val_loss: 0.4093 - val_acc: 0.8347\n",
      "Epoch 356/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8946 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 357/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2953 - acc: 0.8941 - val_loss: 0.4079 - val_acc: 0.8347\n",
      "Epoch 358/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2914 - acc: 0.8953 - val_loss: 0.4070 - val_acc: 0.8327\n",
      "Epoch 359/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2949 - acc: 0.8947 - val_loss: 0.4167 - val_acc: 0.8356\n",
      "Epoch 360/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2942 - acc: 0.8943 - val_loss: 0.4239 - val_acc: 0.8356\n",
      "Epoch 361/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2916 - acc: 0.8955 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 362/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8937 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 363/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.8943 - val_loss: 0.4024 - val_acc: 0.8347\n",
      "Epoch 364/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8957 - val_loss: 0.4188 - val_acc: 0.8356\n",
      "Epoch 365/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8946 - val_loss: 0.4152 - val_acc: 0.8347\n",
      "Epoch 366/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2927 - acc: 0.8947 - val_loss: 0.4101 - val_acc: 0.8347\n",
      "Epoch 367/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8953 - val_loss: 0.4104 - val_acc: 0.8347\n",
      "Epoch 368/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2936 - acc: 0.8944 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 369/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2937 - acc: 0.8945 - val_loss: 0.4077 - val_acc: 0.8327\n",
      "Epoch 370/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2958 - acc: 0.8952 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 371/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2938 - acc: 0.8950 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 372/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8944 - val_loss: 0.4022 - val_acc: 0.8347\n",
      "Epoch 373/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2962 - acc: 0.8948 - val_loss: 0.4106 - val_acc: 0.8347\n",
      "Epoch 374/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2927 - acc: 0.8947 - val_loss: 0.4097 - val_acc: 0.8356\n",
      "Epoch 375/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2922 - acc: 0.8942 - val_loss: 0.4036 - val_acc: 0.8347\n",
      "Epoch 376/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8942 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 377/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2953 - acc: 0.8937 - val_loss: 0.4098 - val_acc: 0.8347\n",
      "Epoch 378/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2936 - acc: 0.8956 - val_loss: 0.4113 - val_acc: 0.8347\n",
      "Epoch 379/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2928 - acc: 0.8940 - val_loss: 0.3986 - val_acc: 0.8347\n",
      "Epoch 380/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2932 - acc: 0.8943 - val_loss: 0.4100 - val_acc: 0.8347\n",
      "Epoch 381/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2943 - acc: 0.8942 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 382/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2944 - acc: 0.8944 - val_loss: 0.4140 - val_acc: 0.8347\n",
      "Epoch 383/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8947 - val_loss: 0.4003 - val_acc: 0.8347\n",
      "Epoch 384/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2941 - acc: 0.8937 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 385/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2945 - acc: 0.8948 - val_loss: 0.4112 - val_acc: 0.8347\n",
      "Epoch 386/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2947 - acc: 0.8942 - val_loss: 0.4123 - val_acc: 0.8347\n",
      "Epoch 387/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2934 - acc: 0.8941 - val_loss: 0.4088 - val_acc: 0.8327\n",
      "Epoch 388/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2942 - acc: 0.8954 - val_loss: 0.4068 - val_acc: 0.8347\n",
      "Epoch 389/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2928 - acc: 0.8955 - val_loss: 0.4293 - val_acc: 0.8376\n",
      "Epoch 390/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.8946 - val_loss: 0.4156 - val_acc: 0.8386\n",
      "Epoch 391/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2921 - acc: 0.8945 - val_loss: 0.4025 - val_acc: 0.8347\n",
      "Epoch 392/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2926 - acc: 0.8951 - val_loss: 0.4064 - val_acc: 0.8347\n",
      "Epoch 393/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8947 - val_loss: 0.4085 - val_acc: 0.8347\n",
      "Epoch 394/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2933 - acc: 0.8944 - val_loss: 0.4091 - val_acc: 0.8347\n",
      "Epoch 395/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2919 - acc: 0.8954 - val_loss: 0.4067 - val_acc: 0.8347\n",
      "Epoch 396/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8957 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 397/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2919 - acc: 0.8952 - val_loss: 0.4129 - val_acc: 0.8376\n",
      "Epoch 398/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2937 - acc: 0.8947 - val_loss: 0.4135 - val_acc: 0.8347\n",
      "Epoch 399/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2912 - acc: 0.8951 - val_loss: 0.4097 - val_acc: 0.8347\n",
      "Epoch 400/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2935 - acc: 0.8943 - val_loss: 0.4013 - val_acc: 0.8347\n",
      "Epoch 401/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8945 - val_loss: 0.4031 - val_acc: 0.8327\n",
      "Epoch 402/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2919 - acc: 0.8944 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 403/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2933 - acc: 0.8944 - val_loss: 0.4061 - val_acc: 0.8327\n",
      "Epoch 404/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2928 - acc: 0.8947 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 405/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2932 - acc: 0.8947 - val_loss: 0.4150 - val_acc: 0.8327\n",
      "Epoch 406/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8950 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 407/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2957 - acc: 0.8943 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 408/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2916 - acc: 0.8943 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 409/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2908 - acc: 0.8954 - val_loss: 0.4092 - val_acc: 0.8327\n",
      "Epoch 410/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2927 - acc: 0.8940 - val_loss: 0.4066 - val_acc: 0.8347\n",
      "Epoch 411/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2938 - acc: 0.8943 - val_loss: 0.4057 - val_acc: 0.8347\n",
      "Epoch 412/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2934 - acc: 0.8942 - val_loss: 0.3985 - val_acc: 0.8347\n",
      "Epoch 413/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2927 - acc: 0.8947 - val_loss: 0.3996 - val_acc: 0.8327\n",
      "Epoch 414/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2932 - acc: 0.8951 - val_loss: 0.4093 - val_acc: 0.8347\n",
      "Epoch 415/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2916 - acc: 0.8947 - val_loss: 0.4070 - val_acc: 0.8347\n",
      "Epoch 416/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8955 - val_loss: 0.4121 - val_acc: 0.8347\n",
      "Epoch 417/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2916 - acc: 0.8941 - val_loss: 0.4016 - val_acc: 0.8327\n",
      "Epoch 418/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2934 - acc: 0.8941 - val_loss: 0.4037 - val_acc: 0.8347\n",
      "Epoch 419/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2920 - acc: 0.8944 - val_loss: 0.4169 - val_acc: 0.8356\n",
      "Epoch 420/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2910 - acc: 0.8941 - val_loss: 0.4075 - val_acc: 0.8347\n",
      "Epoch 421/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2926 - acc: 0.8941 - val_loss: 0.4030 - val_acc: 0.8347\n",
      "Epoch 422/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2930 - acc: 0.8948 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 423/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2923 - acc: 0.8947 - val_loss: 0.4096 - val_acc: 0.8347\n",
      "Epoch 424/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2901 - acc: 0.8951 - val_loss: 0.4138 - val_acc: 0.8356\n",
      "Epoch 425/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2931 - acc: 0.8947 - val_loss: 0.4001 - val_acc: 0.8347\n",
      "Epoch 426/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2929 - acc: 0.8950 - val_loss: 0.4078 - val_acc: 0.8347\n",
      "Epoch 427/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2912 - acc: 0.8942 - val_loss: 0.4056 - val_acc: 0.8347\n",
      "Epoch 428/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8943 - val_loss: 0.4178 - val_acc: 0.8356\n",
      "Epoch 429/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2919 - acc: 0.8943 - val_loss: 0.4144 - val_acc: 0.8356\n",
      "Epoch 430/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2913 - acc: 0.8945 - val_loss: 0.4183 - val_acc: 0.8386\n",
      "Epoch 431/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8947 - val_loss: 0.4050 - val_acc: 0.8327\n",
      "Epoch 432/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2929 - acc: 0.8939 - val_loss: 0.3992 - val_acc: 0.8327\n",
      "Epoch 433/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2896 - acc: 0.8952 - val_loss: 0.4375 - val_acc: 0.8386\n",
      "Epoch 434/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.8942 - val_loss: 0.4110 - val_acc: 0.8347\n",
      "Epoch 435/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2915 - acc: 0.8945 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 436/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2913 - acc: 0.8952 - val_loss: 0.4099 - val_acc: 0.8356\n",
      "Epoch 437/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2936 - acc: 0.8941 - val_loss: 0.4263 - val_acc: 0.8356\n",
      "Epoch 438/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8943 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 439/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2934 - acc: 0.8946 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 440/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2910 - acc: 0.8945 - val_loss: 0.4069 - val_acc: 0.8356\n",
      "Epoch 441/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2936 - acc: 0.8937 - val_loss: 0.4030 - val_acc: 0.8327\n",
      "Epoch 442/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2932 - acc: 0.8942 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 443/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.8951 - val_loss: 0.4074 - val_acc: 0.8347\n",
      "Epoch 444/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2922 - acc: 0.8948 - val_loss: 0.4231 - val_acc: 0.8356\n",
      "Epoch 445/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.8947 - val_loss: 0.4004 - val_acc: 0.8347\n",
      "Epoch 446/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2943 - acc: 0.8939 - val_loss: 0.4111 - val_acc: 0.8347\n",
      "Epoch 447/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8948 - val_loss: 0.4076 - val_acc: 0.8347\n",
      "Epoch 448/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2918 - acc: 0.8948 - val_loss: 0.4131 - val_acc: 0.8347\n",
      "Epoch 449/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2925 - acc: 0.8939 - val_loss: 0.4071 - val_acc: 0.8347\n",
      "Epoch 450/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2909 - acc: 0.8945 - val_loss: 0.4061 - val_acc: 0.8347\n",
      "Epoch 451/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2923 - acc: 0.8948 - val_loss: 0.4130 - val_acc: 0.8356\n",
      "Epoch 452/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2911 - acc: 0.8955 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 453/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2903 - acc: 0.8951 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 454/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.8947 - val_loss: 0.4134 - val_acc: 0.8347\n",
      "Epoch 455/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2917 - acc: 0.8942 - val_loss: 0.4124 - val_acc: 0.8327\n",
      "Epoch 456/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2921 - acc: 0.8944 - val_loss: 0.4040 - val_acc: 0.8347\n",
      "Epoch 457/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8946 - val_loss: 0.4056 - val_acc: 0.8347\n",
      "Epoch 458/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2923 - acc: 0.8945 - val_loss: 0.4166 - val_acc: 0.8356\n",
      "Epoch 459/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8947 - val_loss: 0.4076 - val_acc: 0.8347\n",
      "Epoch 460/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2921 - acc: 0.8944 - val_loss: 0.4092 - val_acc: 0.8347\n",
      "Epoch 461/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2923 - acc: 0.8937 - val_loss: 0.4057 - val_acc: 0.8327\n",
      "Epoch 462/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2930 - acc: 0.8946 - val_loss: 0.4187 - val_acc: 0.8356\n",
      "Epoch 463/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2922 - acc: 0.8955 - val_loss: 0.4166 - val_acc: 0.8356\n",
      "Epoch 464/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2907 - acc: 0.8945 - val_loss: 0.4070 - val_acc: 0.8347\n",
      "Epoch 465/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8950 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 466/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8950 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 467/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8944 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 468/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2911 - acc: 0.8953 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 469/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2914 - acc: 0.8945 - val_loss: 0.4096 - val_acc: 0.8366\n",
      "Epoch 470/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2895 - acc: 0.8937 - val_loss: 0.3975 - val_acc: 0.8327\n",
      "Epoch 471/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2922 - acc: 0.8948 - val_loss: 0.4011 - val_acc: 0.8327\n",
      "Epoch 472/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2911 - acc: 0.8950 - val_loss: 0.3989 - val_acc: 0.8347\n",
      "Epoch 473/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2896 - acc: 0.8945 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 474/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2904 - acc: 0.8940 - val_loss: 0.4042 - val_acc: 0.8347\n",
      "Epoch 475/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2911 - acc: 0.8944 - val_loss: 0.4054 - val_acc: 0.8347\n",
      "Epoch 476/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2915 - acc: 0.8942 - val_loss: 0.4090 - val_acc: 0.8327\n",
      "Epoch 477/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2926 - acc: 0.8953 - val_loss: 0.4073 - val_acc: 0.8347\n",
      "Epoch 478/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2915 - acc: 0.8953 - val_loss: 0.3947 - val_acc: 0.8327\n",
      "Epoch 479/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2905 - acc: 0.8947 - val_loss: 0.4012 - val_acc: 0.8327\n",
      "Epoch 480/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2914 - acc: 0.8947 - val_loss: 0.3980 - val_acc: 0.8327\n",
      "Epoch 481/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2907 - acc: 0.8948 - val_loss: 0.4021 - val_acc: 0.8347\n",
      "Epoch 482/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2898 - acc: 0.8948 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 483/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2904 - acc: 0.8950 - val_loss: 0.4056 - val_acc: 0.8347\n",
      "Epoch 484/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2935 - acc: 0.8941 - val_loss: 0.4126 - val_acc: 0.8337\n",
      "Epoch 485/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8943 - val_loss: 0.4209 - val_acc: 0.8356\n",
      "Epoch 486/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2918 - acc: 0.8951 - val_loss: 0.4112 - val_acc: 0.8356\n",
      "Epoch 487/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2903 - acc: 0.8950 - val_loss: 0.4104 - val_acc: 0.8327\n",
      "Epoch 488/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2903 - acc: 0.8953 - val_loss: 0.4074 - val_acc: 0.8327\n",
      "Epoch 489/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2900 - acc: 0.8951 - val_loss: 0.4123 - val_acc: 0.8356\n",
      "Epoch 490/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2918 - acc: 0.8940 - val_loss: 0.4082 - val_acc: 0.8356\n",
      "Epoch 491/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2899 - acc: 0.8944 - val_loss: 0.4072 - val_acc: 0.8356\n",
      "Epoch 492/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2910 - acc: 0.8945 - val_loss: 0.3954 - val_acc: 0.8347\n",
      "Epoch 493/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2912 - acc: 0.8947 - val_loss: 0.4152 - val_acc: 0.8356\n",
      "Epoch 494/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2919 - acc: 0.8952 - val_loss: 0.4091 - val_acc: 0.8347\n",
      "Epoch 495/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2911 - acc: 0.8945 - val_loss: 0.4104 - val_acc: 0.8347\n",
      "Epoch 496/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2897 - acc: 0.8944 - val_loss: 0.4057 - val_acc: 0.8347\n",
      "Epoch 497/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2918 - acc: 0.8947 - val_loss: 0.4087 - val_acc: 0.8356\n",
      "Epoch 498/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2910 - acc: 0.8942 - val_loss: 0.4068 - val_acc: 0.8347\n",
      "Epoch 499/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2910 - acc: 0.8951 - val_loss: 0.4157 - val_acc: 0.8356\n",
      "Epoch 500/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2913 - acc: 0.8944 - val_loss: 0.4056 - val_acc: 0.8366\n",
      "Epoch 501/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2933 - acc: 0.8941 - val_loss: 0.3984 - val_acc: 0.8347\n",
      "Epoch 502/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2910 - acc: 0.8943 - val_loss: 0.4121 - val_acc: 0.8327\n",
      "Epoch 503/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2910 - acc: 0.8943 - val_loss: 0.4099 - val_acc: 0.8356\n",
      "Epoch 504/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2900 - acc: 0.8946 - val_loss: 0.4080 - val_acc: 0.8376\n",
      "Epoch 505/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2926 - acc: 0.8945 - val_loss: 0.3996 - val_acc: 0.8337\n",
      "Epoch 506/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2889 - acc: 0.8944 - val_loss: 0.4047 - val_acc: 0.8347\n",
      "Epoch 507/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2902 - acc: 0.8944 - val_loss: 0.4137 - val_acc: 0.8347\n",
      "Epoch 508/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2903 - acc: 0.8947 - val_loss: 0.3998 - val_acc: 0.8327\n",
      "Epoch 509/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2907 - acc: 0.8948 - val_loss: 0.4055 - val_acc: 0.8347\n",
      "Epoch 510/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2926 - acc: 0.8947 - val_loss: 0.4083 - val_acc: 0.8347\n",
      "Epoch 511/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8941 - val_loss: 0.4135 - val_acc: 0.8347\n",
      "Epoch 512/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2904 - acc: 0.8953 - val_loss: 0.3976 - val_acc: 0.8327\n",
      "Epoch 513/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2908 - acc: 0.8945 - val_loss: 0.4086 - val_acc: 0.8347\n",
      "Epoch 514/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2904 - acc: 0.8948 - val_loss: 0.4056 - val_acc: 0.8356\n",
      "Epoch 515/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2916 - acc: 0.8946 - val_loss: 0.4015 - val_acc: 0.8347\n",
      "Epoch 516/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2908 - acc: 0.8948 - val_loss: 0.3944 - val_acc: 0.8327\n",
      "Epoch 517/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2904 - acc: 0.8947 - val_loss: 0.4094 - val_acc: 0.8347\n",
      "Epoch 518/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2905 - acc: 0.8950 - val_loss: 0.4074 - val_acc: 0.8347\n",
      "Epoch 519/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.8951 - val_loss: 0.4137 - val_acc: 0.8366\n",
      "Epoch 520/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2917 - acc: 0.8946 - val_loss: 0.3967 - val_acc: 0.8347\n",
      "Epoch 521/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2910 - acc: 0.8946 - val_loss: 0.4146 - val_acc: 0.8347\n",
      "Epoch 522/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2917 - acc: 0.8947 - val_loss: 0.4057 - val_acc: 0.8347\n",
      "Epoch 523/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2880 - acc: 0.8945 - val_loss: 0.4032 - val_acc: 0.8327\n",
      "Epoch 524/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2879 - acc: 0.8957 - val_loss: 0.4174 - val_acc: 0.8356\n",
      "Epoch 525/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2902 - acc: 0.8945 - val_loss: 0.4046 - val_acc: 0.8347\n",
      "Epoch 526/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2908 - acc: 0.8945 - val_loss: 0.4187 - val_acc: 0.8356\n",
      "Epoch 527/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2914 - acc: 0.8947 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 528/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2897 - acc: 0.8944 - val_loss: 0.4078 - val_acc: 0.8347\n",
      "Epoch 529/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2891 - acc: 0.8947 - val_loss: 0.4004 - val_acc: 0.8356\n",
      "Epoch 530/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2912 - acc: 0.8947 - val_loss: 0.4008 - val_acc: 0.8327\n",
      "Epoch 531/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2898 - acc: 0.8947 - val_loss: 0.4158 - val_acc: 0.8356\n",
      "Epoch 532/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8939 - val_loss: 0.3939 - val_acc: 0.8347\n",
      "Epoch 533/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2913 - acc: 0.8944 - val_loss: 0.4066 - val_acc: 0.8356\n",
      "Epoch 534/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2895 - acc: 0.8946 - val_loss: 0.4099 - val_acc: 0.8356\n",
      "Epoch 535/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.8951 - val_loss: 0.3957 - val_acc: 0.8337\n",
      "Epoch 536/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2926 - acc: 0.8950 - val_loss: 0.4028 - val_acc: 0.8327\n",
      "Epoch 537/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2915 - acc: 0.8952 - val_loss: 0.4076 - val_acc: 0.8327\n",
      "Epoch 538/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2889 - acc: 0.8957 - val_loss: 0.3954 - val_acc: 0.8327\n",
      "Epoch 539/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2901 - acc: 0.8940 - val_loss: 0.4127 - val_acc: 0.8356\n",
      "Epoch 540/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2898 - acc: 0.8950 - val_loss: 0.3996 - val_acc: 0.8327\n",
      "Epoch 541/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2908 - acc: 0.8950 - val_loss: 0.4042 - val_acc: 0.8347\n",
      "Epoch 542/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2897 - acc: 0.8951 - val_loss: 0.3983 - val_acc: 0.8337\n",
      "Epoch 543/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.2901 - acc: 0.8951 - val_loss: 0.4023 - val_acc: 0.8327\n",
      "Epoch 544/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2912 - acc: 0.8952 - val_loss: 0.4246 - val_acc: 0.8356\n",
      "Epoch 545/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2905 - acc: 0.8950 - val_loss: 0.4069 - val_acc: 0.8356\n",
      "Epoch 546/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2905 - acc: 0.8955 - val_loss: 0.4039 - val_acc: 0.8327\n",
      "Epoch 547/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2892 - acc: 0.8951 - val_loss: 0.4000 - val_acc: 0.8327\n",
      "Epoch 548/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2907 - acc: 0.8937 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 549/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2912 - acc: 0.8942 - val_loss: 0.4059 - val_acc: 0.8347\n",
      "Epoch 550/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2906 - acc: 0.8946 - val_loss: 0.4037 - val_acc: 0.8356\n",
      "Epoch 551/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2885 - acc: 0.8957 - val_loss: 0.4059 - val_acc: 0.8356\n",
      "Epoch 552/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2896 - acc: 0.8941 - val_loss: 0.3987 - val_acc: 0.8347\n",
      "Epoch 553/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2911 - acc: 0.8946 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 554/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2893 - acc: 0.8950 - val_loss: 0.4012 - val_acc: 0.8327\n",
      "Epoch 555/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2923 - acc: 0.8939 - val_loss: 0.4076 - val_acc: 0.8347\n",
      "Epoch 556/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2908 - acc: 0.8944 - val_loss: 0.4141 - val_acc: 0.8366\n",
      "Epoch 557/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2915 - acc: 0.8944 - val_loss: 0.4096 - val_acc: 0.8347\n",
      "Epoch 558/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2905 - acc: 0.8951 - val_loss: 0.4060 - val_acc: 0.8347\n",
      "Epoch 559/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2895 - acc: 0.8962 - val_loss: 0.3992 - val_acc: 0.8356\n",
      "Epoch 560/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2885 - acc: 0.8953 - val_loss: 0.3918 - val_acc: 0.8327\n",
      "Epoch 561/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2902 - acc: 0.8954 - val_loss: 0.3970 - val_acc: 0.8327\n",
      "Epoch 562/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2913 - acc: 0.8947 - val_loss: 0.3984 - val_acc: 0.8347\n",
      "Epoch 563/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2915 - acc: 0.8941 - val_loss: 0.4090 - val_acc: 0.8347\n",
      "Epoch 564/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.2895 - acc: 0.8956 - val_loss: 0.3986 - val_acc: 0.8347\n",
      "Epoch 565/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2916 - acc: 0.8955 - val_loss: 0.4027 - val_acc: 0.8327\n",
      "Epoch 566/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2888 - acc: 0.8953 - val_loss: 0.4031 - val_acc: 0.8356\n",
      "Epoch 567/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2907 - acc: 0.8952 - val_loss: 0.4078 - val_acc: 0.8347\n",
      "Epoch 568/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2902 - acc: 0.8945 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 569/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2896 - acc: 0.8943 - val_loss: 0.3969 - val_acc: 0.8327\n",
      "Epoch 570/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2892 - acc: 0.8941 - val_loss: 0.4244 - val_acc: 0.8366\n",
      "Epoch 571/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2898 - acc: 0.8959 - val_loss: 0.3999 - val_acc: 0.8337\n",
      "Epoch 572/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2903 - acc: 0.8947 - val_loss: 0.4014 - val_acc: 0.8327\n",
      "Epoch 573/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2895 - acc: 0.8952 - val_loss: 0.4102 - val_acc: 0.8356\n",
      "Epoch 574/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2887 - acc: 0.8946 - val_loss: 0.4049 - val_acc: 0.8366\n",
      "Epoch 575/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2909 - acc: 0.8946 - val_loss: 0.4054 - val_acc: 0.8356\n",
      "Epoch 576/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2883 - acc: 0.8955 - val_loss: 0.3961 - val_acc: 0.8327\n",
      "Epoch 577/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2908 - acc: 0.8944 - val_loss: 0.4020 - val_acc: 0.8347\n",
      "Epoch 578/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2905 - acc: 0.8950 - val_loss: 0.3964 - val_acc: 0.8327\n",
      "Epoch 579/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2898 - acc: 0.8942 - val_loss: 0.4121 - val_acc: 0.8347\n",
      "Epoch 580/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2916 - acc: 0.8950 - val_loss: 0.3981 - val_acc: 0.8337\n",
      "Epoch 581/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2905 - acc: 0.8944 - val_loss: 0.3992 - val_acc: 0.8347\n",
      "Epoch 582/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2883 - acc: 0.8953 - val_loss: 0.4266 - val_acc: 0.8386\n",
      "Epoch 583/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2923 - acc: 0.8948 - val_loss: 0.4077 - val_acc: 0.8327\n",
      "Epoch 584/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2905 - acc: 0.8954 - val_loss: 0.4030 - val_acc: 0.8337\n",
      "Epoch 585/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2889 - acc: 0.8946 - val_loss: 0.4099 - val_acc: 0.8356\n",
      "Epoch 586/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.2905 - acc: 0.8940 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 587/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2896 - acc: 0.8953 - val_loss: 0.4055 - val_acc: 0.8327\n",
      "Epoch 588/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2889 - acc: 0.8956 - val_loss: 0.3986 - val_acc: 0.8347\n",
      "Epoch 589/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2904 - acc: 0.8955 - val_loss: 0.4141 - val_acc: 0.8356\n",
      "Epoch 590/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2904 - acc: 0.8953 - val_loss: 0.3979 - val_acc: 0.8317\n",
      "Epoch 591/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2892 - acc: 0.8955 - val_loss: 0.4075 - val_acc: 0.8327\n",
      "Epoch 592/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2884 - acc: 0.8948 - val_loss: 0.4030 - val_acc: 0.8356\n",
      "Epoch 593/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2895 - acc: 0.8953 - val_loss: 0.4097 - val_acc: 0.8366\n",
      "Epoch 594/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.2906 - acc: 0.8954 - val_loss: 0.4056 - val_acc: 0.8366\n",
      "Epoch 595/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2881 - acc: 0.8943 - val_loss: 0.4051 - val_acc: 0.8356\n",
      "Epoch 596/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2886 - acc: 0.8951 - val_loss: 0.4023 - val_acc: 0.8356\n",
      "Epoch 597/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2903 - acc: 0.8954 - val_loss: 0.4031 - val_acc: 0.8347\n",
      "Epoch 598/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2911 - acc: 0.8946 - val_loss: 0.4078 - val_acc: 0.8366\n",
      "Epoch 599/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.2894 - acc: 0.8954 - val_loss: 0.4085 - val_acc: 0.8356\n",
      "Epoch 600/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.2886 - acc: 0.8957 - val_loss: 0.3959 - val_acc: 0.8535\n"
     ]
    }
   ],
   "source": [
    "# Fitting our model \n",
    "hist = classifier.fit(scaled_train_x, train_y, batch_size = 150, epochs = 600, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca8c5ef4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation3/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca8c5dbe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation3/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "model-id:        00010203-0405-0607-0809-0a0b0c0d0e0f\n",
      "json:        v_f_nice_model_00010203-0405-0607-0809-0a0b0c0d0e0f_.json\n",
      "h5File:        v_f_nice_model_00010203-0405-0607-0809-0a0b0c0d0e0f_.h5\n"
     ]
    }
   ],
   "source": [
    "# uniqId\n",
    "uniqId = uuid.UUID('{00010203-0405-0607-0809-0a0b0c0d0e0f}')\n",
    "jsonFile = 'v_f_nice_model_' + str(uniqId)  +'_.json'\n",
    "h5File = 'v_f_nice_model_' + str(uniqId)  + '_.h5'\n",
    "# Save the model\n",
    "model_json = classifier.to_json()\n",
    "with open(jsonFile, 'w+') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(h5File)\n",
    "\n",
    "print(\"Saved model to disk\")\n",
    "print('model-id:        ' + str(uniqId))\n",
    "print('json:        ' + str(jsonFile))\n",
    "print('h5File:        ' + str(h5File))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with development test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (726)\n",
      "CorrectReject    644\n",
      "GrossFalseAccept 21*3.0 = 63.0\n",
      "PlainFalseAccept 61\n",
      "RejectionRate    0.839\n",
      "\n",
      "CORRECT UTTERANCES (1798)\n",
      "CorrectAccept    1630\n",
      "FalseReject      168\n",
      "RejectionRate    0.093\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.929\n",
      "F                             0.918\n",
      "Sa                            0.886\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             8.974\n",
      "Da                            5.615\n",
      "Df                            7.099\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred = classifier.predict_classes(scaled_dev_test_x)\n",
    "evaluate(dev_test_y, dev_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with st2 test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (250)\n",
      "CorrectReject    244\n",
      "GrossFalseAccept 1*3.0 = 3.0\n",
      "PlainFalseAccept 5\n",
      "RejectionRate    0.968\n",
      "\n",
      "CORRECT UTTERANCES (750)\n",
      "CorrectAccept    697\n",
      "FalseReject      53\n",
      "RejectionRate    0.071\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.989\n",
      "F                             0.958\n",
      "Sa                            0.939\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             13.702\n",
      "Da                            29.274\n",
      "Df                            20.028\n"
     ]
    }
   ],
   "source": [
    "st2_y_pred = classifier.predict_classes(scaled_st2_test_x)\n",
    "evaluate(st2_test_y, st2_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39535,1\n",
      "39538,1\n",
      "39539,1\n",
      "39540,0\n",
      "39541,0\n",
      "39547,0\n",
      "39549,1\n",
      "39550,1\n",
      "39551,0\n",
      "39555,0\n",
      "39556,1\n",
      "39559,1\n",
      "39561,1\n",
      "39562,1\n",
      "39563,1\n",
      "39566,0\n",
      "39567,0\n",
      "39570,1\n",
      "39572,0\n",
      "39573,0\n",
      "39574,1\n",
      "39580,1\n",
      "39583,0\n",
      "39585,0\n",
      "39586,1\n",
      "39588,1\n",
      "39590,1\n",
      "39591,0\n",
      "39596,1\n",
      "39597,0\n",
      "39601,1\n",
      "39605,1\n",
      "39609,1\n",
      "39610,1\n",
      "39611,1\n",
      "39620,1\n",
      "39624,0\n",
      "39627,0\n",
      "39628,1\n",
      "39629,0\n",
      "39630,1\n",
      "39632,1\n",
      "39634,1\n",
      "39635,1\n",
      "39636,0\n",
      "39639,0\n",
      "39641,1\n",
      "39642,1\n",
      "39648,0\n",
      "39649,0\n",
      "39650,1\n",
      "39652,0\n",
      "39655,1\n",
      "39657,1\n",
      "39658,0\n",
      "39664,0\n",
      "40453,1\n",
      "40457,1\n",
      "40459,1\n",
      "40460,1\n",
      "40462,1\n",
      "40464,1\n",
      "40465,1\n",
      "40467,1\n",
      "40469,0\n",
      "40477,1\n",
      "40478,1\n",
      "40481,0\n",
      "40482,1\n",
      "40485,1\n",
      "40486,1\n",
      "40488,0\n",
      "40489,0\n",
      "40490,1\n",
      "40491,0\n",
      "40492,0\n",
      "40494,1\n",
      "40496,1\n",
      "40497,1\n",
      "40498,0\n",
      "40499,1\n",
      "40501,0\n",
      "40502,1\n",
      "40503,1\n",
      "40504,1\n",
      "40505,1\n",
      "40506,1\n",
      "40507,1\n",
      "40509,0\n",
      "40510,1\n",
      "40511,0\n",
      "40515,1\n",
      "40517,0\n",
      "40519,1\n",
      "40520,1\n",
      "40527,1\n",
      "40528,1\n",
      "40530,1\n",
      "40532,1\n",
      "40534,1\n",
      "40535,1\n",
      "40537,1\n",
      "40539,1\n",
      "40540,1\n",
      "40542,1\n",
      "40544,1\n",
      "40546,0\n",
      "40548,1\n",
      "40549,0\n",
      "40552,1\n",
      "40553,1\n",
      "40555,1\n",
      "40556,1\n",
      "40558,1\n",
      "40559,1\n",
      "40560,1\n",
      "40561,1\n",
      "40564,0\n",
      "40565,1\n",
      "40566,1\n",
      "40568,1\n",
      "40569,0\n",
      "40570,1\n",
      "40571,1\n",
      "40572,1\n",
      "40573,1\n",
      "40574,1\n",
      "40577,1\n",
      "40579,1\n",
      "40580,1\n",
      "40582,1\n",
      "40583,1\n",
      "40585,1\n",
      "40587,1\n",
      "40589,0\n",
      "40591,0\n",
      "40592,0\n",
      "40593,0\n",
      "40599,0\n",
      "40603,0\n",
      "40610,0\n",
      "40612,0\n",
      "40619,0\n",
      "40621,0\n",
      "40624,0\n",
      "40634,0\n",
      "40638,0\n",
      "40641,1\n",
      "40643,1\n",
      "40644,1\n",
      "40646,1\n",
      "40649,1\n",
      "40650,1\n",
      "40651,0\n",
      "40652,1\n",
      "40656,1\n",
      "40657,1\n",
      "40658,1\n",
      "40659,0\n",
      "40662,1\n",
      "40664,0\n",
      "40665,0\n",
      "40666,1\n",
      "40667,0\n",
      "40669,1\n",
      "40670,0\n",
      "40673,1\n",
      "40674,0\n",
      "40675,0\n",
      "40676,0\n",
      "40679,0\n",
      "40682,1\n",
      "40687,1\n",
      "40690,0\n",
      "40692,0\n",
      "40697,0\n",
      "40698,0\n",
      "40699,0\n",
      "40700,1\n",
      "40703,1\n",
      "40706,1\n",
      "40707,0\n",
      "40710,0\n",
      "40711,1\n",
      "40714,0\n",
      "40718,0\n",
      "40719,1\n",
      "40721,1\n",
      "40726,1\n",
      "40727,1\n",
      "40728,0\n",
      "40730,0\n",
      "40731,0\n",
      "40734,1\n",
      "40736,1\n",
      "40737,1\n",
      "40738,1\n",
      "40739,0\n",
      "40740,1\n",
      "40741,1\n",
      "40742,1\n",
      "40744,0\n",
      "40745,1\n",
      "40746,1\n",
      "40747,1\n",
      "40748,1\n",
      "40749,1\n",
      "40750,1\n",
      "40752,1\n",
      "40753,1\n",
      "40755,1\n",
      "40756,1\n",
      "40758,0\n",
      "40759,1\n",
      "40760,1\n",
      "40761,0\n",
      "40763,0\n",
      "40765,0\n",
      "40766,1\n",
      "40767,0\n",
      "40768,1\n",
      "40769,1\n",
      "40770,1\n",
      "40772,1\n",
      "40773,1\n",
      "40776,1\n",
      "40777,1\n",
      "40778,0\n",
      "40779,0\n",
      "40780,0\n",
      "40782,1\n",
      "40783,1\n",
      "40784,1\n",
      "40785,1\n",
      "40786,1\n",
      "40788,0\n",
      "40789,0\n",
      "40791,1\n",
      "40797,1\n",
      "40799,1\n",
      "40800,1\n",
      "40803,1\n",
      "40804,1\n",
      "40806,1\n",
      "40807,1\n",
      "40810,1\n",
      "40812,1\n",
      "40815,1\n",
      "40816,1\n",
      "40818,1\n",
      "40822,1\n",
      "40823,1\n",
      "40824,1\n",
      "40826,1\n",
      "40830,1\n",
      "40831,1\n",
      "40833,0\n",
      "40834,0\n",
      "40836,1\n",
      "40838,1\n",
      "40839,0\n",
      "40841,1\n",
      "40842,1\n",
      "40845,1\n",
      "40846,0\n",
      "40847,0\n",
      "40849,1\n",
      "40857,0\n",
      "40858,1\n",
      "40861,1\n",
      "40862,1\n",
      "40863,0\n",
      "40864,1\n",
      "40865,1\n",
      "40866,1\n",
      "40867,1\n",
      "40869,1\n",
      "40871,1\n",
      "40875,1\n",
      "40877,1\n",
      "40878,1\n",
      "40879,1\n",
      "40885,1\n",
      "40887,1\n",
      "40888,1\n",
      "40890,1\n",
      "40892,1\n",
      "40893,1\n",
      "40894,1\n",
      "40897,0\n",
      "40899,1\n",
      "40900,1\n",
      "40902,1\n",
      "40903,1\n",
      "40904,1\n",
      "40905,1\n",
      "40906,1\n",
      "40908,1\n",
      "40909,1\n",
      "40912,1\n",
      "40913,1\n",
      "40914,1\n",
      "40917,1\n",
      "40918,1\n",
      "40920,1\n",
      "40922,1\n",
      "40923,1\n",
      "40924,0\n",
      "40926,1\n",
      "40927,1\n",
      "40928,0\n",
      "40929,1\n",
      "40930,0\n",
      "40931,1\n",
      "40932,1\n",
      "40934,1\n",
      "40939,1\n",
      "40942,1\n",
      "40943,0\n",
      "40944,1\n",
      "40945,1\n",
      "40946,1\n",
      "40949,1\n",
      "40952,0\n",
      "40954,1\n",
      "40955,1\n",
      "40957,1\n",
      "40958,0\n",
      "40959,1\n",
      "40960,0\n",
      "40961,1\n",
      "40964,0\n",
      "40966,1\n",
      "40967,1\n",
      "40969,1\n",
      "40971,1\n",
      "40973,1\n",
      "40974,0\n",
      "40975,1\n",
      "40976,1\n",
      "40977,1\n",
      "40978,0\n",
      "40979,1\n",
      "40981,0\n",
      "40982,0\n",
      "40984,0\n",
      "40986,1\n",
      "40988,0\n",
      "40989,1\n",
      "40991,1\n",
      "40993,1\n",
      "40994,1\n",
      "40996,0\n",
      "40997,0\n",
      "40998,0\n",
      "41001,0\n",
      "41003,0\n",
      "41007,0\n",
      "41008,1\n",
      "41009,1\n",
      "41010,0\n",
      "41011,0\n",
      "41012,1\n",
      "41013,1\n",
      "41018,1\n",
      "41019,0\n",
      "41024,1\n",
      "41026,1\n",
      "41027,0\n",
      "41028,1\n",
      "41030,1\n",
      "41031,1\n",
      "41032,1\n",
      "41034,1\n",
      "41035,1\n",
      "41036,1\n",
      "41039,1\n",
      "41043,0\n",
      "41045,1\n",
      "41046,1\n",
      "41049,1\n",
      "41052,0\n",
      "41053,1\n",
      "41054,1\n",
      "41055,0\n",
      "41056,1\n",
      "41058,1\n",
      "41060,1\n",
      "41061,1\n",
      "41062,0\n",
      "41064,0\n",
      "41065,1\n",
      "41066,1\n",
      "41067,1\n",
      "41068,1\n",
      "41069,1\n",
      "41070,1\n",
      "41072,1\n",
      "41073,1\n",
      "41074,1\n",
      "41076,1\n",
      "41077,1\n",
      "41078,1\n",
      "41083,1\n",
      "41085,0\n",
      "41087,1\n",
      "41090,1\n",
      "41092,1\n",
      "41093,1\n",
      "41094,1\n",
      "41096,1\n",
      "41098,1\n",
      "41099,1\n",
      "41100,1\n",
      "41103,1\n",
      "41104,1\n",
      "41105,0\n",
      "41106,1\n",
      "41107,1\n",
      "41109,0\n",
      "41110,1\n",
      "41112,1\n",
      "41114,1\n",
      "41116,1\n",
      "41117,1\n",
      "41118,0\n",
      "41120,1\n",
      "41121,1\n",
      "41122,1\n",
      "41123,1\n",
      "41124,1\n",
      "41125,1\n",
      "41126,1\n",
      "41127,1\n",
      "41128,1\n",
      "41129,1\n",
      "41131,1\n",
      "41133,1\n",
      "41135,1\n",
      "41136,1\n",
      "41137,0\n",
      "41138,1\n",
      "41139,1\n",
      "41140,1\n",
      "41142,1\n",
      "41148,1\n",
      "41149,0\n",
      "41154,1\n",
      "41155,0\n",
      "41157,1\n",
      "41159,1\n",
      "41160,1\n",
      "41161,1\n",
      "41162,1\n",
      "41163,0\n",
      "41164,1\n",
      "41165,1\n",
      "41166,1\n",
      "41169,1\n",
      "41171,1\n",
      "41172,1\n",
      "41173,0\n",
      "41175,1\n",
      "41177,0\n",
      "41178,0\n",
      "41179,1\n",
      "41180,1\n",
      "41181,0\n",
      "41184,1\n",
      "41185,1\n",
      "41186,1\n",
      "41188,0\n",
      "41189,1\n",
      "41190,1\n",
      "41191,1\n",
      "41192,1\n",
      "41194,1\n",
      "41195,1\n",
      "41197,0\n",
      "41198,1\n",
      "41201,0\n",
      "41203,1\n",
      "41205,1\n",
      "41206,1\n",
      "41208,1\n",
      "41210,1\n",
      "41213,1\n",
      "41215,1\n",
      "41216,1\n",
      "41217,1\n",
      "41218,1\n",
      "41220,1\n",
      "41221,0\n",
      "41222,0\n",
      "41223,1\n",
      "41226,1\n",
      "41227,1\n",
      "41229,1\n",
      "41231,0\n",
      "41232,0\n",
      "41233,0\n",
      "41234,0\n",
      "41237,1\n",
      "41238,0\n",
      "41240,0\n",
      "41241,1\n",
      "41242,0\n",
      "41243,1\n",
      "41244,1\n",
      "41245,1\n",
      "41247,0\n",
      "41248,1\n",
      "41249,1\n",
      "41251,0\n",
      "41252,1\n",
      "41253,0\n",
      "41255,1\n",
      "41256,1\n",
      "41257,0\n",
      "41258,1\n",
      "41259,1\n",
      "41260,0\n",
      "41263,0\n",
      "41265,1\n",
      "41272,0\n",
      "41273,1\n",
      "41277,1\n",
      "41279,0\n",
      "41282,1\n",
      "41283,1\n",
      "41284,0\n",
      "41287,0\n",
      "41291,0\n",
      "41293,1\n",
      "41294,0\n",
      "41296,0\n",
      "41297,0\n",
      "41298,1\n",
      "41299,1\n",
      "41302,0\n",
      "41308,0\n",
      "41311,1\n",
      "41312,1\n",
      "41314,1\n",
      "41318,1\n",
      "41320,1\n",
      "41322,1\n",
      "41323,1\n",
      "41327,1\n",
      "41328,0\n",
      "41329,0\n",
      "41330,1\n",
      "41331,1\n",
      "41334,1\n",
      "41337,1\n",
      "41338,0\n",
      "39666,1\n",
      "39667,0\n",
      "39671,0\n",
      "39672,0\n",
      "39675,1\n",
      "39677,0\n",
      "39678,1\n",
      "39682,0\n",
      "39684,1\n",
      "39687,1\n",
      "39690,0\n",
      "39694,0\n",
      "39695,0\n",
      "39701,0\n",
      "39702,0\n",
      "39703,0\n",
      "39704,0\n",
      "39707,0\n",
      "39708,1\n",
      "39709,0\n",
      "39710,1\n",
      "39714,0\n",
      "39718,0\n",
      "39719,1\n",
      "39720,1\n",
      "39721,1\n",
      "39722,1\n",
      "39726,1\n",
      "39727,0\n",
      "39728,1\n",
      "39729,0\n",
      "39732,1\n",
      "39740,1\n",
      "39741,1\n",
      "39742,0\n",
      "39744,1\n",
      "39747,0\n",
      "39750,0\n",
      "39751,1\n",
      "39752,1\n",
      "39754,1\n",
      "39756,1\n",
      "39758,0\n",
      "39759,0\n",
      "39763,1\n",
      "39764,1\n",
      "39767,0\n",
      "39768,0\n",
      "39769,1\n",
      "39770,1\n",
      "39772,1\n",
      "39774,1\n",
      "39777,1\n",
      "39779,0\n",
      "39782,0\n",
      "39785,1\n",
      "39787,1\n",
      "39789,1\n",
      "39791,0\n",
      "39792,1\n",
      "39794,0\n",
      "39795,0\n",
      "39798,1\n",
      "39801,1\n",
      "39802,1\n",
      "39803,1\n",
      "39804,1\n",
      "39806,1\n",
      "39807,1\n",
      "39808,0\n",
      "39809,1\n",
      "39810,1\n",
      "39812,1\n",
      "39813,0\n",
      "39817,0\n",
      "39823,0\n",
      "39824,0\n",
      "39825,0\n",
      "39826,0\n",
      "39827,0\n",
      "39831,0\n",
      "39832,0\n",
      "39833,1\n",
      "39834,0\n",
      "39836,0\n",
      "39840,1\n",
      "39841,0\n",
      "39842,1\n",
      "39846,0\n",
      "39849,0\n",
      "39850,0\n",
      "39854,1\n",
      "39855,1\n",
      "39856,1\n",
      "39857,1\n",
      "39859,1\n",
      "39860,0\n",
      "39861,0\n",
      "39862,1\n",
      "39863,1\n",
      "39864,1\n",
      "39866,0\n",
      "39867,1\n",
      "39869,1\n",
      "39871,1\n",
      "39872,1\n",
      "39875,1\n",
      "39876,1\n",
      "39877,1\n",
      "39878,1\n",
      "39879,1\n",
      "39880,0\n",
      "39881,1\n",
      "39882,1\n",
      "39885,1\n",
      "39886,1\n",
      "39887,1\n",
      "39888,1\n",
      "39890,1\n",
      "39891,1\n",
      "39894,1\n",
      "39897,1\n",
      "39898,1\n",
      "39899,1\n",
      "39901,1\n",
      "39902,1\n",
      "39904,1\n",
      "39905,1\n",
      "39906,1\n",
      "39912,1\n",
      "39913,1\n",
      "39915,1\n",
      "39916,1\n",
      "39920,1\n",
      "39922,1\n",
      "39923,1\n",
      "39925,1\n",
      "39930,1\n",
      "39934,0\n",
      "39937,1\n",
      "39938,1\n",
      "39941,1\n",
      "39942,1\n",
      "39943,1\n",
      "39945,1\n",
      "39948,1\n",
      "39950,1\n",
      "39951,1\n",
      "39952,1\n",
      "39953,1\n",
      "39954,0\n",
      "39958,1\n",
      "39960,0\n",
      "39963,1\n",
      "39964,1\n",
      "39965,1\n",
      "39968,0\n",
      "39972,1\n",
      "39973,1\n",
      "39974,1\n",
      "39977,1\n",
      "39979,0\n",
      "39982,1\n",
      "39984,1\n",
      "39986,0\n",
      "39988,1\n",
      "39990,1\n",
      "39993,1\n",
      "39994,1\n",
      "39996,0\n",
      "39998,1\n",
      "39999,0\n",
      "40000,0\n",
      "40001,0\n",
      "40003,1\n",
      "40005,1\n",
      "40006,1\n",
      "40007,1\n",
      "40012,0\n",
      "40015,0\n",
      "40017,0\n",
      "40019,0\n",
      "40020,0\n",
      "40021,1\n",
      "40022,1\n",
      "40023,0\n",
      "40024,0\n",
      "40027,0\n",
      "40028,1\n",
      "40030,1\n",
      "40033,0\n",
      "40036,0\n",
      "40037,1\n",
      "40040,1\n",
      "40045,1\n",
      "40046,1\n",
      "40051,0\n",
      "40052,0\n",
      "40053,1\n",
      "40054,1\n",
      "40055,1\n",
      "40056,1\n",
      "40059,1\n",
      "40060,0\n",
      "40061,0\n",
      "40065,1\n",
      "40066,1\n",
      "40067,0\n",
      "40068,1\n",
      "40069,1\n",
      "40071,1\n",
      "40073,1\n",
      "40074,1\n",
      "40075,1\n",
      "40077,0\n",
      "40078,0\n",
      "40079,1\n",
      "40080,1\n",
      "40081,1\n",
      "40084,1\n",
      "40085,0\n",
      "40087,1\n",
      "40088,1\n",
      "40089,1\n",
      "40090,0\n",
      "40092,1\n",
      "40094,1\n",
      "40096,1\n",
      "40097,1\n",
      "40098,0\n",
      "40101,1\n",
      "40103,0\n",
      "40104,1\n",
      "40105,1\n",
      "40108,0\n",
      "40109,0\n",
      "40110,1\n",
      "40114,1\n",
      "40116,1\n",
      "40117,1\n",
      "40120,1\n",
      "40122,0\n",
      "40123,1\n",
      "40124,1\n",
      "40125,1\n",
      "40126,0\n",
      "40127,0\n",
      "40128,1\n",
      "40130,1\n",
      "40132,1\n",
      "40133,1\n",
      "40136,1\n",
      "40138,1\n",
      "40140,1\n",
      "40141,1\n",
      "40143,1\n",
      "40144,1\n",
      "40146,0\n",
      "40147,0\n",
      "40149,0\n",
      "40152,1\n",
      "40155,1\n",
      "40158,0\n",
      "40159,0\n",
      "40160,1\n",
      "40161,1\n",
      "40163,0\n",
      "40167,1\n",
      "40168,1\n",
      "40170,0\n",
      "40173,0\n",
      "40174,1\n",
      "40175,1\n",
      "40178,1\n",
      "40179,1\n",
      "40181,1\n",
      "40182,1\n",
      "40183,1\n",
      "40184,1\n",
      "40185,1\n",
      "40186,0\n",
      "40187,1\n",
      "40189,1\n",
      "40190,1\n",
      "40192,0\n",
      "40196,0\n",
      "40201,0\n",
      "40202,1\n",
      "40204,1\n",
      "40207,1\n",
      "40208,1\n",
      "40209,0\n",
      "40211,0\n",
      "40212,1\n",
      "40213,1\n",
      "40214,1\n",
      "40215,1\n",
      "40216,1\n",
      "40217,1\n",
      "40220,1\n",
      "40223,0\n",
      "40225,1\n",
      "40226,1\n",
      "40229,1\n",
      "40231,1\n",
      "40232,1\n",
      "40233,1\n",
      "40234,1\n",
      "40235,1\n",
      "40237,1\n",
      "40238,1\n",
      "40241,1\n",
      "40243,1\n",
      "40244,1\n",
      "40245,1\n",
      "40246,0\n",
      "40248,1\n",
      "40250,1\n",
      "40251,1\n",
      "40252,1\n",
      "40254,1\n",
      "40257,1\n",
      "40258,1\n",
      "40261,1\n",
      "40262,1\n",
      "40263,0\n",
      "40264,0\n",
      "40265,1\n",
      "40266,0\n",
      "40268,0\n",
      "40273,0\n",
      "40274,0\n",
      "40276,1\n",
      "40277,1\n",
      "40278,1\n",
      "40283,1\n",
      "40284,0\n",
      "40285,1\n",
      "40287,1\n",
      "40290,1\n",
      "40292,1\n",
      "40293,1\n",
      "40294,1\n",
      "40295,1\n",
      "40296,1\n",
      "40297,0\n",
      "40298,1\n",
      "40300,1\n",
      "40301,1\n",
      "40303,0\n",
      "40304,1\n",
      "40305,0\n",
      "40306,1\n",
      "40308,1\n",
      "40310,1\n",
      "40311,1\n",
      "40312,0\n",
      "40313,1\n",
      "40315,0\n",
      "40316,0\n",
      "40318,1\n",
      "40320,1\n",
      "40322,1\n",
      "40323,1\n",
      "40326,1\n",
      "40327,1\n",
      "40329,1\n",
      "40331,1\n",
      "40333,0\n",
      "40335,0\n",
      "40336,0\n",
      "40338,1\n",
      "40339,0\n",
      "40340,1\n",
      "40341,1\n",
      "40342,1\n",
      "40343,1\n",
      "40344,1\n",
      "40345,1\n",
      "40346,1\n",
      "40347,1\n",
      "40349,1\n",
      "40351,1\n",
      "40354,1\n",
      "40355,1\n",
      "40356,1\n",
      "40359,0\n",
      "40360,1\n",
      "40362,0\n",
      "40365,0\n",
      "40366,1\n",
      "40367,0\n",
      "40368,1\n",
      "40369,1\n",
      "40374,1\n",
      "40375,1\n",
      "40377,1\n",
      "40378,1\n",
      "40382,0\n",
      "40384,1\n",
      "40386,0\n",
      "40387,0\n",
      "40388,0\n",
      "40389,1\n",
      "40390,1\n",
      "40391,1\n",
      "40393,0\n",
      "40394,0\n",
      "40396,1\n",
      "40397,1\n",
      "40398,1\n",
      "40401,1\n",
      "40402,0\n",
      "40404,1\n",
      "40406,1\n",
      "40408,1\n",
      "40409,1\n",
      "40410,1\n",
      "40411,0\n",
      "40412,0\n",
      "40413,1\n",
      "40414,1\n",
      "40417,1\n",
      "40419,1\n",
      "40421,1\n",
      "40423,1\n",
      "40425,1\n",
      "40426,1\n",
      "40427,1\n",
      "40428,1\n",
      "40429,1\n",
      "40430,1\n",
      "40433,1\n",
      "40434,1\n",
      "40435,1\n",
      "40436,1\n",
      "40438,1\n",
      "40445,1\n",
      "40446,1\n",
      "40447,1\n",
      "40448,1\n",
      "40449,1\n",
      "40450,1\n",
      "40451,1\n"
     ]
    }
   ],
   "source": [
    "with open('/home/spokencall/dnnPaper/expValidation3/vec_st2_test_y.csv', 'r') as csvfile:\n",
    "    lines = csvfile.readlines()\n",
    "    ids = []\n",
    "    for line in lines:\n",
    "        ids.append(line.split(',')[0])\n",
    "    \n",
    "    if len(ids) == len(st2_y_pred):\n",
    "        with open('/home/spokencall/dnnPaper/expValidation3/judgements.csv', 'w+') as result:\n",
    "            for idx, prediction in enumerate(st2_y_pred):\n",
    "                judgment = str(ids[idx]) + ',' + str(prediction).replace('[', '').replace(']', '')\n",
    "                result.write(judgment)\n",
    "                result.write('\\n')\n",
    "                print(judgment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
