{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN_PAPER_V2 regenerate all feature vectors for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clearY(y):\n",
    "    clean_input = np.array([]).reshape(0, 1)\n",
    "    for data in y:\n",
    "        pos1 = data[0]\n",
    "        pos2 = data[1]\n",
    "        pos3 = data[2]\n",
    "        if  pos1 == 1 and pos2 == 0 and pos3 ==0:\n",
    "                clean_input = np.vstack((clean_input, [1]))\n",
    "        else:\n",
    "                clean_input = np.vstack((clean_input, [0]))\n",
    "    return clean_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_y, pred_y):\n",
    "    true_classes = []\n",
    "    for array in true_y:\n",
    "        if np.array_equal(array,[1, 0, 0]):\n",
    "            true_classes.append(0)\n",
    "        elif np.array_equal(array,[0, 1, 0]):\n",
    "            true_classes.append(1)\n",
    "        else:\n",
    "            true_classes.append(2)\n",
    "        \n",
    "    CR, CA, PFA, GFA, FR, k = 0, 0, 0, 0, 0, 3.0\n",
    "    for idx, prediction in enumerate(pred_y):\n",
    "        # the students answer is correct in meaning and language\n",
    "        # the system says the same -> accept\n",
    "        if true_classes[idx] == 0 and prediction == 1:\n",
    "            CA += 1\n",
    "        # the system says correct meaning wrong language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "\n",
    "        # students answer is correct in meaning and wrong in language\n",
    "        #The system says the same -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning and correct language -> accept\n",
    "        elif true_classes[idx] == 1 and prediction == 1:\n",
    "            PFA += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "        # students answer is incorrect in meaning and incorrect in language\n",
    "        # the system says the same -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning correct language -> accept\n",
    "        elif true_classes[idx] == 2 and prediction == 1: \n",
    "            GFA += 1\n",
    "        # the system says correct meaning incorrect language -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "    FA = PFA + k * GFA\n",
    "    Correct = CA + FR\n",
    "    Incorrect = CR + GFA + PFA\n",
    "    IncorrectRejectionRate = CR / ( CR + FA + 0.0 )\n",
    "    CorrectRejectionRate = FR / ( FR + CA + 0.0 )\n",
    "    # Further metrics\n",
    "    Z = CA + CR + FA + FR\n",
    "    Ca = CA / Z\n",
    "    Cr = CR / Z\n",
    "    Fa = FA / Z\n",
    "    Fr = FR / Z\n",
    "    \n",
    "    P = Ca / (Ca + Fa)\n",
    "    R = Ca / (Ca + Fr)\n",
    "    F = (2 * P * R)/( P + R)\n",
    "    \n",
    "    RCa = Ca / (Fr + Ca)\n",
    "    RFa = Fa / (Cr + Fa)\n",
    "    \n",
    "    D = IncorrectRejectionRate / CorrectRejectionRate\n",
    "    Da = RCa / RFa\n",
    "    Df = math.sqrt((Da*D))\n",
    "    \n",
    "    print('\\nINCORRECT UTTERANCES (' + str(Incorrect) + ')' )\n",
    "    print('CorrectReject    ' + str(CR) )\n",
    "    print('GrossFalseAccept ' + str(GFA) + '*' + str(k) + ' = ' + str(GFA * k) )\n",
    "    print('PlainFalseAccept ' + str(PFA) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(IncorrectRejectionRate) )\n",
    "\n",
    "    print('\\nCORRECT UTTERANCES (' + str(Correct) + ')')\n",
    "    print('CorrectAccept    ' + str(CA) )\n",
    "    print('FalseReject      ' + str(FR) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(CorrectRejectionRate) )\n",
    "    \n",
    "    print('\\n--------------REPORT---------------')\n",
    "    print('-----------------------------------')\n",
    "    print('Pr                            ' +  \"{:.3f}\".format(P) )\n",
    "    print('F                             ' +  \"{:.3f}\".format(F) )\n",
    "    print('Sa                            ' +  \"{:.3f}\".format(R) )\n",
    "    #print('R                             ' +  \"{:.3f}\".format(R) )\n",
    "\n",
    "    \n",
    "    print('\\n--------------Metrics--------------')\n",
    "    print('D                             ' +  \"{:.3f}\".format(D) )\n",
    "    print('Da                            ' +  \"{:.3f}\".format(Da) )\n",
    "    print('Df                            ' +  \"{:.3f}\".format(Df) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x:    10092\n",
      "Train_y:    10092\n",
      "\n",
      "\n",
      "Dev_Test_x:    2524\n",
      "Dev_Test_y:    2524\n",
      "\n",
      "\n",
      "St2_Test_x:    1000\n",
      "St2_Test_y:    1000\n"
     ]
    }
   ],
   "source": [
    "train_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_train_x.csv' ,delimiter=',',usecols=range(11)[1:])\n",
    "train_y = clearY(np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_train_y.csv', delimiter=',',usecols=range(4)[1:]))\n",
    "\n",
    "print('Train_x:    ' + str(len(train_x)))\n",
    "print('Train_y:    ' + str(len(train_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "dev_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "dev_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('Dev_Test_x:    ' + str(len(dev_test_x)))\n",
    "print('Dev_Test_y:    ' + str(len(dev_test_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "st2_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_st2_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "st2_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_st2_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('St2_Test_x:    ' + str(len(st2_test_x)))\n",
    "print('St2_Test_y:    ' + str(len(st2_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sclae the vectors inorder to get better classification\n",
    "sc = StandardScaler()\n",
    "scaled_train_x = sc.fit_transform(train_x)\n",
    "scaled_dev_test_x = sc.transform(dev_test_x)\n",
    "scaled_st2_test_x = sc.transform(st2_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(64, activation='relu', input_dim=10))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "classifier.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9082 samples, validate on 1010 samples\n",
      "Epoch 1/600\n",
      "9082/9082 [==============================] - 0s 36us/step - loss: 0.5944 - acc: 0.7120 - val_loss: 0.5325 - val_acc: 0.7287\n",
      "Epoch 2/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.5258 - acc: 0.7470 - val_loss: 0.5079 - val_acc: 0.7396\n",
      "Epoch 3/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.4971 - acc: 0.7553 - val_loss: 0.4788 - val_acc: 0.7713\n",
      "Epoch 4/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.4627 - acc: 0.7777 - val_loss: 0.4482 - val_acc: 0.7861\n",
      "Epoch 5/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.4304 - acc: 0.8071 - val_loss: 0.4263 - val_acc: 0.8119\n",
      "Epoch 6/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.4025 - acc: 0.8324 - val_loss: 0.4171 - val_acc: 0.8218\n",
      "Epoch 7/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3857 - acc: 0.8504 - val_loss: 0.4144 - val_acc: 0.8248\n",
      "Epoch 8/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3740 - acc: 0.8584 - val_loss: 0.4144 - val_acc: 0.8267\n",
      "Epoch 9/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3625 - acc: 0.8638 - val_loss: 0.4123 - val_acc: 0.8327\n",
      "Epoch 10/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3596 - acc: 0.8685 - val_loss: 0.4143 - val_acc: 0.8317\n",
      "Epoch 11/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3537 - acc: 0.8738 - val_loss: 0.4140 - val_acc: 0.8337\n",
      "Epoch 12/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3508 - acc: 0.8755 - val_loss: 0.4222 - val_acc: 0.8337\n",
      "Epoch 13/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3440 - acc: 0.8789 - val_loss: 0.4167 - val_acc: 0.8366\n",
      "Epoch 14/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3449 - acc: 0.8783 - val_loss: 0.4157 - val_acc: 0.8396\n",
      "Epoch 15/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3380 - acc: 0.8815 - val_loss: 0.4187 - val_acc: 0.8396\n",
      "Epoch 16/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3386 - acc: 0.8813 - val_loss: 0.4164 - val_acc: 0.8386\n",
      "Epoch 17/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3393 - acc: 0.8831 - val_loss: 0.4220 - val_acc: 0.8396\n",
      "Epoch 18/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3356 - acc: 0.8857 - val_loss: 0.4175 - val_acc: 0.8416\n",
      "Epoch 19/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3358 - acc: 0.8850 - val_loss: 0.4205 - val_acc: 0.8396\n",
      "Epoch 20/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3384 - acc: 0.8860 - val_loss: 0.4185 - val_acc: 0.8416\n",
      "Epoch 21/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3362 - acc: 0.8856 - val_loss: 0.4162 - val_acc: 0.8396\n",
      "Epoch 22/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3330 - acc: 0.8888 - val_loss: 0.4241 - val_acc: 0.8416\n",
      "Epoch 23/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3350 - acc: 0.8876 - val_loss: 0.4246 - val_acc: 0.8406\n",
      "Epoch 24/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3300 - acc: 0.8878 - val_loss: 0.4202 - val_acc: 0.8406\n",
      "Epoch 25/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3302 - acc: 0.8891 - val_loss: 0.4253 - val_acc: 0.8416\n",
      "Epoch 26/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3292 - acc: 0.8886 - val_loss: 0.4200 - val_acc: 0.8406\n",
      "Epoch 27/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3271 - acc: 0.8892 - val_loss: 0.4207 - val_acc: 0.8406\n",
      "Epoch 28/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3299 - acc: 0.8885 - val_loss: 0.4236 - val_acc: 0.8406\n",
      "Epoch 29/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3274 - acc: 0.8891 - val_loss: 0.4302 - val_acc: 0.8396\n",
      "Epoch 30/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3263 - acc: 0.8912 - val_loss: 0.4252 - val_acc: 0.8416\n",
      "Epoch 31/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3293 - acc: 0.8901 - val_loss: 0.4204 - val_acc: 0.8386\n",
      "Epoch 32/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3271 - acc: 0.8896 - val_loss: 0.4244 - val_acc: 0.8426\n",
      "Epoch 33/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3233 - acc: 0.8921 - val_loss: 0.4250 - val_acc: 0.8406\n",
      "Epoch 34/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3265 - acc: 0.8893 - val_loss: 0.4231 - val_acc: 0.8386\n",
      "Epoch 35/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3263 - acc: 0.8898 - val_loss: 0.4301 - val_acc: 0.8406\n",
      "Epoch 36/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3249 - acc: 0.8882 - val_loss: 0.4262 - val_acc: 0.8406\n",
      "Epoch 37/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3252 - acc: 0.8911 - val_loss: 0.4210 - val_acc: 0.8386\n",
      "Epoch 38/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3255 - acc: 0.8919 - val_loss: 0.4226 - val_acc: 0.8356\n",
      "Epoch 39/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3240 - acc: 0.8914 - val_loss: 0.4269 - val_acc: 0.8386\n",
      "Epoch 40/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3227 - acc: 0.8910 - val_loss: 0.4233 - val_acc: 0.8356\n",
      "Epoch 41/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3233 - acc: 0.8922 - val_loss: 0.4253 - val_acc: 0.8356\n",
      "Epoch 42/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3241 - acc: 0.8911 - val_loss: 0.4305 - val_acc: 0.8386\n",
      "Epoch 43/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3275 - acc: 0.8906 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 44/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3207 - acc: 0.8908 - val_loss: 0.4231 - val_acc: 0.8356\n",
      "Epoch 45/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3204 - acc: 0.8906 - val_loss: 0.4199 - val_acc: 0.8356\n",
      "Epoch 46/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3253 - acc: 0.8912 - val_loss: 0.4299 - val_acc: 0.8386\n",
      "Epoch 47/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3202 - acc: 0.8922 - val_loss: 0.4284 - val_acc: 0.8386\n",
      "Epoch 48/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3253 - acc: 0.8922 - val_loss: 0.4266 - val_acc: 0.8356\n",
      "Epoch 49/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3253 - acc: 0.8915 - val_loss: 0.4253 - val_acc: 0.8356\n",
      "Epoch 50/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3212 - acc: 0.8923 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 51/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3244 - acc: 0.8923 - val_loss: 0.4291 - val_acc: 0.8386\n",
      "Epoch 52/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3246 - acc: 0.8925 - val_loss: 0.4237 - val_acc: 0.8376\n",
      "Epoch 53/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3207 - acc: 0.8921 - val_loss: 0.4306 - val_acc: 0.8376\n",
      "Epoch 54/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3235 - acc: 0.8914 - val_loss: 0.4270 - val_acc: 0.8366\n",
      "Epoch 55/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3190 - acc: 0.8912 - val_loss: 0.4254 - val_acc: 0.8356\n",
      "Epoch 56/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3210 - acc: 0.8906 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 57/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3170 - acc: 0.8928 - val_loss: 0.4358 - val_acc: 0.8406\n",
      "Epoch 58/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3239 - acc: 0.8921 - val_loss: 0.4311 - val_acc: 0.8386\n",
      "Epoch 59/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3193 - acc: 0.8924 - val_loss: 0.4285 - val_acc: 0.8376\n",
      "Epoch 60/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3223 - acc: 0.8926 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 61/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3206 - acc: 0.8917 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 62/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3199 - acc: 0.8931 - val_loss: 0.4258 - val_acc: 0.8347\n",
      "Epoch 63/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3165 - acc: 0.8924 - val_loss: 0.4293 - val_acc: 0.8356\n",
      "Epoch 64/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3201 - acc: 0.8926 - val_loss: 0.4294 - val_acc: 0.8366\n",
      "Epoch 65/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3169 - acc: 0.8926 - val_loss: 0.4333 - val_acc: 0.8386\n",
      "Epoch 66/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3233 - acc: 0.8925 - val_loss: 0.4365 - val_acc: 0.8386\n",
      "Epoch 67/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3187 - acc: 0.8921 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 68/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3199 - acc: 0.8922 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 69/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3208 - acc: 0.8930 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 70/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3192 - acc: 0.8926 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 71/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3163 - acc: 0.8929 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 72/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3183 - acc: 0.8932 - val_loss: 0.4304 - val_acc: 0.8347\n",
      "Epoch 73/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3213 - acc: 0.8917 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 74/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3199 - acc: 0.8928 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 75/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3193 - acc: 0.8919 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 76/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3170 - acc: 0.8922 - val_loss: 0.4311 - val_acc: 0.8347\n",
      "Epoch 77/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3147 - acc: 0.8940 - val_loss: 0.4338 - val_acc: 0.8356\n",
      "Epoch 78/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3172 - acc: 0.8924 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 79/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3166 - acc: 0.8925 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 80/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3162 - acc: 0.8933 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 81/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3165 - acc: 0.8926 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 82/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3169 - acc: 0.8931 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 83/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3149 - acc: 0.8928 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 84/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3175 - acc: 0.8926 - val_loss: 0.4317 - val_acc: 0.8347\n",
      "Epoch 85/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3167 - acc: 0.8935 - val_loss: 0.4364 - val_acc: 0.8386\n",
      "Epoch 86/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3184 - acc: 0.8932 - val_loss: 0.4289 - val_acc: 0.8366\n",
      "Epoch 87/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3181 - acc: 0.8925 - val_loss: 0.4258 - val_acc: 0.8347\n",
      "Epoch 88/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3171 - acc: 0.8931 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 89/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3159 - acc: 0.8930 - val_loss: 0.4291 - val_acc: 0.8347\n",
      "Epoch 90/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3171 - acc: 0.8934 - val_loss: 0.4302 - val_acc: 0.8347\n",
      "Epoch 91/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3136 - acc: 0.8941 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 92/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3154 - acc: 0.8926 - val_loss: 0.4303 - val_acc: 0.8347\n",
      "Epoch 93/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3186 - acc: 0.8925 - val_loss: 0.4304 - val_acc: 0.8347\n",
      "Epoch 94/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3190 - acc: 0.8930 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 95/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3131 - acc: 0.8928 - val_loss: 0.4276 - val_acc: 0.8386\n",
      "Epoch 96/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3152 - acc: 0.8920 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 97/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3147 - acc: 0.8932 - val_loss: 0.4321 - val_acc: 0.8347\n",
      "Epoch 98/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3176 - acc: 0.8930 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 99/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3159 - acc: 0.8937 - val_loss: 0.4289 - val_acc: 0.8347\n",
      "Epoch 100/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3191 - acc: 0.8932 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 101/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3163 - acc: 0.8936 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 102/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3172 - acc: 0.8939 - val_loss: 0.4303 - val_acc: 0.8347\n",
      "Epoch 103/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3160 - acc: 0.8923 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 104/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3156 - acc: 0.8930 - val_loss: 0.4286 - val_acc: 0.8347\n",
      "Epoch 105/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3190 - acc: 0.8928 - val_loss: 0.4328 - val_acc: 0.8366\n",
      "Epoch 106/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3133 - acc: 0.8933 - val_loss: 0.4292 - val_acc: 0.8347\n",
      "Epoch 107/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3150 - acc: 0.8923 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 108/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3115 - acc: 0.8922 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 109/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3153 - acc: 0.8932 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 110/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3146 - acc: 0.8925 - val_loss: 0.4356 - val_acc: 0.8347\n",
      "Epoch 111/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3147 - acc: 0.8937 - val_loss: 0.4308 - val_acc: 0.8347\n",
      "Epoch 112/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3164 - acc: 0.8933 - val_loss: 0.4325 - val_acc: 0.8347\n",
      "Epoch 113/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3158 - acc: 0.8924 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 114/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3145 - acc: 0.8930 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 115/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3133 - acc: 0.8925 - val_loss: 0.4268 - val_acc: 0.8347\n",
      "Epoch 116/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3152 - acc: 0.8934 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 117/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3120 - acc: 0.8932 - val_loss: 0.4284 - val_acc: 0.8356\n",
      "Epoch 118/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3162 - acc: 0.8936 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 119/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3148 - acc: 0.8929 - val_loss: 0.4249 - val_acc: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3153 - acc: 0.8930 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 121/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3130 - acc: 0.8944 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 122/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3115 - acc: 0.8918 - val_loss: 0.4291 - val_acc: 0.8347\n",
      "Epoch 123/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3137 - acc: 0.8925 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 124/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3142 - acc: 0.8930 - val_loss: 0.4320 - val_acc: 0.8347\n",
      "Epoch 125/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3132 - acc: 0.8939 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 126/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3171 - acc: 0.8924 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 127/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3133 - acc: 0.8936 - val_loss: 0.4343 - val_acc: 0.8366\n",
      "Epoch 128/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3130 - acc: 0.8934 - val_loss: 0.4344 - val_acc: 0.8347\n",
      "Epoch 129/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3136 - acc: 0.8932 - val_loss: 0.4258 - val_acc: 0.8347\n",
      "Epoch 130/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3135 - acc: 0.8935 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 131/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3150 - acc: 0.8920 - val_loss: 0.4290 - val_acc: 0.8347\n",
      "Epoch 132/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3132 - acc: 0.8930 - val_loss: 0.4324 - val_acc: 0.8347\n",
      "Epoch 133/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3154 - acc: 0.8934 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 134/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3162 - acc: 0.8935 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 135/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3151 - acc: 0.8936 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 136/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3121 - acc: 0.8929 - val_loss: 0.4269 - val_acc: 0.8347\n",
      "Epoch 137/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3152 - acc: 0.8936 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 138/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3139 - acc: 0.8929 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 139/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3134 - acc: 0.8933 - val_loss: 0.4301 - val_acc: 0.8347\n",
      "Epoch 140/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3109 - acc: 0.8940 - val_loss: 0.4329 - val_acc: 0.8356\n",
      "Epoch 141/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3138 - acc: 0.8929 - val_loss: 0.4337 - val_acc: 0.8356\n",
      "Epoch 142/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3138 - acc: 0.8925 - val_loss: 0.4295 - val_acc: 0.8347\n",
      "Epoch 143/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3125 - acc: 0.8939 - val_loss: 0.4294 - val_acc: 0.8347\n",
      "Epoch 144/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3123 - acc: 0.8923 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 145/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3135 - acc: 0.8934 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 146/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3132 - acc: 0.8932 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 147/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3107 - acc: 0.8933 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 148/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3112 - acc: 0.8930 - val_loss: 0.4316 - val_acc: 0.8356\n",
      "Epoch 149/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3145 - acc: 0.8929 - val_loss: 0.4294 - val_acc: 0.8347\n",
      "Epoch 150/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3103 - acc: 0.8934 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 151/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3105 - acc: 0.8942 - val_loss: 0.4317 - val_acc: 0.8347\n",
      "Epoch 152/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3137 - acc: 0.8931 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 153/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3120 - acc: 0.8932 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 154/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3140 - acc: 0.8923 - val_loss: 0.4291 - val_acc: 0.8347\n",
      "Epoch 155/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3171 - acc: 0.8920 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 156/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3135 - acc: 0.8921 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 157/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3148 - acc: 0.8930 - val_loss: 0.4286 - val_acc: 0.8347\n",
      "Epoch 158/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3104 - acc: 0.8937 - val_loss: 0.4311 - val_acc: 0.8347\n",
      "Epoch 159/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3123 - acc: 0.8922 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 160/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3124 - acc: 0.8933 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 161/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3121 - acc: 0.8924 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 162/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3110 - acc: 0.8928 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 163/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3122 - acc: 0.8931 - val_loss: 0.4306 - val_acc: 0.8347\n",
      "Epoch 164/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3148 - acc: 0.8932 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 165/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3129 - acc: 0.8930 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 166/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3147 - acc: 0.8934 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 167/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3121 - acc: 0.8936 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 168/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3131 - acc: 0.8935 - val_loss: 0.4290 - val_acc: 0.8347\n",
      "Epoch 169/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3147 - acc: 0.8934 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 170/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3115 - acc: 0.8933 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 171/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3126 - acc: 0.8937 - val_loss: 0.4265 - val_acc: 0.8366\n",
      "Epoch 172/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3116 - acc: 0.8943 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 173/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3127 - acc: 0.8933 - val_loss: 0.4326 - val_acc: 0.8347\n",
      "Epoch 174/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3120 - acc: 0.8931 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 175/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3110 - acc: 0.8939 - val_loss: 0.4302 - val_acc: 0.8347\n",
      "Epoch 176/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3100 - acc: 0.8939 - val_loss: 0.4297 - val_acc: 0.8347\n",
      "Epoch 177/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3123 - acc: 0.8937 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 178/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3087 - acc: 0.8937 - val_loss: 0.4361 - val_acc: 0.8347\n",
      "Epoch 179/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3111 - acc: 0.8933 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 180/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3106 - acc: 0.8929 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 181/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3123 - acc: 0.8933 - val_loss: 0.4306 - val_acc: 0.8347\n",
      "Epoch 182/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3112 - acc: 0.8924 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 183/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3109 - acc: 0.8940 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 184/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3118 - acc: 0.8931 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 185/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3115 - acc: 0.8923 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 186/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3126 - acc: 0.8942 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 187/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3124 - acc: 0.8928 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 188/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3137 - acc: 0.8933 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 189/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3140 - acc: 0.8933 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 190/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3123 - acc: 0.8935 - val_loss: 0.4374 - val_acc: 0.8366\n",
      "Epoch 191/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3122 - acc: 0.8943 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 192/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8932 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 193/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3123 - acc: 0.8930 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 194/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3107 - acc: 0.8943 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 195/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3097 - acc: 0.8935 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 196/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3119 - acc: 0.8941 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 197/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3103 - acc: 0.8933 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 198/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3083 - acc: 0.8941 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 199/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3118 - acc: 0.8936 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 200/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3113 - acc: 0.8934 - val_loss: 0.4280 - val_acc: 0.8347\n",
      "Epoch 201/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3123 - acc: 0.8923 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 202/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3137 - acc: 0.8936 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 203/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3094 - acc: 0.8933 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 204/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3106 - acc: 0.8936 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 205/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3108 - acc: 0.8933 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 206/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3092 - acc: 0.8934 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 207/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3115 - acc: 0.8925 - val_loss: 0.4279 - val_acc: 0.8347\n",
      "Epoch 208/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3109 - acc: 0.8931 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 209/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3108 - acc: 0.8944 - val_loss: 0.4286 - val_acc: 0.8347\n",
      "Epoch 210/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3099 - acc: 0.8930 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 211/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3121 - acc: 0.8929 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 212/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3108 - acc: 0.8939 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 213/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3107 - acc: 0.8929 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 214/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3115 - acc: 0.8940 - val_loss: 0.4274 - val_acc: 0.8356\n",
      "Epoch 215/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3119 - acc: 0.8930 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 216/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3107 - acc: 0.8941 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 217/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3095 - acc: 0.8932 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 218/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3097 - acc: 0.8933 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 219/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3074 - acc: 0.8942 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 220/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3109 - acc: 0.8931 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 221/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3090 - acc: 0.8939 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 222/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3092 - acc: 0.8930 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 223/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3076 - acc: 0.8925 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 224/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3098 - acc: 0.8939 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 225/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3089 - acc: 0.8935 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 226/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3091 - acc: 0.8940 - val_loss: 0.4212 - val_acc: 0.8327\n",
      "Epoch 227/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3097 - acc: 0.8932 - val_loss: 0.4301 - val_acc: 0.8347\n",
      "Epoch 228/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3078 - acc: 0.8940 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 229/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3115 - acc: 0.8933 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 230/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3136 - acc: 0.8934 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 231/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3103 - acc: 0.8936 - val_loss: 0.4330 - val_acc: 0.8347\n",
      "Epoch 232/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3109 - acc: 0.8935 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 233/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3086 - acc: 0.8934 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 234/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3107 - acc: 0.8937 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 235/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3098 - acc: 0.8936 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 236/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3119 - acc: 0.8931 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 237/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3079 - acc: 0.8941 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 238/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3065 - acc: 0.8936 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 239/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3145 - acc: 0.8940 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 240/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3090 - acc: 0.8935 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 241/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3095 - acc: 0.8931 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 242/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3052 - acc: 0.8930 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 243/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3115 - acc: 0.8937 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 244/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3097 - acc: 0.8929 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 245/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3109 - acc: 0.8925 - val_loss: 0.4273 - val_acc: 0.8347\n",
      "Epoch 246/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3090 - acc: 0.8942 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 247/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3086 - acc: 0.8932 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 248/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3085 - acc: 0.8932 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 249/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3100 - acc: 0.8923 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 250/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3074 - acc: 0.8940 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 251/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3107 - acc: 0.8932 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 252/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3099 - acc: 0.8934 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 253/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3089 - acc: 0.8930 - val_loss: 0.4271 - val_acc: 0.8356\n",
      "Epoch 254/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3115 - acc: 0.8930 - val_loss: 0.4288 - val_acc: 0.8347\n",
      "Epoch 255/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3092 - acc: 0.8936 - val_loss: 0.4154 - val_acc: 0.8327\n",
      "Epoch 256/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3096 - acc: 0.8941 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 257/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3099 - acc: 0.8930 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 258/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3099 - acc: 0.8943 - val_loss: 0.4317 - val_acc: 0.8347\n",
      "Epoch 259/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3097 - acc: 0.8937 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 260/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3059 - acc: 0.8941 - val_loss: 0.4328 - val_acc: 0.8347\n",
      "Epoch 261/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3081 - acc: 0.8941 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 262/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3116 - acc: 0.8940 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 263/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3093 - acc: 0.8933 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 264/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3113 - acc: 0.8936 - val_loss: 0.4340 - val_acc: 0.8347\n",
      "Epoch 265/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3060 - acc: 0.8936 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 266/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3067 - acc: 0.8942 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 267/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3088 - acc: 0.8935 - val_loss: 0.4168 - val_acc: 0.8327\n",
      "Epoch 268/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3080 - acc: 0.8925 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 269/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3081 - acc: 0.8935 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 270/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3099 - acc: 0.8941 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 271/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3106 - acc: 0.8932 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 272/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3078 - acc: 0.8935 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 273/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3111 - acc: 0.8934 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 274/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3038 - acc: 0.8935 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 275/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3118 - acc: 0.8931 - val_loss: 0.4292 - val_acc: 0.8347\n",
      "Epoch 276/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3097 - acc: 0.8941 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 277/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3068 - acc: 0.8934 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 278/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3086 - acc: 0.8926 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 279/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3093 - acc: 0.8940 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 280/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3090 - acc: 0.8934 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 281/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3095 - acc: 0.8932 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 282/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8934 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 283/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3070 - acc: 0.8940 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 284/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3063 - acc: 0.8941 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 285/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3100 - acc: 0.8936 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 286/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3085 - acc: 0.8934 - val_loss: 0.4273 - val_acc: 0.8347\n",
      "Epoch 287/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3073 - acc: 0.8925 - val_loss: 0.4327 - val_acc: 0.8347\n",
      "Epoch 288/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3084 - acc: 0.8943 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 289/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3078 - acc: 0.8931 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 290/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3076 - acc: 0.8945 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 291/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3078 - acc: 0.8939 - val_loss: 0.4270 - val_acc: 0.8347\n",
      "Epoch 292/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3064 - acc: 0.8935 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 293/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8934 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 294/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3082 - acc: 0.8934 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 295/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3049 - acc: 0.8935 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 296/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3087 - acc: 0.8935 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 297/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3075 - acc: 0.8933 - val_loss: 0.4341 - val_acc: 0.8347\n",
      "Epoch 298/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3088 - acc: 0.8932 - val_loss: 0.4187 - val_acc: 0.8327\n",
      "Epoch 299/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3079 - acc: 0.8936 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 300/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3083 - acc: 0.8930 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 301/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3080 - acc: 0.8935 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 302/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3101 - acc: 0.8933 - val_loss: 0.4193 - val_acc: 0.8327\n",
      "Epoch 303/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3070 - acc: 0.8931 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 304/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3108 - acc: 0.8935 - val_loss: 0.4212 - val_acc: 0.8347\n",
      "Epoch 305/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3066 - acc: 0.8940 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 306/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3083 - acc: 0.8926 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 307/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3075 - acc: 0.8939 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 308/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3068 - acc: 0.8934 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 309/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3060 - acc: 0.8937 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 310/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3083 - acc: 0.8941 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 311/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3121 - acc: 0.8926 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 312/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8943 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 313/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3064 - acc: 0.8937 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 314/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3082 - acc: 0.8928 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 315/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3095 - acc: 0.8935 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 316/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3101 - acc: 0.8936 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 317/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3063 - acc: 0.8936 - val_loss: 0.4280 - val_acc: 0.8347\n",
      "Epoch 318/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3061 - acc: 0.8934 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 319/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3042 - acc: 0.8934 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 320/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3112 - acc: 0.8936 - val_loss: 0.4299 - val_acc: 0.8347\n",
      "Epoch 321/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3092 - acc: 0.8925 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 322/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3097 - acc: 0.8936 - val_loss: 0.4255 - val_acc: 0.8347\n",
      "Epoch 323/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3061 - acc: 0.8942 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 324/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8937 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 325/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8931 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 326/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8932 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 327/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3084 - acc: 0.8944 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 328/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8939 - val_loss: 0.4293 - val_acc: 0.8347\n",
      "Epoch 329/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3070 - acc: 0.8931 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 330/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3064 - acc: 0.8942 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 331/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3104 - acc: 0.8934 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 332/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3063 - acc: 0.8931 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 333/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8937 - val_loss: 0.4345 - val_acc: 0.8347\n",
      "Epoch 334/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3077 - acc: 0.8939 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 335/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3080 - acc: 0.8936 - val_loss: 0.4279 - val_acc: 0.8347\n",
      "Epoch 336/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3075 - acc: 0.8935 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 337/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3096 - acc: 0.8934 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 338/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3092 - acc: 0.8934 - val_loss: 0.4216 - val_acc: 0.8327\n",
      "Epoch 339/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3106 - acc: 0.8941 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 340/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3086 - acc: 0.8925 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 341/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8930 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 342/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3081 - acc: 0.8935 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 343/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3071 - acc: 0.8934 - val_loss: 0.4142 - val_acc: 0.8327\n",
      "Epoch 344/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3074 - acc: 0.8936 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 345/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3061 - acc: 0.8931 - val_loss: 0.4190 - val_acc: 0.8327\n",
      "Epoch 346/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3070 - acc: 0.8933 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 347/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3057 - acc: 0.8941 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 348/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3091 - acc: 0.8937 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 349/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3073 - acc: 0.8937 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 350/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3081 - acc: 0.8934 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 351/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3067 - acc: 0.8941 - val_loss: 0.4274 - val_acc: 0.8347\n",
      "Epoch 352/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3087 - acc: 0.8932 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 353/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3071 - acc: 0.8939 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 354/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3059 - acc: 0.8932 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 355/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3075 - acc: 0.8937 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 356/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3056 - acc: 0.8925 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 357/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3081 - acc: 0.8935 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 358/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3056 - acc: 0.8933 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 359/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3090 - acc: 0.8936 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 360/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8941 - val_loss: 0.4289 - val_acc: 0.8347\n",
      "Epoch 361/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8934 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 362/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3100 - acc: 0.8930 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 363/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3075 - acc: 0.8936 - val_loss: 0.4097 - val_acc: 0.8347\n",
      "Epoch 364/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8942 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 365/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3074 - acc: 0.8924 - val_loss: 0.4304 - val_acc: 0.8347\n",
      "Epoch 366/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8934 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 367/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3059 - acc: 0.8939 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 368/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3041 - acc: 0.8937 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 369/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3070 - acc: 0.8934 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 370/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3073 - acc: 0.8942 - val_loss: 0.4327 - val_acc: 0.8347\n",
      "Epoch 371/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3063 - acc: 0.8939 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 372/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3057 - acc: 0.8946 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 373/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3082 - acc: 0.8940 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 374/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3059 - acc: 0.8932 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 375/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3080 - acc: 0.8935 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 376/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3052 - acc: 0.8934 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 377/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3079 - acc: 0.8936 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 378/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3051 - acc: 0.8944 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 379/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3035 - acc: 0.8944 - val_loss: 0.4164 - val_acc: 0.8327\n",
      "Epoch 380/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3077 - acc: 0.8935 - val_loss: 0.4152 - val_acc: 0.8347\n",
      "Epoch 381/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3045 - acc: 0.8931 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 382/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3074 - acc: 0.8928 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 383/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3041 - acc: 0.8941 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 384/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3077 - acc: 0.8928 - val_loss: 0.4313 - val_acc: 0.8347\n",
      "Epoch 385/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8935 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 386/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3057 - acc: 0.8937 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 387/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8931 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 388/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3072 - acc: 0.8939 - val_loss: 0.4143 - val_acc: 0.8347\n",
      "Epoch 389/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3056 - acc: 0.8940 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 390/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3076 - acc: 0.8935 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 391/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3060 - acc: 0.8928 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 392/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3072 - acc: 0.8946 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 393/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3064 - acc: 0.8935 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 394/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3042 - acc: 0.8931 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 395/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3068 - acc: 0.8940 - val_loss: 0.4175 - val_acc: 0.8347\n",
      "Epoch 396/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3069 - acc: 0.8941 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 397/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3074 - acc: 0.8944 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 398/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3028 - acc: 0.8934 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 399/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3050 - acc: 0.8940 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 400/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3108 - acc: 0.8939 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 401/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3065 - acc: 0.8936 - val_loss: 0.4115 - val_acc: 0.8327\n",
      "Epoch 402/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3040 - acc: 0.8937 - val_loss: 0.4149 - val_acc: 0.8327\n",
      "Epoch 403/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3070 - acc: 0.8941 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 404/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3048 - acc: 0.8933 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 405/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3071 - acc: 0.8940 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 406/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3085 - acc: 0.8940 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 407/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3056 - acc: 0.8934 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 408/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3053 - acc: 0.8936 - val_loss: 0.4269 - val_acc: 0.8347\n",
      "Epoch 409/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3044 - acc: 0.8941 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 410/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3076 - acc: 0.8928 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 411/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3078 - acc: 0.8939 - val_loss: 0.4110 - val_acc: 0.8327\n",
      "Epoch 412/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3071 - acc: 0.8941 - val_loss: 0.4118 - val_acc: 0.8347\n",
      "Epoch 413/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3034 - acc: 0.8942 - val_loss: 0.4112 - val_acc: 0.8347\n",
      "Epoch 414/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3061 - acc: 0.8941 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 415/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8943 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 416/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3089 - acc: 0.8932 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 417/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3040 - acc: 0.8934 - val_loss: 0.4106 - val_acc: 0.8327\n",
      "Epoch 418/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3075 - acc: 0.8934 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 419/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3062 - acc: 0.8943 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 420/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3037 - acc: 0.8932 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 421/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8943 - val_loss: 0.4119 - val_acc: 0.8347\n",
      "Epoch 422/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3057 - acc: 0.8932 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 423/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3038 - acc: 0.8937 - val_loss: 0.4220 - val_acc: 0.8327\n",
      "Epoch 424/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3046 - acc: 0.8941 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 425/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3074 - acc: 0.8939 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 426/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3045 - acc: 0.8943 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 427/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3057 - acc: 0.8943 - val_loss: 0.4116 - val_acc: 0.8347\n",
      "Epoch 428/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3047 - acc: 0.8939 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 429/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3073 - acc: 0.8936 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 430/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3066 - acc: 0.8940 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 431/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3037 - acc: 0.8936 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 432/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3087 - acc: 0.8940 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 433/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3044 - acc: 0.8930 - val_loss: 0.4367 - val_acc: 0.8356\n",
      "Epoch 434/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3086 - acc: 0.8932 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 435/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3061 - acc: 0.8942 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 436/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3052 - acc: 0.8940 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 437/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8930 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 438/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3045 - acc: 0.8934 - val_loss: 0.4308 - val_acc: 0.8347\n",
      "Epoch 439/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.8929 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 440/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3054 - acc: 0.8936 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 441/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8934 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 442/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3072 - acc: 0.8937 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 443/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3028 - acc: 0.8934 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 444/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3044 - acc: 0.8933 - val_loss: 0.4263 - val_acc: 0.8347\n",
      "Epoch 445/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3043 - acc: 0.8946 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 446/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3065 - acc: 0.8929 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 447/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3044 - acc: 0.8940 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 448/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3058 - acc: 0.8933 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 449/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3044 - acc: 0.8943 - val_loss: 0.4125 - val_acc: 0.8347\n",
      "Epoch 450/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8933 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 451/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3047 - acc: 0.8942 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 452/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3061 - acc: 0.8941 - val_loss: 0.4249 - val_acc: 0.8327\n",
      "Epoch 453/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3031 - acc: 0.8933 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 454/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3055 - acc: 0.8939 - val_loss: 0.4289 - val_acc: 0.8347\n",
      "Epoch 455/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3077 - acc: 0.8940 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 456/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3058 - acc: 0.8940 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 457/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3045 - acc: 0.8944 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 458/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3060 - acc: 0.8933 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 459/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3050 - acc: 0.8935 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 460/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3060 - acc: 0.8942 - val_loss: 0.4153 - val_acc: 0.8327\n",
      "Epoch 461/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3062 - acc: 0.8934 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 462/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3062 - acc: 0.8939 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 463/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3053 - acc: 0.8939 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 464/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3043 - acc: 0.8937 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 465/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3053 - acc: 0.8932 - val_loss: 0.4268 - val_acc: 0.8347\n",
      "Epoch 466/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3055 - acc: 0.8936 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 467/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3051 - acc: 0.8941 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 468/600\n",
      "9082/9082 [==============================] - 0s 25us/step - loss: 0.3041 - acc: 0.8930 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 469/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3033 - acc: 0.8942 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 470/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3039 - acc: 0.8939 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 471/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3041 - acc: 0.8935 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 472/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3054 - acc: 0.8933 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 473/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8936 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 474/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3038 - acc: 0.8937 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 475/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3048 - acc: 0.8940 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 476/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3046 - acc: 0.8933 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 477/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3049 - acc: 0.8936 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 478/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3023 - acc: 0.8947 - val_loss: 0.4064 - val_acc: 0.8347\n",
      "Epoch 479/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3049 - acc: 0.8928 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 480/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3034 - acc: 0.8936 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 481/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3066 - acc: 0.8941 - val_loss: 0.4101 - val_acc: 0.8347\n",
      "Epoch 482/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3055 - acc: 0.8939 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 483/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3018 - acc: 0.8943 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 484/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8941 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 485/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8941 - val_loss: 0.4310 - val_acc: 0.8347\n",
      "Epoch 486/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3037 - acc: 0.8937 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 487/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3048 - acc: 0.8940 - val_loss: 0.4163 - val_acc: 0.8347\n",
      "Epoch 488/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3035 - acc: 0.8946 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 489/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3039 - acc: 0.8935 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 490/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3033 - acc: 0.8935 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 491/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3046 - acc: 0.8931 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 492/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3029 - acc: 0.8944 - val_loss: 0.4127 - val_acc: 0.8347\n",
      "Epoch 493/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3054 - acc: 0.8932 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 494/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3038 - acc: 0.8941 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 495/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3063 - acc: 0.8941 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 496/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3007 - acc: 0.8943 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 497/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3032 - acc: 0.8937 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 498/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3053 - acc: 0.8940 - val_loss: 0.4168 - val_acc: 0.8347\n",
      "Epoch 499/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8935 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 500/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3031 - acc: 0.8942 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 501/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3060 - acc: 0.8946 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 502/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3050 - acc: 0.8941 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 503/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3068 - acc: 0.8935 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 504/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3045 - acc: 0.8939 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 505/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3023 - acc: 0.8933 - val_loss: 0.4134 - val_acc: 0.8327\n",
      "Epoch 506/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8933 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 507/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3027 - acc: 0.8936 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 508/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8939 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 509/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3065 - acc: 0.8936 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 510/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3057 - acc: 0.8936 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 511/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3029 - acc: 0.8943 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 512/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3057 - acc: 0.8932 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 513/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8935 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 514/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3041 - acc: 0.8934 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 515/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3072 - acc: 0.8939 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 516/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3036 - acc: 0.8936 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 517/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3018 - acc: 0.8939 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 518/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3040 - acc: 0.8940 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 519/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8934 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 520/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3071 - acc: 0.8934 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 521/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3072 - acc: 0.8932 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 522/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3062 - acc: 0.8942 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 523/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3022 - acc: 0.8937 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 524/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3026 - acc: 0.8941 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 525/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3060 - acc: 0.8936 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 526/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8932 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 527/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3047 - acc: 0.8933 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 528/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3048 - acc: 0.8940 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 529/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 530/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3059 - acc: 0.8940 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 531/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3045 - acc: 0.8935 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 532/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3014 - acc: 0.8936 - val_loss: 0.4092 - val_acc: 0.8347\n",
      "Epoch 533/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3050 - acc: 0.8935 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 534/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3025 - acc: 0.8934 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 535/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3033 - acc: 0.8936 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 536/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3072 - acc: 0.8935 - val_loss: 0.4136 - val_acc: 0.8347\n",
      "Epoch 537/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3060 - acc: 0.8935 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 538/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3039 - acc: 0.8944 - val_loss: 0.4118 - val_acc: 0.8347\n",
      "Epoch 539/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3030 - acc: 0.8939 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 540/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3013 - acc: 0.8948 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 541/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3061 - acc: 0.8937 - val_loss: 0.4141 - val_acc: 0.8347\n",
      "Epoch 542/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.2988 - acc: 0.8941 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 543/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3035 - acc: 0.8934 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 544/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3057 - acc: 0.8940 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 545/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3017 - acc: 0.8946 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 546/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3029 - acc: 0.8934 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 547/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3049 - acc: 0.8937 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 548/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8937 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 549/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3051 - acc: 0.8932 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 550/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3063 - acc: 0.8941 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 551/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3040 - acc: 0.8935 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 552/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3023 - acc: 0.8939 - val_loss: 0.4157 - val_acc: 0.8347\n",
      "Epoch 553/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3045 - acc: 0.8943 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 554/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3045 - acc: 0.8944 - val_loss: 0.4121 - val_acc: 0.8327\n",
      "Epoch 555/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8936 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 556/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3042 - acc: 0.8937 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 557/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3044 - acc: 0.8940 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 558/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3062 - acc: 0.8935 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 559/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3042 - acc: 0.8939 - val_loss: 0.4114 - val_acc: 0.8347\n",
      "Epoch 560/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3029 - acc: 0.8931 - val_loss: 0.4104 - val_acc: 0.8347\n",
      "Epoch 561/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3050 - acc: 0.8932 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 562/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3033 - acc: 0.8939 - val_loss: 0.4123 - val_acc: 0.8347\n",
      "Epoch 563/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8942 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 564/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3049 - acc: 0.8941 - val_loss: 0.4098 - val_acc: 0.8327\n",
      "Epoch 565/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3058 - acc: 0.8936 - val_loss: 0.4109 - val_acc: 0.8327\n",
      "Epoch 566/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8936 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 567/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3063 - acc: 0.8940 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 568/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3057 - acc: 0.8935 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 569/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3044 - acc: 0.8944 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 570/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3016 - acc: 0.8944 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 571/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8939 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 572/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3040 - acc: 0.8934 - val_loss: 0.4147 - val_acc: 0.8347\n",
      "Epoch 573/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3014 - acc: 0.8941 - val_loss: 0.4241 - val_acc: 0.8347\n",
      "Epoch 574/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3045 - acc: 0.8940 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 575/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3058 - acc: 0.8935 - val_loss: 0.4181 - val_acc: 0.8327\n",
      "Epoch 576/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8939 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 577/600\n",
      "9082/9082 [==============================] - 0s 21us/step - loss: 0.3043 - acc: 0.8943 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 578/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3039 - acc: 0.8941 - val_loss: 0.4117 - val_acc: 0.8327\n",
      "Epoch 579/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3052 - acc: 0.8946 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 580/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3056 - acc: 0.8931 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 581/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3050 - acc: 0.8936 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 582/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3029 - acc: 0.8948 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 583/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3025 - acc: 0.8941 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 584/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3081 - acc: 0.8937 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 585/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8937 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 586/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3043 - acc: 0.8933 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 587/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3036 - acc: 0.8930 - val_loss: 0.4255 - val_acc: 0.8347\n",
      "Epoch 588/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3028 - acc: 0.8942 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 589/600\n",
      "9082/9082 [==============================] - 0s 22us/step - loss: 0.3019 - acc: 0.8937 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 590/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3043 - acc: 0.8935 - val_loss: 0.4120 - val_acc: 0.8347\n",
      "Epoch 591/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3044 - acc: 0.8942 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 592/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3037 - acc: 0.8936 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 593/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3019 - acc: 0.8940 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 594/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3036 - acc: 0.8933 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 595/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3053 - acc: 0.8935 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 596/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3012 - acc: 0.8939 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 597/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3035 - acc: 0.8942 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 598/600\n",
      "9082/9082 [==============================] - 0s 23us/step - loss: 0.3042 - acc: 0.8935 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 599/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3052 - acc: 0.8940 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 600/600\n",
      "9082/9082 [==============================] - 0s 24us/step - loss: 0.3027 - acc: 0.8931 - val_loss: 0.4122 - val_acc: 0.8347\n"
     ]
    }
   ],
   "source": [
    "# Fitting our model \n",
    "hist = classifier.fit(scaled_train_x, train_y, batch_size = 150, epochs = 600, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8130af9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation2/loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8130af92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation2/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with development test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (726)\n",
      "CorrectReject    628\n",
      "GrossFalseAccept 21*3.0 = 63.0\n",
      "PlainFalseAccept 77\n",
      "RejectionRate    0.818\n",
      "\n",
      "CORRECT UTTERANCES (1798)\n",
      "CorrectAccept    1631\n",
      "FalseReject      167\n",
      "RejectionRate    0.093\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.921\n",
      "F                             0.914\n",
      "Sa                            0.907\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             8.804\n",
      "Da                            4.976\n",
      "Df                            6.619\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred = classifier.predict_classes(scaled_dev_test_x)\n",
    "evaluate(dev_test_y, dev_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with st2 test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (250)\n",
      "CorrectReject    222\n",
      "GrossFalseAccept 1*3.0 = 3.0\n",
      "PlainFalseAccept 27\n",
      "RejectionRate    0.881\n",
      "\n",
      "CORRECT UTTERANCES (750)\n",
      "CorrectAccept    696\n",
      "FalseReject      54\n",
      "RejectionRate    0.072\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.959\n",
      "F                             0.943\n",
      "Sa                            0.928\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             12.235\n",
      "Da                            7.795\n",
      "Df                            9.766\n"
     ]
    }
   ],
   "source": [
    "st2_y_pred = classifier.predict_classes(scaled_st2_test_x)\n",
    "evaluate(st2_test_y, st2_y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
