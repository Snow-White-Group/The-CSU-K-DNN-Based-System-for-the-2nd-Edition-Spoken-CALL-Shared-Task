{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN_PAPER_V2 regenerate all feature vectors for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/spokencall/.conda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clearY(y):\n",
    "    clean_input = np.array([]).reshape(0, 1)\n",
    "    for data in y:\n",
    "        pos1 = data[0]\n",
    "        pos2 = data[1]\n",
    "        pos3 = data[2]\n",
    "        if  pos1 == 1 and pos2 == 0 and pos3 ==0:\n",
    "                clean_input = np.vstack((clean_input, [1]))\n",
    "        else:\n",
    "                clean_input = np.vstack((clean_input, [0]))\n",
    "    return clean_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_y, pred_y):\n",
    "    true_classes = []\n",
    "    for array in true_y:\n",
    "        if np.array_equal(array,[1, 0, 0]):\n",
    "            true_classes.append(0)\n",
    "        elif np.array_equal(array,[0, 1, 0]):\n",
    "            true_classes.append(1)\n",
    "        else:\n",
    "            true_classes.append(2)\n",
    "        \n",
    "    CR, CA, PFA, GFA, FR, k = 0, 0, 0, 0, 0, 3.0\n",
    "    for idx, prediction in enumerate(pred_y):\n",
    "        # the students answer is correct in meaning and language\n",
    "        # the system says the same -> accept\n",
    "        if true_classes[idx] == 0 and prediction == 1:\n",
    "            CA += 1\n",
    "        # the system says correct meaning wrong language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 0 and prediction == 0:\n",
    "            FR += 1\n",
    "\n",
    "        # students answer is correct in meaning and wrong in language\n",
    "        #The system says the same -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning and correct language -> accept\n",
    "        elif true_classes[idx] == 1 and prediction == 1:\n",
    "            PFA += 1\n",
    "        # the system says incorrect meaning and incorrect language -> reject\n",
    "        elif true_classes[idx] == 1 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "        # students answer is incorrect in meaning and incorrect in language\n",
    "        # the system says the same -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "        # the system says correct meaning correct language -> accept\n",
    "        elif true_classes[idx] == 2 and prediction == 1: \n",
    "            GFA += 1\n",
    "        # the system says correct meaning incorrect language -> reject\n",
    "        elif true_classes[idx] == 2 and prediction == 0:\n",
    "            CR += 1\n",
    "\n",
    "    FA = PFA + k * GFA\n",
    "    Correct = CA + FR\n",
    "    Incorrect = CR + GFA + PFA\n",
    "    IncorrectRejectionRate = CR / ( CR + FA + 0.0 )\n",
    "    CorrectRejectionRate = FR / ( FR + CA + 0.0 )\n",
    "    # Further metrics\n",
    "    Z = CA + CR + FA + FR\n",
    "    Ca = CA / Z\n",
    "    Cr = CR / Z\n",
    "    Fa = FA / Z\n",
    "    Fr = FR / Z\n",
    "    \n",
    "    P = Ca / (Ca + Fa)\n",
    "    R = Ca / (Ca + Fr)\n",
    "    SA = Ca + Cr\n",
    "    F = (2 * P * R)/( P + R)\n",
    "    \n",
    "    RCa = Ca / (Fr + Ca)\n",
    "    RFa = Fa / (Cr + Fa)\n",
    "    \n",
    "    D = IncorrectRejectionRate / CorrectRejectionRate\n",
    "    Da = RCa / RFa\n",
    "    Df = math.sqrt((Da*D))\n",
    "    \n",
    "    print('\\nINCORRECT UTTERANCES (' + str(Incorrect) + ')' )\n",
    "    print('CorrectReject    ' + str(CR) )\n",
    "    print('GrossFalseAccept ' + str(GFA) + '*' + str(k) + ' = ' + str(GFA * k) )\n",
    "    print('PlainFalseAccept ' + str(PFA) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(IncorrectRejectionRate) )\n",
    "\n",
    "    print('\\nCORRECT UTTERANCES (' + str(Correct) + ')')\n",
    "    print('CorrectAccept    ' + str(CA) )\n",
    "    print('FalseReject      ' + str(FR) )\n",
    "    print('RejectionRate    ' + \"{:.3f}\".format(CorrectRejectionRate) )\n",
    "    \n",
    "    print('\\n--------------REPORT---------------')\n",
    "    print('-----------------------------------')\n",
    "    print('Pr                            ' +  \"{:.3f}\".format(P) )\n",
    "    print('F                             ' +  \"{:.3f}\".format(F) )\n",
    "    print('Sa                            ' +  \"{:.3f}\".format(SA) )\n",
    "    \n",
    "    print('\\n--------------Metrics--------------')\n",
    "    print('D                             ' +  \"{:.3f}\".format(D) )\n",
    "    print('Da                            ' +  \"{:.3f}\".format(Da) )\n",
    "    print('Df                            ' +  \"{:.3f}\".format(Df) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x:    10092\n",
      "Train_y:    10092\n",
      "\n",
      "\n",
      "Dev_Test_x:    2524\n",
      "Dev_Test_y:    2524\n"
     ]
    }
   ],
   "source": [
    "train_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_train_x.csv' ,delimiter=',',usecols=range(11)[1:])\n",
    "train_y = clearY(np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_train_y.csv', delimiter=',',usecols=range(4)[1:]))\n",
    "\n",
    "print('Train_x:    ' + str(len(train_x)))\n",
    "print('Train_y:    ' + str(len(train_y)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "dev_test_x = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_test_x.csv', delimiter=',',usecols=range(11)[1:])\n",
    "dev_test_y = np.loadtxt('/home/spokencall/dnnPaper/expValidation2/vec_test_y.csv', delimiter=',',usecols=range(4)[1:])\n",
    "print('Dev_Test_x:    ' + str(len(dev_test_x)))\n",
    "print('Dev_Test_y:    ' + str(len(dev_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sclae the vectors inorder to get better classification\n",
    "sc = StandardScaler()\n",
    "scaled_train_x = sc.fit_transform(train_x)\n",
    "scaled_dev_test_x = sc.transform(dev_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(64, activation='relu', input_dim=10))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "classifier.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,929\n",
      "Trainable params: 4,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.summary())\n",
    "plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9082 samples, validate on 1010 samples\n",
      "Epoch 1/600\n",
      "9082/9082 [==============================] - 0s 28us/step - loss: 0.5944 - acc: 0.7120 - val_loss: 0.5325 - val_acc: 0.7287\n",
      "Epoch 2/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.5258 - acc: 0.7468 - val_loss: 0.5080 - val_acc: 0.7396\n",
      "Epoch 3/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.4972 - acc: 0.7555 - val_loss: 0.4788 - val_acc: 0.7713\n",
      "Epoch 4/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.4627 - acc: 0.7777 - val_loss: 0.4482 - val_acc: 0.7851\n",
      "Epoch 5/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.4304 - acc: 0.8071 - val_loss: 0.4264 - val_acc: 0.8119\n",
      "Epoch 6/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.4025 - acc: 0.8327 - val_loss: 0.4171 - val_acc: 0.8228\n",
      "Epoch 7/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3857 - acc: 0.8503 - val_loss: 0.4144 - val_acc: 0.8248\n",
      "Epoch 8/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3739 - acc: 0.8585 - val_loss: 0.4144 - val_acc: 0.8267\n",
      "Epoch 9/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3625 - acc: 0.8639 - val_loss: 0.4121 - val_acc: 0.8327\n",
      "Epoch 10/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3596 - acc: 0.8682 - val_loss: 0.4143 - val_acc: 0.8317\n",
      "Epoch 11/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3537 - acc: 0.8735 - val_loss: 0.4140 - val_acc: 0.8337\n",
      "Epoch 12/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3508 - acc: 0.8756 - val_loss: 0.4223 - val_acc: 0.8337\n",
      "Epoch 13/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3440 - acc: 0.8786 - val_loss: 0.4168 - val_acc: 0.8366\n",
      "Epoch 14/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3449 - acc: 0.8784 - val_loss: 0.4157 - val_acc: 0.8386\n",
      "Epoch 15/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3380 - acc: 0.8815 - val_loss: 0.4188 - val_acc: 0.8396\n",
      "Epoch 16/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3385 - acc: 0.8812 - val_loss: 0.4164 - val_acc: 0.8396\n",
      "Epoch 17/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3393 - acc: 0.8832 - val_loss: 0.4221 - val_acc: 0.8396\n",
      "Epoch 18/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3356 - acc: 0.8858 - val_loss: 0.4174 - val_acc: 0.8406\n",
      "Epoch 19/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3358 - acc: 0.8850 - val_loss: 0.4206 - val_acc: 0.8406\n",
      "Epoch 20/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3384 - acc: 0.8861 - val_loss: 0.4183 - val_acc: 0.8416\n",
      "Epoch 21/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3363 - acc: 0.8859 - val_loss: 0.4164 - val_acc: 0.8396\n",
      "Epoch 22/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3330 - acc: 0.8889 - val_loss: 0.4241 - val_acc: 0.8416\n",
      "Epoch 23/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3351 - acc: 0.8875 - val_loss: 0.4248 - val_acc: 0.8406\n",
      "Epoch 24/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3300 - acc: 0.8876 - val_loss: 0.4204 - val_acc: 0.8406\n",
      "Epoch 25/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3302 - acc: 0.8889 - val_loss: 0.4251 - val_acc: 0.8416\n",
      "Epoch 26/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3292 - acc: 0.8885 - val_loss: 0.4203 - val_acc: 0.8406\n",
      "Epoch 27/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3271 - acc: 0.8891 - val_loss: 0.4207 - val_acc: 0.8406\n",
      "Epoch 28/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3299 - acc: 0.8882 - val_loss: 0.4233 - val_acc: 0.8406\n",
      "Epoch 29/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3273 - acc: 0.8891 - val_loss: 0.4299 - val_acc: 0.8396\n",
      "Epoch 30/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3263 - acc: 0.8910 - val_loss: 0.4249 - val_acc: 0.8416\n",
      "Epoch 31/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3292 - acc: 0.8900 - val_loss: 0.4206 - val_acc: 0.8386\n",
      "Epoch 32/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3271 - acc: 0.8897 - val_loss: 0.4243 - val_acc: 0.8416\n",
      "Epoch 33/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3234 - acc: 0.8918 - val_loss: 0.4248 - val_acc: 0.8386\n",
      "Epoch 34/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3265 - acc: 0.8890 - val_loss: 0.4227 - val_acc: 0.8386\n",
      "Epoch 35/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3264 - acc: 0.8897 - val_loss: 0.4295 - val_acc: 0.8406\n",
      "Epoch 36/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3248 - acc: 0.8886 - val_loss: 0.4264 - val_acc: 0.8396\n",
      "Epoch 37/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3252 - acc: 0.8909 - val_loss: 0.4209 - val_acc: 0.8386\n",
      "Epoch 38/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3254 - acc: 0.8919 - val_loss: 0.4221 - val_acc: 0.8356\n",
      "Epoch 39/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3240 - acc: 0.8915 - val_loss: 0.4269 - val_acc: 0.8386\n",
      "Epoch 40/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3226 - acc: 0.8911 - val_loss: 0.4232 - val_acc: 0.8356\n",
      "Epoch 41/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3232 - acc: 0.8921 - val_loss: 0.4255 - val_acc: 0.8356\n",
      "Epoch 42/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3241 - acc: 0.8910 - val_loss: 0.4302 - val_acc: 0.8386\n",
      "Epoch 43/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3274 - acc: 0.8907 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 44/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3208 - acc: 0.8908 - val_loss: 0.4234 - val_acc: 0.8356\n",
      "Epoch 45/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3204 - acc: 0.8906 - val_loss: 0.4201 - val_acc: 0.8356\n",
      "Epoch 46/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3254 - acc: 0.8914 - val_loss: 0.4299 - val_acc: 0.8386\n",
      "Epoch 47/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3202 - acc: 0.8922 - val_loss: 0.4290 - val_acc: 0.8386\n",
      "Epoch 48/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3253 - acc: 0.8924 - val_loss: 0.4266 - val_acc: 0.8356\n",
      "Epoch 49/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3254 - acc: 0.8920 - val_loss: 0.4254 - val_acc: 0.8356\n",
      "Epoch 50/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3213 - acc: 0.8922 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 51/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3243 - acc: 0.8922 - val_loss: 0.4294 - val_acc: 0.8376\n",
      "Epoch 52/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3246 - acc: 0.8928 - val_loss: 0.4235 - val_acc: 0.8376\n",
      "Epoch 53/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3206 - acc: 0.8921 - val_loss: 0.4302 - val_acc: 0.8376\n",
      "Epoch 54/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3235 - acc: 0.8913 - val_loss: 0.4267 - val_acc: 0.8366\n",
      "Epoch 55/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3190 - acc: 0.8910 - val_loss: 0.4259 - val_acc: 0.8356\n",
      "Epoch 56/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3209 - acc: 0.8907 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 57/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3170 - acc: 0.8930 - val_loss: 0.4355 - val_acc: 0.8386\n",
      "Epoch 58/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3238 - acc: 0.8921 - val_loss: 0.4307 - val_acc: 0.8386\n",
      "Epoch 59/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3193 - acc: 0.8923 - val_loss: 0.4285 - val_acc: 0.8376\n",
      "Epoch 60/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8930 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 61/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3206 - acc: 0.8917 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 62/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3199 - acc: 0.8931 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 63/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3165 - acc: 0.8922 - val_loss: 0.4293 - val_acc: 0.8356\n",
      "Epoch 64/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3200 - acc: 0.8926 - val_loss: 0.4287 - val_acc: 0.8356\n",
      "Epoch 65/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3168 - acc: 0.8928 - val_loss: 0.4339 - val_acc: 0.8386\n",
      "Epoch 66/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3232 - acc: 0.8922 - val_loss: 0.4365 - val_acc: 0.8386\n",
      "Epoch 67/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.8921 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 68/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3199 - acc: 0.8922 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 69/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3208 - acc: 0.8930 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 70/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3190 - acc: 0.8924 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 71/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3163 - acc: 0.8925 - val_loss: 0.4281 - val_acc: 0.8347\n",
      "Epoch 72/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3185 - acc: 0.8933 - val_loss: 0.4303 - val_acc: 0.8347\n",
      "Epoch 73/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3212 - acc: 0.8920 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 74/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3199 - acc: 0.8926 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 75/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3192 - acc: 0.8915 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 76/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3170 - acc: 0.8923 - val_loss: 0.4309 - val_acc: 0.8347\n",
      "Epoch 77/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3147 - acc: 0.8939 - val_loss: 0.4338 - val_acc: 0.8356\n",
      "Epoch 78/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3170 - acc: 0.8924 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 79/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3166 - acc: 0.8923 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 80/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3162 - acc: 0.8934 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 81/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3165 - acc: 0.8925 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 82/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3169 - acc: 0.8931 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 83/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3148 - acc: 0.8928 - val_loss: 0.4273 - val_acc: 0.8347\n",
      "Epoch 84/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3173 - acc: 0.8931 - val_loss: 0.4320 - val_acc: 0.8347\n",
      "Epoch 85/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3166 - acc: 0.8932 - val_loss: 0.4365 - val_acc: 0.8386\n",
      "Epoch 86/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3184 - acc: 0.8928 - val_loss: 0.4289 - val_acc: 0.8366\n",
      "Epoch 87/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3176 - acc: 0.8924 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 88/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3170 - acc: 0.8933 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 89/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3157 - acc: 0.8933 - val_loss: 0.4295 - val_acc: 0.8347\n",
      "Epoch 90/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3169 - acc: 0.8934 - val_loss: 0.4301 - val_acc: 0.8347\n",
      "Epoch 91/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3136 - acc: 0.8941 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 92/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3155 - acc: 0.8928 - val_loss: 0.4304 - val_acc: 0.8347\n",
      "Epoch 93/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.8923 - val_loss: 0.4293 - val_acc: 0.8347\n",
      "Epoch 94/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3188 - acc: 0.8931 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 95/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.8928 - val_loss: 0.4267 - val_acc: 0.8386\n",
      "Epoch 96/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3150 - acc: 0.8918 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 97/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3145 - acc: 0.8934 - val_loss: 0.4324 - val_acc: 0.8347\n",
      "Epoch 98/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3173 - acc: 0.8926 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 99/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3158 - acc: 0.8935 - val_loss: 0.4288 - val_acc: 0.8347\n",
      "Epoch 100/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3190 - acc: 0.8931 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 101/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3163 - acc: 0.8930 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 102/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3169 - acc: 0.8936 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 103/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3159 - acc: 0.8923 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 104/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3157 - acc: 0.8929 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 105/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3189 - acc: 0.8932 - val_loss: 0.4318 - val_acc: 0.8366\n",
      "Epoch 106/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3130 - acc: 0.8932 - val_loss: 0.4295 - val_acc: 0.8347\n",
      "Epoch 107/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3146 - acc: 0.8920 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 108/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3114 - acc: 0.8923 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 109/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8934 - val_loss: 0.4306 - val_acc: 0.8347\n",
      "Epoch 110/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3145 - acc: 0.8930 - val_loss: 0.4353 - val_acc: 0.8347\n",
      "Epoch 111/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3146 - acc: 0.8934 - val_loss: 0.4310 - val_acc: 0.8347\n",
      "Epoch 112/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3165 - acc: 0.8929 - val_loss: 0.4316 - val_acc: 0.8347\n",
      "Epoch 113/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3158 - acc: 0.8923 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 114/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3145 - acc: 0.8930 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 115/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3131 - acc: 0.8926 - val_loss: 0.4280 - val_acc: 0.8347\n",
      "Epoch 116/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3151 - acc: 0.8935 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 117/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.8933 - val_loss: 0.4287 - val_acc: 0.8356\n",
      "Epoch 118/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3163 - acc: 0.8934 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 119/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3148 - acc: 0.8929 - val_loss: 0.4247 - val_acc: 0.8347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3153 - acc: 0.8933 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 121/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3129 - acc: 0.8943 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 122/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3116 - acc: 0.8917 - val_loss: 0.4286 - val_acc: 0.8347\n",
      "Epoch 123/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3135 - acc: 0.8929 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 124/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3138 - acc: 0.8930 - val_loss: 0.4326 - val_acc: 0.8347\n",
      "Epoch 125/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3131 - acc: 0.8934 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 126/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3170 - acc: 0.8920 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 127/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3133 - acc: 0.8936 - val_loss: 0.4342 - val_acc: 0.8376\n",
      "Epoch 128/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3128 - acc: 0.8935 - val_loss: 0.4341 - val_acc: 0.8347\n",
      "Epoch 129/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3136 - acc: 0.8936 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 130/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3135 - acc: 0.8936 - val_loss: 0.4273 - val_acc: 0.8347\n",
      "Epoch 131/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3149 - acc: 0.8924 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 132/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3131 - acc: 0.8928 - val_loss: 0.4323 - val_acc: 0.8347\n",
      "Epoch 133/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3151 - acc: 0.8929 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 134/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3166 - acc: 0.8934 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 135/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3147 - acc: 0.8940 - val_loss: 0.4280 - val_acc: 0.8347\n",
      "Epoch 136/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3121 - acc: 0.8930 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 137/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3151 - acc: 0.8939 - val_loss: 0.4276 - val_acc: 0.8347\n",
      "Epoch 138/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3140 - acc: 0.8929 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 139/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.8932 - val_loss: 0.4290 - val_acc: 0.8347\n",
      "Epoch 140/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.8936 - val_loss: 0.4314 - val_acc: 0.8356\n",
      "Epoch 141/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3138 - acc: 0.8929 - val_loss: 0.4325 - val_acc: 0.8347\n",
      "Epoch 142/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3139 - acc: 0.8924 - val_loss: 0.4294 - val_acc: 0.8347\n",
      "Epoch 143/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3124 - acc: 0.8939 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 144/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.8928 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 145/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3134 - acc: 0.8933 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 146/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3133 - acc: 0.8933 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 147/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3109 - acc: 0.8932 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 148/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8930 - val_loss: 0.4313 - val_acc: 0.8347\n",
      "Epoch 149/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3142 - acc: 0.8930 - val_loss: 0.4294 - val_acc: 0.8347\n",
      "Epoch 150/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3104 - acc: 0.8939 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 151/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3100 - acc: 0.8945 - val_loss: 0.4311 - val_acc: 0.8347\n",
      "Epoch 152/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3135 - acc: 0.8933 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 153/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3119 - acc: 0.8937 - val_loss: 0.4263 - val_acc: 0.8347\n",
      "Epoch 154/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3139 - acc: 0.8925 - val_loss: 0.4298 - val_acc: 0.8347\n",
      "Epoch 155/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3167 - acc: 0.8922 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 156/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3128 - acc: 0.8925 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 157/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3147 - acc: 0.8929 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 158/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3101 - acc: 0.8943 - val_loss: 0.4309 - val_acc: 0.8347\n",
      "Epoch 159/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3116 - acc: 0.8923 - val_loss: 0.4239 - val_acc: 0.8347\n",
      "Epoch 160/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3121 - acc: 0.8934 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 161/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3115 - acc: 0.8930 - val_loss: 0.4167 - val_acc: 0.8347\n",
      "Epoch 162/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3109 - acc: 0.8926 - val_loss: 0.4306 - val_acc: 0.8347\n",
      "Epoch 163/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3124 - acc: 0.8929 - val_loss: 0.4299 - val_acc: 0.8347\n",
      "Epoch 164/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3150 - acc: 0.8929 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 165/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3123 - acc: 0.8925 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 166/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3146 - acc: 0.8936 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 167/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3118 - acc: 0.8936 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 168/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3135 - acc: 0.8939 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 169/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3149 - acc: 0.8933 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 170/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3114 - acc: 0.8933 - val_loss: 0.4270 - val_acc: 0.8347\n",
      "Epoch 171/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3125 - acc: 0.8939 - val_loss: 0.4252 - val_acc: 0.8376\n",
      "Epoch 172/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3115 - acc: 0.8943 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 173/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3123 - acc: 0.8932 - val_loss: 0.4327 - val_acc: 0.8347\n",
      "Epoch 174/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.8926 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 175/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3107 - acc: 0.8941 - val_loss: 0.4290 - val_acc: 0.8347\n",
      "Epoch 176/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3099 - acc: 0.8939 - val_loss: 0.4317 - val_acc: 0.8347\n",
      "Epoch 177/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3119 - acc: 0.8935 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 178/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3086 - acc: 0.8939 - val_loss: 0.4357 - val_acc: 0.8347\n",
      "Epoch 179/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3108 - acc: 0.8936 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 180/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8932 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 181/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3121 - acc: 0.8930 - val_loss: 0.4309 - val_acc: 0.8347\n",
      "Epoch 182/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8928 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 183/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3112 - acc: 0.8936 - val_loss: 0.4212 - val_acc: 0.8347\n",
      "Epoch 184/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3117 - acc: 0.8929 - val_loss: 0.4192 - val_acc: 0.8327\n",
      "Epoch 185/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8926 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 186/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3123 - acc: 0.8942 - val_loss: 0.4204 - val_acc: 0.8327\n",
      "Epoch 187/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3129 - acc: 0.8925 - val_loss: 0.4292 - val_acc: 0.8347\n",
      "Epoch 188/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3140 - acc: 0.8929 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 189/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3144 - acc: 0.8932 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 190/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3123 - acc: 0.8936 - val_loss: 0.4363 - val_acc: 0.8366\n",
      "Epoch 191/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3119 - acc: 0.8944 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 192/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.8932 - val_loss: 0.4270 - val_acc: 0.8347\n",
      "Epoch 193/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3122 - acc: 0.8928 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 194/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3108 - acc: 0.8941 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 195/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3098 - acc: 0.8932 - val_loss: 0.4264 - val_acc: 0.8347\n",
      "Epoch 196/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.8941 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 197/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3107 - acc: 0.8937 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 198/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3086 - acc: 0.8939 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 199/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3119 - acc: 0.8934 - val_loss: 0.4217 - val_acc: 0.8347\n",
      "Epoch 200/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3112 - acc: 0.8933 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 201/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3122 - acc: 0.8929 - val_loss: 0.4293 - val_acc: 0.8347\n",
      "Epoch 202/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3136 - acc: 0.8937 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 203/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3095 - acc: 0.8931 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 204/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3108 - acc: 0.8935 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 205/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8932 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 206/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3093 - acc: 0.8933 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 207/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3112 - acc: 0.8926 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 208/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3110 - acc: 0.8932 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 209/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8939 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 210/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3102 - acc: 0.8926 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 211/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3118 - acc: 0.8928 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 212/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3107 - acc: 0.8936 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 213/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8928 - val_loss: 0.4282 - val_acc: 0.8347\n",
      "Epoch 214/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3119 - acc: 0.8936 - val_loss: 0.4262 - val_acc: 0.8366\n",
      "Epoch 215/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3125 - acc: 0.8928 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 216/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8935 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 217/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3094 - acc: 0.8932 - val_loss: 0.4293 - val_acc: 0.8347\n",
      "Epoch 218/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3098 - acc: 0.8930 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 219/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3074 - acc: 0.8942 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 220/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3113 - acc: 0.8931 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 221/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3090 - acc: 0.8935 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 222/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3092 - acc: 0.8932 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 223/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3074 - acc: 0.8925 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 224/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3099 - acc: 0.8937 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 225/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3086 - acc: 0.8939 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 226/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3091 - acc: 0.8937 - val_loss: 0.4204 - val_acc: 0.8327\n",
      "Epoch 227/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3095 - acc: 0.8928 - val_loss: 0.4284 - val_acc: 0.8347\n",
      "Epoch 228/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8932 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 229/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3114 - acc: 0.8930 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 230/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3140 - acc: 0.8932 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 231/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3100 - acc: 0.8932 - val_loss: 0.4336 - val_acc: 0.8347\n",
      "Epoch 232/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3109 - acc: 0.8936 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 233/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3087 - acc: 0.8932 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 234/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3111 - acc: 0.8932 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 235/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3105 - acc: 0.8933 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 236/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3119 - acc: 0.8930 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 237/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3079 - acc: 0.8940 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 238/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8939 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 239/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3146 - acc: 0.8936 - val_loss: 0.4303 - val_acc: 0.8347\n",
      "Epoch 240/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3090 - acc: 0.8934 - val_loss: 0.4263 - val_acc: 0.8347\n",
      "Epoch 241/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.8933 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 242/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3051 - acc: 0.8930 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 243/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3112 - acc: 0.8936 - val_loss: 0.4228 - val_acc: 0.8347\n",
      "Epoch 244/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3095 - acc: 0.8926 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 245/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3113 - acc: 0.8923 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 246/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3091 - acc: 0.8945 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 247/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3091 - acc: 0.8931 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 248/600\n",
      "9082/9082 [==============================] - 0s 20us/step - loss: 0.3086 - acc: 0.8937 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 249/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3098 - acc: 0.8920 - val_loss: 0.4244 - val_acc: 0.8347\n",
      "Epoch 250/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3071 - acc: 0.8939 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 251/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3103 - acc: 0.8929 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 252/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3099 - acc: 0.8935 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 253/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3092 - acc: 0.8930 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 254/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.8928 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 255/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3085 - acc: 0.8934 - val_loss: 0.4157 - val_acc: 0.8327\n",
      "Epoch 256/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3096 - acc: 0.8937 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 257/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3104 - acc: 0.8934 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 258/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3097 - acc: 0.8942 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 259/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3104 - acc: 0.8937 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 260/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3057 - acc: 0.8941 - val_loss: 0.4313 - val_acc: 0.8347\n",
      "Epoch 261/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3072 - acc: 0.8942 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 262/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3113 - acc: 0.8941 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 263/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3091 - acc: 0.8935 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 264/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3113 - acc: 0.8935 - val_loss: 0.4323 - val_acc: 0.8347\n",
      "Epoch 265/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8931 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 266/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3067 - acc: 0.8945 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 267/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.8935 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 268/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8929 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 269/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3079 - acc: 0.8933 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 270/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3109 - acc: 0.8933 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 271/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3105 - acc: 0.8934 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 272/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3077 - acc: 0.8936 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 273/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3111 - acc: 0.8930 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 274/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3042 - acc: 0.8930 - val_loss: 0.4201 - val_acc: 0.8347\n",
      "Epoch 275/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3115 - acc: 0.8933 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 276/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3096 - acc: 0.8942 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 277/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3079 - acc: 0.8931 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 278/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3091 - acc: 0.8931 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 279/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3093 - acc: 0.8943 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 280/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3095 - acc: 0.8934 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 281/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3098 - acc: 0.8932 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 282/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8937 - val_loss: 0.4234 - val_acc: 0.8347\n",
      "Epoch 283/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3075 - acc: 0.8939 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 284/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3065 - acc: 0.8941 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 285/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3098 - acc: 0.8935 - val_loss: 0.4255 - val_acc: 0.8347\n",
      "Epoch 286/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3080 - acc: 0.8936 - val_loss: 0.4269 - val_acc: 0.8347\n",
      "Epoch 287/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.8931 - val_loss: 0.4308 - val_acc: 0.8347\n",
      "Epoch 288/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3085 - acc: 0.8942 - val_loss: 0.4267 - val_acc: 0.8347\n",
      "Epoch 289/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3071 - acc: 0.8934 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 290/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3077 - acc: 0.8943 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 291/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8940 - val_loss: 0.4260 - val_acc: 0.8347\n",
      "Epoch 292/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3065 - acc: 0.8932 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 293/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3075 - acc: 0.8935 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 294/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3080 - acc: 0.8933 - val_loss: 0.4212 - val_acc: 0.8347\n",
      "Epoch 295/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.4300 - val_acc: 0.8347\n",
      "Epoch 296/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3082 - acc: 0.8935 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 297/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8934 - val_loss: 0.4329 - val_acc: 0.8347\n",
      "Epoch 298/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3083 - acc: 0.8932 - val_loss: 0.4174 - val_acc: 0.8327\n",
      "Epoch 299/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3079 - acc: 0.8933 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 300/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3086 - acc: 0.8926 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 301/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3083 - acc: 0.8933 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 302/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3101 - acc: 0.8936 - val_loss: 0.4168 - val_acc: 0.8347\n",
      "Epoch 303/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3068 - acc: 0.8929 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 304/600\n",
      "9082/9082 [==============================] - 0s 20us/step - loss: 0.3106 - acc: 0.8934 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 305/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3064 - acc: 0.8939 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 306/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3085 - acc: 0.8930 - val_loss: 0.4248 - val_acc: 0.8347\n",
      "Epoch 307/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3072 - acc: 0.8936 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 308/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3067 - acc: 0.8935 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 309/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3066 - acc: 0.8937 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 310/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8943 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 311/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3120 - acc: 0.8925 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 312/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3068 - acc: 0.8944 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 313/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3064 - acc: 0.8933 - val_loss: 0.4175 - val_acc: 0.8347\n",
      "Epoch 314/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3084 - acc: 0.8926 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 315/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3094 - acc: 0.8939 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 316/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3097 - acc: 0.8937 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 317/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8937 - val_loss: 0.4288 - val_acc: 0.8347\n",
      "Epoch 318/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3058 - acc: 0.8932 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 319/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8935 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 320/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3109 - acc: 0.8932 - val_loss: 0.4321 - val_acc: 0.8356\n",
      "Epoch 321/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3091 - acc: 0.8928 - val_loss: 0.4238 - val_acc: 0.8347\n",
      "Epoch 322/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3092 - acc: 0.8939 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 323/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3067 - acc: 0.8941 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 324/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8935 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 325/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3059 - acc: 0.8930 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 326/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3076 - acc: 0.8925 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 327/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3082 - acc: 0.8940 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 328/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8935 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 329/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3071 - acc: 0.8929 - val_loss: 0.4237 - val_acc: 0.8347\n",
      "Epoch 330/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.8940 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 331/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3097 - acc: 0.8935 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 332/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3066 - acc: 0.8931 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 333/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.8939 - val_loss: 0.4335 - val_acc: 0.8347\n",
      "Epoch 334/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.8934 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 335/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3075 - acc: 0.8934 - val_loss: 0.4279 - val_acc: 0.8347\n",
      "Epoch 336/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3072 - acc: 0.8937 - val_loss: 0.4137 - val_acc: 0.8347\n",
      "Epoch 337/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8935 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 338/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.8932 - val_loss: 0.4210 - val_acc: 0.8327\n",
      "Epoch 339/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3109 - acc: 0.8939 - val_loss: 0.4203 - val_acc: 0.8347\n",
      "Epoch 340/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8925 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 341/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8933 - val_loss: 0.4285 - val_acc: 0.8347\n",
      "Epoch 342/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3078 - acc: 0.8939 - val_loss: 0.4288 - val_acc: 0.8347\n",
      "Epoch 343/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3064 - acc: 0.8937 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 344/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3073 - acc: 0.8940 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 345/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3061 - acc: 0.8933 - val_loss: 0.4163 - val_acc: 0.8347\n",
      "Epoch 346/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3064 - acc: 0.8933 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 347/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3053 - acc: 0.8941 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 348/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3083 - acc: 0.8937 - val_loss: 0.4223 - val_acc: 0.8347\n",
      "Epoch 349/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3077 - acc: 0.8934 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 350/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.8935 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 351/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3066 - acc: 0.8943 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 352/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3089 - acc: 0.8930 - val_loss: 0.4136 - val_acc: 0.8347\n",
      "Epoch 353/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3066 - acc: 0.8935 - val_loss: 0.4182 - val_acc: 0.8347\n",
      "Epoch 354/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3056 - acc: 0.8929 - val_loss: 0.4212 - val_acc: 0.8347\n",
      "Epoch 355/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3072 - acc: 0.8936 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 356/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3055 - acc: 0.8929 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 357/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3082 - acc: 0.8939 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 358/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8937 - val_loss: 0.4218 - val_acc: 0.8347\n",
      "Epoch 359/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3094 - acc: 0.8935 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 360/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3056 - acc: 0.8941 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 361/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3044 - acc: 0.8932 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 362/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3101 - acc: 0.8934 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 363/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8935 - val_loss: 0.4087 - val_acc: 0.8347\n",
      "Epoch 364/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3081 - acc: 0.8939 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 365/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3074 - acc: 0.8924 - val_loss: 0.4298 - val_acc: 0.8347\n",
      "Epoch 366/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3056 - acc: 0.8935 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 367/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3063 - acc: 0.8932 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 368/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8936 - val_loss: 0.4229 - val_acc: 0.8347\n",
      "Epoch 369/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3069 - acc: 0.8935 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 370/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8947 - val_loss: 0.4326 - val_acc: 0.8347\n",
      "Epoch 371/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3068 - acc: 0.8934 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 372/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3050 - acc: 0.8939 - val_loss: 0.4158 - val_acc: 0.8347\n",
      "Epoch 373/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3084 - acc: 0.8942 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 374/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3063 - acc: 0.8929 - val_loss: 0.4225 - val_acc: 0.8347\n",
      "Epoch 375/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3082 - acc: 0.8937 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 376/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3050 - acc: 0.8932 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 377/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3079 - acc: 0.8937 - val_loss: 0.4192 - val_acc: 0.8347\n",
      "Epoch 378/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3051 - acc: 0.8942 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 379/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3032 - acc: 0.8942 - val_loss: 0.4174 - val_acc: 0.8327\n",
      "Epoch 380/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3074 - acc: 0.8935 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 381/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3044 - acc: 0.8926 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 382/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3077 - acc: 0.8928 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 383/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8941 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 384/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3076 - acc: 0.8926 - val_loss: 0.4324 - val_acc: 0.8347\n",
      "Epoch 385/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3060 - acc: 0.8935 - val_loss: 0.4205 - val_acc: 0.8347\n",
      "Epoch 386/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8935 - val_loss: 0.4245 - val_acc: 0.8347\n",
      "Epoch 387/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3069 - acc: 0.8929 - val_loss: 0.4278 - val_acc: 0.8347\n",
      "Epoch 388/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3069 - acc: 0.8934 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 389/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3054 - acc: 0.8942 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 390/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3081 - acc: 0.8933 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 391/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3058 - acc: 0.8926 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 392/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3061 - acc: 0.8947 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 393/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3062 - acc: 0.8936 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 394/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3029 - acc: 0.8928 - val_loss: 0.4240 - val_acc: 0.8347\n",
      "Epoch 395/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3076 - acc: 0.8935 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 396/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3072 - acc: 0.8939 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 397/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.8941 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 398/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.8932 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 399/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8934 - val_loss: 0.4243 - val_acc: 0.8347\n",
      "Epoch 400/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3104 - acc: 0.8932 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 401/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3071 - acc: 0.8935 - val_loss: 0.4115 - val_acc: 0.8327\n",
      "Epoch 402/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3040 - acc: 0.8939 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 403/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.8940 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 404/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8934 - val_loss: 0.4191 - val_acc: 0.8347\n",
      "Epoch 405/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3062 - acc: 0.8942 - val_loss: 0.4287 - val_acc: 0.8347\n",
      "Epoch 406/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3083 - acc: 0.8943 - val_loss: 0.4251 - val_acc: 0.8347\n",
      "Epoch 407/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8932 - val_loss: 0.4250 - val_acc: 0.8347\n",
      "Epoch 408/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3050 - acc: 0.8936 - val_loss: 0.4265 - val_acc: 0.8347\n",
      "Epoch 409/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8939 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 410/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3072 - acc: 0.8929 - val_loss: 0.4233 - val_acc: 0.8347\n",
      "Epoch 411/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3067 - acc: 0.8939 - val_loss: 0.4127 - val_acc: 0.8327\n",
      "Epoch 412/600\n",
      "9082/9082 [==============================] - 0s 13us/step - loss: 0.3068 - acc: 0.8940 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 413/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3036 - acc: 0.8942 - val_loss: 0.4124 - val_acc: 0.8347\n",
      "Epoch 414/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3063 - acc: 0.8936 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 415/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3065 - acc: 0.8945 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 416/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8932 - val_loss: 0.4247 - val_acc: 0.8347\n",
      "Epoch 417/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3037 - acc: 0.8934 - val_loss: 0.4102 - val_acc: 0.8327\n",
      "Epoch 418/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3071 - acc: 0.8935 - val_loss: 0.4183 - val_acc: 0.8347\n",
      "Epoch 419/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3059 - acc: 0.8945 - val_loss: 0.4180 - val_acc: 0.8347\n",
      "Epoch 420/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3031 - acc: 0.8936 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 421/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3045 - acc: 0.8940 - val_loss: 0.4090 - val_acc: 0.8347\n",
      "Epoch 422/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8932 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 423/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8931 - val_loss: 0.4219 - val_acc: 0.8327\n",
      "Epoch 424/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3045 - acc: 0.8937 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 425/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3071 - acc: 0.8937 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 426/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3045 - acc: 0.8946 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 427/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3054 - acc: 0.8945 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 428/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3047 - acc: 0.8937 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 429/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3075 - acc: 0.8934 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 430/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3058 - acc: 0.8936 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 431/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3036 - acc: 0.8937 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 432/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3083 - acc: 0.8940 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 433/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3043 - acc: 0.8934 - val_loss: 0.4349 - val_acc: 0.8347\n",
      "Epoch 434/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3081 - acc: 0.8934 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 435/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3065 - acc: 0.8943 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 436/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3052 - acc: 0.8937 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 437/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3072 - acc: 0.8929 - val_loss: 0.4216 - val_acc: 0.8347\n",
      "Epoch 438/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3042 - acc: 0.8937 - val_loss: 0.4298 - val_acc: 0.8347\n",
      "Epoch 439/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8924 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 440/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3046 - acc: 0.8939 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 441/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8943 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 442/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3072 - acc: 0.8935 - val_loss: 0.4178 - val_acc: 0.8347\n",
      "Epoch 443/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3022 - acc: 0.8939 - val_loss: 0.4182 - val_acc: 0.8327\n",
      "Epoch 444/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3046 - acc: 0.8936 - val_loss: 0.4259 - val_acc: 0.8347\n",
      "Epoch 445/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8944 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 446/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8929 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 447/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3042 - acc: 0.8937 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 448/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3062 - acc: 0.8934 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 449/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3044 - acc: 0.8939 - val_loss: 0.4117 - val_acc: 0.8347\n",
      "Epoch 450/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3035 - acc: 0.8935 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 451/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8941 - val_loss: 0.4309 - val_acc: 0.8347\n",
      "Epoch 452/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8945 - val_loss: 0.4254 - val_acc: 0.8347\n",
      "Epoch 453/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3023 - acc: 0.8934 - val_loss: 0.4258 - val_acc: 0.8347\n",
      "Epoch 454/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8936 - val_loss: 0.4283 - val_acc: 0.8347\n",
      "Epoch 455/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3076 - acc: 0.8940 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 456/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3050 - acc: 0.8939 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 457/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8944 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 458/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3062 - acc: 0.8934 - val_loss: 0.4177 - val_acc: 0.8347\n",
      "Epoch 459/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3046 - acc: 0.8934 - val_loss: 0.4133 - val_acc: 0.8347\n",
      "Epoch 460/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3062 - acc: 0.8945 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 461/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3060 - acc: 0.8933 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 462/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3066 - acc: 0.8935 - val_loss: 0.4207 - val_acc: 0.8347\n",
      "Epoch 463/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8941 - val_loss: 0.4198 - val_acc: 0.8347\n",
      "Epoch 464/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8939 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 465/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3050 - acc: 0.8933 - val_loss: 0.4280 - val_acc: 0.8347\n",
      "Epoch 466/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 467/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3056 - acc: 0.8937 - val_loss: 0.4235 - val_acc: 0.8347\n",
      "Epoch 468/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.8934 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 469/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3030 - acc: 0.8940 - val_loss: 0.4232 - val_acc: 0.8347\n",
      "Epoch 470/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3039 - acc: 0.8936 - val_loss: 0.4160 - val_acc: 0.8347\n",
      "Epoch 471/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3041 - acc: 0.8937 - val_loss: 0.4154 - val_acc: 0.8327\n",
      "Epoch 472/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3050 - acc: 0.8936 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 473/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3029 - acc: 0.8934 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 474/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3039 - acc: 0.8932 - val_loss: 0.4206 - val_acc: 0.8347\n",
      "Epoch 475/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3047 - acc: 0.8943 - val_loss: 0.4187 - val_acc: 0.8347\n",
      "Epoch 476/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3042 - acc: 0.8936 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 477/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3046 - acc: 0.8940 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 478/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3026 - acc: 0.8945 - val_loss: 0.4080 - val_acc: 0.8327\n",
      "Epoch 479/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3040 - acc: 0.8930 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 480/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3031 - acc: 0.8936 - val_loss: 0.4170 - val_acc: 0.8347\n",
      "Epoch 481/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3058 - acc: 0.8941 - val_loss: 0.4083 - val_acc: 0.8347\n",
      "Epoch 482/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8943 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 483/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3015 - acc: 0.8941 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 484/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3036 - acc: 0.8937 - val_loss: 0.4219 - val_acc: 0.8347\n",
      "Epoch 485/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8942 - val_loss: 0.4319 - val_acc: 0.8347\n",
      "Epoch 486/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8940 - val_loss: 0.4290 - val_acc: 0.8347\n",
      "Epoch 487/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3041 - acc: 0.8941 - val_loss: 0.4175 - val_acc: 0.8347\n",
      "Epoch 488/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3028 - acc: 0.8943 - val_loss: 0.4252 - val_acc: 0.8347\n",
      "Epoch 489/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3044 - acc: 0.8937 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 490/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3031 - acc: 0.8933 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 491/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3047 - acc: 0.8930 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 492/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3036 - acc: 0.8941 - val_loss: 0.4105 - val_acc: 0.8347\n",
      "Epoch 493/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3052 - acc: 0.8940 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 494/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3038 - acc: 0.8939 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 495/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3064 - acc: 0.8937 - val_loss: 0.4169 - val_acc: 0.8347\n",
      "Epoch 496/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3009 - acc: 0.8943 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 497/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.8940 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "Epoch 498/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3053 - acc: 0.8936 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 499/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.8929 - val_loss: 0.4253 - val_acc: 0.8347\n",
      "Epoch 500/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3028 - acc: 0.8944 - val_loss: 0.4257 - val_acc: 0.8347\n",
      "Epoch 501/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8945 - val_loss: 0.4138 - val_acc: 0.8347\n",
      "Epoch 502/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8937 - val_loss: 0.4242 - val_acc: 0.8347\n",
      "Epoch 503/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3067 - acc: 0.8935 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 504/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3043 - acc: 0.8941 - val_loss: 0.4164 - val_acc: 0.8347\n",
      "Epoch 505/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.8935 - val_loss: 0.4135 - val_acc: 0.8327\n",
      "Epoch 506/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3024 - acc: 0.8936 - val_loss: 0.4144 - val_acc: 0.8347\n",
      "Epoch 507/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3026 - acc: 0.8936 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 508/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.8941 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 509/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3061 - acc: 0.8940 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 510/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3055 - acc: 0.8934 - val_loss: 0.4159 - val_acc: 0.8347\n",
      "Epoch 511/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3030 - acc: 0.8940 - val_loss: 0.4197 - val_acc: 0.8347\n",
      "Epoch 512/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3055 - acc: 0.8940 - val_loss: 0.4152 - val_acc: 0.8347\n",
      "Epoch 513/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3033 - acc: 0.8940 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 514/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3034 - acc: 0.8934 - val_loss: 0.4249 - val_acc: 0.8347\n",
      "Epoch 515/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.8937 - val_loss: 0.4149 - val_acc: 0.8347\n",
      "Epoch 516/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3037 - acc: 0.8935 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 517/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3017 - acc: 0.8941 - val_loss: 0.4154 - val_acc: 0.8347\n",
      "Epoch 518/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8936 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 519/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3051 - acc: 0.8937 - val_loss: 0.4155 - val_acc: 0.8347\n",
      "Epoch 520/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8932 - val_loss: 0.4161 - val_acc: 0.8347\n",
      "Epoch 521/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3068 - acc: 0.8937 - val_loss: 0.4275 - val_acc: 0.8347\n",
      "Epoch 522/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3061 - acc: 0.8940 - val_loss: 0.4189 - val_acc: 0.8347\n",
      "Epoch 523/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3020 - acc: 0.8936 - val_loss: 0.4200 - val_acc: 0.8347\n",
      "Epoch 524/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3021 - acc: 0.8937 - val_loss: 0.4211 - val_acc: 0.8347\n",
      "Epoch 525/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3060 - acc: 0.8939 - val_loss: 0.4190 - val_acc: 0.8347\n",
      "Epoch 526/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3031 - acc: 0.8939 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 527/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3044 - acc: 0.8932 - val_loss: 0.4262 - val_acc: 0.8347\n",
      "Epoch 528/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8937 - val_loss: 0.4214 - val_acc: 0.8347\n",
      "Epoch 529/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3048 - acc: 0.8943 - val_loss: 0.4230 - val_acc: 0.8347\n",
      "Epoch 530/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3056 - acc: 0.8940 - val_loss: 0.4261 - val_acc: 0.8347\n",
      "Epoch 531/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8936 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 532/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3013 - acc: 0.8940 - val_loss: 0.4087 - val_acc: 0.8347\n",
      "Epoch 533/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8937 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 534/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8935 - val_loss: 0.4145 - val_acc: 0.8347\n",
      "Epoch 535/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3028 - acc: 0.8942 - val_loss: 0.4151 - val_acc: 0.8347\n",
      "Epoch 536/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3067 - acc: 0.8937 - val_loss: 0.4176 - val_acc: 0.8347\n",
      "Epoch 537/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3065 - acc: 0.8936 - val_loss: 0.4166 - val_acc: 0.8347\n",
      "Epoch 538/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3041 - acc: 0.8944 - val_loss: 0.4101 - val_acc: 0.8347\n",
      "Epoch 539/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8936 - val_loss: 0.4224 - val_acc: 0.8347\n",
      "Epoch 540/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3009 - acc: 0.8944 - val_loss: 0.4132 - val_acc: 0.8347\n",
      "Epoch 541/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3059 - acc: 0.8941 - val_loss: 0.4146 - val_acc: 0.8347\n",
      "Epoch 542/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.2985 - acc: 0.8940 - val_loss: 0.4210 - val_acc: 0.8347\n",
      "Epoch 543/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3036 - acc: 0.8936 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 544/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3050 - acc: 0.8940 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 545/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3025 - acc: 0.8942 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 546/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3026 - acc: 0.8934 - val_loss: 0.4255 - val_acc: 0.8347\n",
      "Epoch 547/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3042 - acc: 0.8941 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 548/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3051 - acc: 0.8936 - val_loss: 0.4272 - val_acc: 0.8347\n",
      "Epoch 549/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3052 - acc: 0.8933 - val_loss: 0.4172 - val_acc: 0.8347\n",
      "Epoch 550/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3061 - acc: 0.8942 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 551/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3040 - acc: 0.8935 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 552/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3021 - acc: 0.8937 - val_loss: 0.4142 - val_acc: 0.8347\n",
      "Epoch 553/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8945 - val_loss: 0.4292 - val_acc: 0.8347\n",
      "Epoch 554/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3035 - acc: 0.8945 - val_loss: 0.4122 - val_acc: 0.8347\n",
      "Epoch 555/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8940 - val_loss: 0.4193 - val_acc: 0.8347\n",
      "Epoch 556/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.4146 - val_acc: 0.8347\n",
      "Epoch 557/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3044 - acc: 0.8943 - val_loss: 0.4209 - val_acc: 0.8347\n",
      "Epoch 558/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3056 - acc: 0.8937 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 559/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3038 - acc: 0.8942 - val_loss: 0.4116 - val_acc: 0.8347\n",
      "Epoch 560/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3025 - acc: 0.8935 - val_loss: 0.4075 - val_acc: 0.8347\n",
      "Epoch 561/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8932 - val_loss: 0.4215 - val_acc: 0.8347\n",
      "Epoch 562/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3027 - acc: 0.8939 - val_loss: 0.4134 - val_acc: 0.8347\n",
      "Epoch 563/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3043 - acc: 0.8946 - val_loss: 0.4220 - val_acc: 0.8347\n",
      "Epoch 564/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8939 - val_loss: 0.4104 - val_acc: 0.8327\n",
      "Epoch 565/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3053 - acc: 0.8932 - val_loss: 0.4076 - val_acc: 0.8327\n",
      "Epoch 566/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3036 - acc: 0.8936 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 567/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3058 - acc: 0.8940 - val_loss: 0.4196 - val_acc: 0.8347\n",
      "Epoch 568/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3050 - acc: 0.8940 - val_loss: 0.4171 - val_acc: 0.8347\n",
      "Epoch 569/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3037 - acc: 0.8942 - val_loss: 0.4194 - val_acc: 0.8347\n",
      "Epoch 570/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3015 - acc: 0.8943 - val_loss: 0.4293 - val_acc: 0.8347\n",
      "Epoch 571/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.8936 - val_loss: 0.4195 - val_acc: 0.8347\n",
      "Epoch 572/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3049 - acc: 0.8934 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 573/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.8939 - val_loss: 0.4226 - val_acc: 0.8347\n",
      "Epoch 574/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3034 - acc: 0.8939 - val_loss: 0.4146 - val_acc: 0.8347\n",
      "Epoch 575/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8934 - val_loss: 0.4139 - val_acc: 0.8327\n",
      "Epoch 576/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.8934 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 577/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3037 - acc: 0.8943 - val_loss: 0.4165 - val_acc: 0.8347\n",
      "Epoch 578/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3033 - acc: 0.8940 - val_loss: 0.4139 - val_acc: 0.8347\n",
      "Epoch 579/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8940 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 580/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8932 - val_loss: 0.4188 - val_acc: 0.8347\n",
      "Epoch 581/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3046 - acc: 0.8939 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 582/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3029 - acc: 0.8951 - val_loss: 0.4231 - val_acc: 0.8347\n",
      "Epoch 583/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3025 - acc: 0.8937 - val_loss: 0.4199 - val_acc: 0.8347\n",
      "Epoch 584/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3075 - acc: 0.8941 - val_loss: 0.4153 - val_acc: 0.8347\n",
      "Epoch 585/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3027 - acc: 0.8940 - val_loss: 0.4156 - val_acc: 0.8347\n",
      "Epoch 586/600\n",
      "9082/9082 [==============================] - 0s 17us/step - loss: 0.3039 - acc: 0.8933 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 587/600\n",
      "9082/9082 [==============================] - 0s 14us/step - loss: 0.3028 - acc: 0.8926 - val_loss: 0.4271 - val_acc: 0.8347\n",
      "Epoch 588/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3026 - acc: 0.8937 - val_loss: 0.4173 - val_acc: 0.8347\n",
      "Epoch 589/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3021 - acc: 0.8935 - val_loss: 0.4186 - val_acc: 0.8347\n",
      "Epoch 590/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8939 - val_loss: 0.4130 - val_acc: 0.8347\n",
      "Epoch 591/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8939 - val_loss: 0.4174 - val_acc: 0.8347\n",
      "Epoch 592/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8935 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 593/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3017 - acc: 0.8939 - val_loss: 0.4289 - val_acc: 0.8347\n",
      "Epoch 594/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3038 - acc: 0.8937 - val_loss: 0.4150 - val_acc: 0.8347\n",
      "Epoch 595/600\n",
      "9082/9082 [==============================] - 0s 18us/step - loss: 0.3047 - acc: 0.8934 - val_loss: 0.4185 - val_acc: 0.8347\n",
      "Epoch 596/600\n",
      "9082/9082 [==============================] - 0s 19us/step - loss: 0.3000 - acc: 0.8937 - val_loss: 0.4148 - val_acc: 0.8347\n",
      "Epoch 597/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3031 - acc: 0.8944 - val_loss: 0.4221 - val_acc: 0.8347\n",
      "Epoch 598/600\n",
      "9082/9082 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8934 - val_loss: 0.4184 - val_acc: 0.8347\n",
      "Epoch 599/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8941 - val_loss: 0.4181 - val_acc: 0.8347\n",
      "Epoch 600/600\n",
      "9082/9082 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8934 - val_loss: 0.4079 - val_acc: 0.8347\n"
     ]
    }
   ],
   "source": [
    "# Fitting our model \n",
    "hist = classifier.fit(scaled_train_x, train_y, batch_size = 150, epochs = 600, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46404f9a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation2/loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45e01f65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"/home/spokencall/dnnPaper/expValidation2/accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "model-id:        00010203-0405-0607-0809-0a0b0c0d0e0f\n",
      "json:        v2_model_00010203-0405-0607-0809-0a0b0c0d0e0f_.json\n",
      "h5File:        v2_model_00010203-0405-0607-0809-0a0b0c0d0e0f_.h5\n"
     ]
    }
   ],
   "source": [
    "# uniqId\n",
    "uniqId = uuid.UUID('{00010203-0405-0607-0809-0a0b0c0d0e0f}')\n",
    "jsonFile = 'v2_model_' + str(uniqId)  +'_.json'\n",
    "h5File = 'v2_model_' + str(uniqId)  + '_.h5'\n",
    "# Save the model\n",
    "model_json = classifier.to_json()\n",
    "with open(jsonFile, 'w+') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "classifier.save_weights(h5File)\n",
    "\n",
    "print(\"Saved model to disk\")\n",
    "print('model-id:        ' + str(uniqId))\n",
    "print('json:        ' + str(jsonFile))\n",
    "print('h5File:        ' + str(h5File))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model with development test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INCORRECT UTTERANCES (726)\n",
      "CorrectReject    628\n",
      "GrossFalseAccept 21*3.0 = 63.0\n",
      "PlainFalseAccept 77\n",
      "RejectionRate    0.818\n",
      "\n",
      "CORRECT UTTERANCES (1798)\n",
      "CorrectAccept    1631\n",
      "FalseReject      167\n",
      "RejectionRate    0.093\n",
      "\n",
      "--------------REPORT---------------\n",
      "-----------------------------------\n",
      "Pr                            0.921\n",
      "F                             0.914\n",
      "Sa                            0.880\n",
      "\n",
      "--------------Metrics--------------\n",
      "D                             8.804\n",
      "Da                            4.976\n",
      "Df                            6.619\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred = classifier.predict_classes(scaled_dev_test_x)\n",
    "evaluate(dev_test_y, dev_y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
